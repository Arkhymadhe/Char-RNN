{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f42d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torchinfo import summary\n",
    "\n",
    "from collections import namedtuple\n",
    "import PyPDF3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6efbf683",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf96c506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc7cb135",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.getcwd().replace('notebooks', 'data')\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'anna.txt'), 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e114c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverything was in confusion i'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7672a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, extend = True, unique_chars = None):\n",
    "    result_tuple = namedtuple('results', ['encoded_text', 'unique_char', 'int2char', 'char2int'])\n",
    "    \n",
    "    if unique_chars is None:\n",
    "        unique_chars = list(set(text).union(set('#[]{}+-*=!')))\n",
    "    if extend:\n",
    "        unique_chars.extend(list('#[]{}+-*=!'))\n",
    "        \n",
    "    char2int = {char : unique_chars.index(char) for char in unique_chars}\n",
    "    int2char = {v : k for (k, v) in char2int.items()}\n",
    "    \n",
    "    encoded_text = np.array(list(map(lambda x: char2int[x], list(text))))\n",
    "    \n",
    "    return result_tuple(encoded_text, unique_chars, int2char, char2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "986d95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_length = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a96b27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "numel_seq = batch_size * seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4358a639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numel_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d9da284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sequence(arr, batch_size, seq_length):\n",
    "    numel_seq = batch_size * seq_length\n",
    "    num_batches = arr.size // numel_seq\n",
    "    \n",
    "    arr = arr[: num_batches * numel_seq].reshape(batch_size, -1)\n",
    "    #print(arr.shape)\n",
    "    \n",
    "    batched_data = [(arr[:, n : n + seq_length], arr[:, n + 1 : n + 1 + seq_length])\n",
    "                    for n in range(0, arr.shape[1], seq_length)]\n",
    "    \n",
    "    ### Finalize final array size\n",
    "    batched_data[-1] = (batched_data[-1][0],\n",
    "                        np.append(batched_data[-1][1], batched_data[0][1][:, 0].reshape(-1, 1), axis = 1))\n",
    "    \n",
    "    ###batched_arr = [arr[n : n + numel_seq].reshape(batch_size, seq_length) for n in range(num_batches)]\n",
    "    return iter(batched_data), num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e269196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994adf7b",
   "metadata": {},
   "source": [
    "\\begin{array}{ll} \\\\\n",
    "        i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "        f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "        g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "        o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "        c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "        h_t = o_t \\odot \\tanh(c_t) \\\\\n",
    "    \\end{array}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2f70b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LSTM in module torch.nn.modules.rnn:\n",
      "\n",
      "class LSTM(RNNBase)\n",
      " |  LSTM(*args, **kwargs)\n",
      " |  \n",
      " |  Applies a multi-layer long short-term memory (LSTM) RNN to an input\n",
      " |  sequence.\n",
      " |  \n",
      " |  \n",
      " |  For each element in the input sequence, each layer computes the following\n",
      " |  function:\n",
      " |  \n",
      " |  .. math::\n",
      " |      \\begin{array}{ll} \\\\\n",
      " |          i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
      " |          f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
      " |          g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
      " |          o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
      " |          c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
      " |          h_t = o_t \\odot \\tanh(c_t) \\\\\n",
      " |      \\end{array}\n",
      " |  \n",
      " |  where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell\n",
      " |  state at time `t`, :math:`x_t` is the input at time `t`, :math:`h_{t-1}`\n",
      " |  is the hidden state of the layer at time `t-1` or the initial hidden\n",
      " |  state at time `0`, and :math:`i_t`, :math:`f_t`, :math:`g_t`,\n",
      " |  :math:`o_t` are the input, forget, cell, and output gates, respectively.\n",
      " |  :math:`\\sigma` is the sigmoid function, and :math:`\\odot` is the Hadamard product.\n",
      " |  \n",
      " |  In a multilayer LSTM, the input :math:`x^{(l)}_t` of the :math:`l` -th layer\n",
      " |  (:math:`l >= 2`) is the hidden state :math:`h^{(l-1)}_t` of the previous layer multiplied by\n",
      " |  dropout :math:`\\delta^{(l-1)}_t` where each :math:`\\delta^{(l-1)}_t` is a Bernoulli random\n",
      " |  variable which is :math:`0` with probability :attr:`dropout`.\n",
      " |  \n",
      " |  If ``proj_size > 0`` is specified, LSTM with projections will be used. This changes\n",
      " |  the LSTM cell in the following way. First, the dimension of :math:`h_t` will be changed from\n",
      " |  ``hidden_size`` to ``proj_size`` (dimensions of :math:`W_{hi}` will be changed accordingly).\n",
      " |  Second, the output hidden state of each layer will be multiplied by a learnable projection\n",
      " |  matrix: :math:`h_t = W_{hr}h_t`. Note that as a consequence of this, the output\n",
      " |  of LSTM network will be of different shape as well. See Inputs/Outputs sections below for exact\n",
      " |  dimensions of all variables. You can find more details in https://arxiv.org/abs/1402.1128.\n",
      " |  \n",
      " |  Args:\n",
      " |      input_size: The number of expected features in the input `x`\n",
      " |      hidden_size: The number of features in the hidden state `h`\n",
      " |      num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``\n",
      " |          would mean stacking two LSTMs together to form a `stacked LSTM`,\n",
      " |          with the second LSTM taking in outputs of the first LSTM and\n",
      " |          computing the final results. Default: 1\n",
      " |      bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.\n",
      " |          Default: ``True``\n",
      " |      batch_first: If ``True``, then the input and output tensors are provided\n",
      " |          as `(batch, seq, feature)` instead of `(seq, batch, feature)`.\n",
      " |          Note that this does not apply to hidden or cell states. See the\n",
      " |          Inputs/Outputs sections below for details.  Default: ``False``\n",
      " |      dropout: If non-zero, introduces a `Dropout` layer on the outputs of each\n",
      " |          LSTM layer except the last layer, with dropout probability equal to\n",
      " |          :attr:`dropout`. Default: 0\n",
      " |      bidirectional: If ``True``, becomes a bidirectional LSTM. Default: ``False``\n",
      " |      proj_size: If ``> 0``, will use LSTM with projections of corresponding size. Default: 0\n",
      " |  \n",
      " |  Inputs: input, (h_0, c_0)\n",
      " |      * **input**: tensor of shape :math:`(L, H_{in})` for unbatched input,\n",
      " |        :math:`(L, N, H_{in})` when ``batch_first=False`` or\n",
      " |        :math:`(N, L, H_{in})` when ``batch_first=True`` containing the features of\n",
      " |        the input sequence.  The input can also be a packed variable length sequence.\n",
      " |        See :func:`torch.nn.utils.rnn.pack_padded_sequence` or\n",
      " |        :func:`torch.nn.utils.rnn.pack_sequence` for details.\n",
      " |      * **h_0**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` for unbatched input or\n",
      " |        :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the\n",
      " |        initial hidden state for each element in the input sequence.\n",
      " |        Defaults to zeros if (h_0, c_0) is not provided.\n",
      " |      * **c_0**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{cell})` for unbatched input or\n",
      " |        :math:`(D * \\text{num\\_layers}, N, H_{cell})` containing the\n",
      " |        initial cell state for each element in the input sequence.\n",
      " |        Defaults to zeros if (h_0, c_0) is not provided.\n",
      " |  \n",
      " |      where:\n",
      " |  \n",
      " |      .. math::\n",
      " |          \\begin{aligned}\n",
      " |              N ={} & \\text{batch size} \\\\\n",
      " |              L ={} & \\text{sequence length} \\\\\n",
      " |              D ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\n",
      " |              H_{in} ={} & \\text{input\\_size} \\\\\n",
      " |              H_{cell} ={} & \\text{hidden\\_size} \\\\\n",
      " |              H_{out} ={} & \\text{proj\\_size if } \\text{proj\\_size}>0 \\text{ otherwise hidden\\_size} \\\\\n",
      " |          \\end{aligned}\n",
      " |  \n",
      " |  Outputs: output, (h_n, c_n)\n",
      " |      * **output**: tensor of shape :math:`(L, D * H_{out})` for unbatched input,\n",
      " |        :math:`(L, N, D * H_{out})` when ``batch_first=False`` or\n",
      " |        :math:`(N, L, D * H_{out})` when ``batch_first=True`` containing the output features\n",
      " |        `(h_t)` from the last layer of the LSTM, for each `t`. If a\n",
      " |        :class:`torch.nn.utils.rnn.PackedSequence` has been given as the input, the output\n",
      " |        will also be a packed sequence.\n",
      " |      * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` for unbatched input or\n",
      " |        :math:`(D * \\text{num\\_layers}, N, H_{out})` containing the\n",
      " |        final hidden state for each element in the sequence.\n",
      " |      * **c_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{cell})` for unbatched input or\n",
      " |        :math:`(D * \\text{num\\_layers}, N, H_{cell})` containing the\n",
      " |        final cell state for each element in the sequence.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight_ih_l[k] : the learnable input-hidden weights of the :math:`\\text{k}^{th}` layer\n",
      " |          `(W_ii|W_if|W_ig|W_io)`, of shape `(4*hidden_size, input_size)` for `k = 0`.\n",
      " |          Otherwise, the shape is `(4*hidden_size, num_directions * hidden_size)`. If\n",
      " |          ``proj_size > 0`` was specified, the shape will be\n",
      " |          `(4*hidden_size, num_directions * proj_size)` for `k > 0`\n",
      " |      weight_hh_l[k] : the learnable hidden-hidden weights of the :math:`\\text{k}^{th}` layer\n",
      " |          `(W_hi|W_hf|W_hg|W_ho)`, of shape `(4*hidden_size, hidden_size)`. If ``proj_size > 0``\n",
      " |          was specified, the shape will be `(4*hidden_size, proj_size)`.\n",
      " |      bias_ih_l[k] : the learnable input-hidden bias of the :math:`\\text{k}^{th}` layer\n",
      " |          `(b_ii|b_if|b_ig|b_io)`, of shape `(4*hidden_size)`\n",
      " |      bias_hh_l[k] : the learnable hidden-hidden bias of the :math:`\\text{k}^{th}` layer\n",
      " |          `(b_hi|b_hf|b_hg|b_ho)`, of shape `(4*hidden_size)`\n",
      " |      weight_hr_l[k] : the learnable projection weights of the :math:`\\text{k}^{th}` layer\n",
      " |          of shape `(proj_size, hidden_size)`. Only present when ``proj_size > 0`` was\n",
      " |          specified.\n",
      " |      weight_ih_l[k]_reverse: Analogous to `weight_ih_l[k]` for the reverse direction.\n",
      " |          Only present when ``bidirectional=True``.\n",
      " |      weight_hh_l[k]_reverse:  Analogous to `weight_hh_l[k]` for the reverse direction.\n",
      " |          Only present when ``bidirectional=True``.\n",
      " |      bias_ih_l[k]_reverse:  Analogous to `bias_ih_l[k]` for the reverse direction.\n",
      " |          Only present when ``bidirectional=True``.\n",
      " |      bias_hh_l[k]_reverse:  Analogous to `bias_hh_l[k]` for the reverse direction.\n",
      " |          Only present when ``bidirectional=True``.\n",
      " |      weight_hr_l[k]_reverse:  Analogous to `weight_hr_l[k]` for the reverse direction.\n",
      " |          Only present when ``bidirectional=True`` and ``proj_size > 0`` was specified.\n",
      " |  \n",
      " |  .. note::\n",
      " |      All the weights and biases are initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`\n",
      " |      where :math:`k = \\frac{1}{\\text{hidden\\_size}}`\n",
      " |  \n",
      " |  .. note::\n",
      " |      For bidirectional LSTMs, forward and backward are directions 0 and 1 respectively.\n",
      " |      Example of splitting the output layers when ``batch_first=False``:\n",
      " |      ``output.view(seq_len, batch, num_directions, hidden_size)``.\n",
      " |  \n",
      " |  .. note::\n",
      " |      ``batch_first`` argument is ignored for unbatched inputs.\n",
      " |  \n",
      " |  .. include:: ../cudnn_rnn_determinism.rst\n",
      " |  \n",
      " |  .. include:: ../cudnn_persistent_rnn.rst\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> rnn = nn.LSTM(10, 20, 2)\n",
      " |      >>> input = torch.randn(5, 3, 10)\n",
      " |      >>> h0 = torch.randn(2, 3, 20)\n",
      " |      >>> c0 = torch.randn(2, 3, 20)\n",
      " |      >>> output, (hn, cn) = rnn(input, (h0, c0))\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LSTM\n",
      " |      RNNBase\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwargs)\n",
      " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  check_forward_args(self, input: torch.Tensor, hidden: Tuple[torch.Tensor, torch.Tensor], batch_sizes: Optional[torch.Tensor])\n",
      " |      # In the future, we should prevent mypy from applying contravariance rules here.\n",
      " |      # See torch/nn/modules/module.py::_forward_unimplemented\n",
      " |  \n",
      " |  forward(self, input, hx=None)\n",
      " |      Defines the computation performed at every call.\n",
      " |      \n",
      " |      Should be overridden by all subclasses.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Although the recipe for forward pass needs to be defined within\n",
      " |          this function, one should call the :class:`Module` instance afterwards\n",
      " |          instead of this since the former takes care of running the\n",
      " |          registered hooks while the latter silently ignores them.\n",
      " |  \n",
      " |  get_expected_cell_size(self, input: torch.Tensor, batch_sizes: Optional[torch.Tensor]) -> Tuple[int, int, int]\n",
      " |  \n",
      " |  permute_hidden(self, hx: Tuple[torch.Tensor, torch.Tensor], permutation: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]\n",
      " |      # Same as above, see torch/nn/modules/module.py::_forward_unimplemented\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from RNNBase:\n",
      " |  \n",
      " |  __setattr__(self, attr, value)\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, d)\n",
      " |  \n",
      " |  check_hidden_size(self, hx: torch.Tensor, expected_hidden_size: Tuple[int, int, int], msg: str = 'Expected hidden size {}, got {}') -> None\n",
      " |  \n",
      " |  check_input(self, input: torch.Tensor, batch_sizes: Optional[torch.Tensor]) -> None\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Set the extra representation of the module\n",
      " |      \n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |  \n",
      " |  flatten_parameters(self) -> None\n",
      " |      Resets parameter data pointer so that they can use faster code paths.\n",
      " |      \n",
      " |      Right now, this works only if the module is on the GPU and cuDNN is enabled.\n",
      " |      Otherwise, it's a no-op.\n",
      " |  \n",
      " |  get_expected_hidden_size(self, input: torch.Tensor, batch_sizes: Optional[torch.Tensor]) -> Tuple[int, int, int]\n",
      " |  \n",
      " |  reset_parameters(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from RNNBase:\n",
      " |  \n",
      " |  all_weights\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from RNNBase:\n",
      " |  \n",
      " |  __annotations__ = {'batch_first': <class 'bool'>, 'bias': <class 'bool...\n",
      " |  \n",
      " |  __constants__ = ['mode', 'input_size', 'hidden_size', 'num_layers', 'b...\n",
      " |  \n",
      " |  __jit_unused_properties__ = ['all_weights']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _call_impl(self, *input, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Adds a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
      " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      " |      as well as self. Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 1.,  1.],\n",
      " |                  [ 1.,  1.]])\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
      " |      Returns an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  cpu(self: ~T) -> ~T\n",
      " |      Moves all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self: ~T) -> ~T\n",
      " |      Sets the module in evaluation mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  float(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Returns the buffer given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Returns any extra state to include in the module's state_dict.\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be pickleable to ensure working serialization\n",
      " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Returns the parameter given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Returns the submodule given by ``target`` if it exists,\n",
      " |      otherwise throws an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block::text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Module``\n",
      " |  \n",
      " |  half(self: ~T) -> ~T\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
      " |      Copies parameters and buffers from :attr:`state_dict` into\n",
      " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * **missing_keys** is a list of str containing the missing keys\n",
      " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
      " |      Returns an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      " |      Returns an iterator over module buffers, yielding both the\n",
      " |      name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>    if name in ['running_var']:\n",
      " |          >>>        print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
      " |      Returns an iterator over immediate children modules, yielding both\n",
      " |      the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Returns an iterator over all modules in the network, yielding\n",
      " |      both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |                  print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Returns an iterator over module parameters, yielding both the\n",
      " |      name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (string, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>    if name in ['bias']:\n",
      " |          >>>        print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
      " |      Returns an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Adds a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not to be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input, output) -> None or modified output\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the output. It can modify the input inplace but\n",
      " |      it will not have effect on forward since this is called after\n",
      " |      :func:`forward` is called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      It should have the following signature::\n",
      " |      \n",
      " |          hook(module, input) -> None or modified input\n",
      " |      \n",
      " |      The input contains only the positional arguments given to the module.\n",
      " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      " |      The hook can modify the input. User can either return a tuple or a\n",
      " |      single modified value in the hook. We will wrap the value into a tuple\n",
      " |      if a single value is returned(unless that value is already a tuple).\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Registers a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to module\n",
      " |      inputs are computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Adds a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (string): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
      " |      Change if autograd should record operations on parameters in this\n",
      " |      module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any)\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  share_memory(self: ~T) -> ~T\n",
      " |      See :meth:`torch.Tensor.share_memory_`\n",
      " |  \n",
      " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      " |      Returns a dictionary containing a whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Moves and/or casts the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
      " |      Moves the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self: ~T, mode: bool = True) -> ~T\n",
      " |      Sets the module in training mode.\n",
      " |      \n",
      " |      This has any effect only on certain modules. See documentations of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
      " |      Moves all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = False) -> None\n",
      " |      Sets gradients of all model parameters to zero. See similar function\n",
      " |      under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(nn.LSTM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "958d85dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(fpath):\n",
    "    with open(fpath, \"rb\") as f:\n",
    "        pdf = PyPDF3.PdfFileReader(f)\n",
    "        text = str()\n",
    "        for page_num in range(pdf.numPages):\n",
    "            page = pdf.getPage(page_num)\n",
    "            text = text + ' ' + page.extractText()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29044e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_char.pkl', 'rb') as f:\n",
    "    unique_chars = pickle.load(f)\n",
    "    \n",
    "with open('weights.pt', 'rb') as f:\n",
    "    info = torch.load(f, map_location = torch.device('cpu') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "766af69d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2',\n",
       " '4',\n",
       " 'u',\n",
       " 'V',\n",
       " 'Q',\n",
       " '@',\n",
       " 'l',\n",
       " 'F',\n",
       " '`',\n",
       " '6',\n",
       " 'f',\n",
       " ' ',\n",
       " 'C',\n",
       " 'b',\n",
       " 'x',\n",
       " '(',\n",
       " 'H',\n",
       " 'I',\n",
       " 'X',\n",
       " 'S',\n",
       " '=',\n",
       " '?',\n",
       " 'L',\n",
       " 'M',\n",
       " '{',\n",
       " 'P',\n",
       " 'k',\n",
       " 'q',\n",
       " '*',\n",
       " ',',\n",
       " 'y',\n",
       " 'A',\n",
       " 'B',\n",
       " 'D',\n",
       " 'W',\n",
       " 'G',\n",
       " 'Y',\n",
       " '9',\n",
       " '+',\n",
       " 'J',\n",
       " 'o',\n",
       " 'z',\n",
       " ':',\n",
       " 'N',\n",
       " ')',\n",
       " '3',\n",
       " 'n',\n",
       " '-',\n",
       " 'c',\n",
       " 'g',\n",
       " '5',\n",
       " '[',\n",
       " ';',\n",
       " '7',\n",
       " '\"',\n",
       " 'a',\n",
       " 'e',\n",
       " 'R',\n",
       " 'E',\n",
       " '\\n',\n",
       " 'h',\n",
       " ']',\n",
       " '1',\n",
       " '0',\n",
       " 'm',\n",
       " 'Z',\n",
       " 'r',\n",
       " 'j',\n",
       " '$',\n",
       " '8',\n",
       " '#',\n",
       " 'p',\n",
       " 'v',\n",
       " '}',\n",
       " 'i',\n",
       " 'd',\n",
       " '!',\n",
       " 'U',\n",
       " 't',\n",
       " 'T',\n",
       " '_',\n",
       " 's',\n",
       " '&',\n",
       " 'O',\n",
       " '.',\n",
       " 'K',\n",
       " \"'\",\n",
       " 'w',\n",
       " '/',\n",
       " '%',\n",
       " '#',\n",
       " '[',\n",
       " ']',\n",
       " '{',\n",
       " '}',\n",
       " '+',\n",
       " '-',\n",
       " '*',\n",
       " '=',\n",
       " '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fdc8f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_state_dict': OrderedDict([('lstm.weight_ih_l0',\n",
       "               tensor([[ 0.0153, -0.0876,  0.1679,  ...,  0.0518, -0.0367, -0.0339],\n",
       "                       [-0.1801,  0.0243,  0.0953,  ..., -0.0628,  0.0793,  0.0684],\n",
       "                       [-0.2093,  0.0910,  0.6245,  ..., -0.0290,  0.0812, -0.0207],\n",
       "                       ...,\n",
       "                       [ 0.1932, -0.1144, -0.2806,  ...,  0.0126,  0.0307,  0.0137],\n",
       "                       [ 0.2034,  0.1398,  0.4949,  ...,  0.0112,  0.0776,  0.0411],\n",
       "                       [ 0.0148,  0.1126,  0.0408,  ..., -0.0366,  0.0138,  0.0754]])),\n",
       "              ('lstm.weight_hh_l0',\n",
       "               tensor([[ 0.2525,  0.0819, -0.1941,  ..., -0.1613,  0.0800, -0.1786],\n",
       "                       [ 0.4471,  0.2502, -0.4032,  ..., -0.0685,  0.2477, -0.5673],\n",
       "                       [ 0.1754, -0.2310, -0.3944,  ..., -0.2395, -0.0897,  0.0155],\n",
       "                       ...,\n",
       "                       [ 0.2115, -0.0121, -0.0146,  ..., -0.3699,  0.0284, -0.2815],\n",
       "                       [ 0.1041, -0.3609, -0.2411,  ...,  0.0475,  0.0252,  0.2176],\n",
       "                       [ 0.2837, -0.0715,  0.0336,  ..., -0.1820,  0.0480, -0.0192]])),\n",
       "              ('lstm.bias_ih_l0',\n",
       "               tensor([ 2.2760e-01,  4.0086e-01,  7.9306e-02,  1.7152e-01,  1.5031e-01,\n",
       "                        2.8102e-01,  3.2666e-01,  3.3190e-01,  1.6188e-01,  1.4034e-01,\n",
       "                        3.4059e-01,  2.4425e-01,  1.4405e-01,  2.5033e-01,  1.2665e-01,\n",
       "                        7.4504e-02,  9.6995e-02,  1.6627e-01,  2.1899e-01,  2.4031e-01,\n",
       "                        1.4542e-01,  2.2002e-01,  6.8706e-02,  1.3383e-01,  3.1181e-01,\n",
       "                        2.7359e-01,  2.7666e-01,  4.4692e-01,  2.4166e-01,  7.8161e-02,\n",
       "                        2.9540e-01,  1.6894e-01,  3.5969e-01,  3.4065e-01,  2.4531e-01,\n",
       "                        4.1594e-01,  1.5822e-01,  1.2436e-01,  1.9644e-01,  5.9306e-02,\n",
       "                        2.8602e-01,  1.5532e-01,  2.0360e-01,  9.4659e-02,  2.0616e-01,\n",
       "                        2.4068e-01,  7.6187e-02,  3.6122e-01,  3.0875e-02,  2.4050e-01,\n",
       "                        1.5695e-01,  1.8884e-01,  3.2967e-01,  2.8098e-01,  3.8305e-01,\n",
       "                        3.1671e-01,  2.9572e-01,  2.3568e-01,  6.6861e-02,  1.5995e-01,\n",
       "                        1.7590e-01,  1.6135e-01,  2.1431e-01,  3.3504e-01,  6.3549e-02,\n",
       "                        1.4288e-01,  3.7153e-01,  2.8297e-01,  1.8761e-01,  9.8359e-02,\n",
       "                        1.6853e-01,  2.7423e-01,  1.1915e-01,  2.0792e-01,  2.0671e-01,\n",
       "                        2.3665e-01,  2.0068e-01,  1.3654e-01,  6.6389e-02,  3.9569e-01,\n",
       "                        1.6046e-01,  1.6261e-01,  2.0783e-01,  9.9252e-02,  1.4036e-01,\n",
       "                        2.3490e-01,  1.7318e-01,  3.0132e-01,  1.8092e-01,  2.7758e-01,\n",
       "                        2.8542e-02,  2.0479e-01,  8.6323e-02,  2.5977e-01,  1.4725e-01,\n",
       "                        9.7182e-02,  3.5040e-01,  2.0578e-01,  1.0072e-01,  5.0576e-01,\n",
       "                        7.3878e-02,  1.8464e-01,  2.1011e-01,  7.6674e-02,  2.9495e-01,\n",
       "                        1.1603e-01,  2.2738e-01,  1.2484e-01,  3.0930e-01,  2.5773e-01,\n",
       "                        1.7378e-01,  3.4236e-01,  2.1225e-01,  1.7154e-01,  2.2171e-01,\n",
       "                        3.5792e-01,  5.6423e-02,  2.2541e-03,  2.0664e-01,  4.1367e-01,\n",
       "                        3.0368e-01,  2.4737e-01,  2.4581e-01,  8.4596e-02,  2.0534e-01,\n",
       "                        2.3755e-01,  6.4739e-02,  2.0703e-01,  1.0504e-01, -6.3819e-02,\n",
       "                        3.7135e-02,  4.3467e-02, -5.1847e-02,  1.0633e-02,  2.7436e-02,\n",
       "                       -5.0384e-02, -1.0137e-01,  3.1042e-02, -7.5160e-02, -3.8907e-02,\n",
       "                       -4.0667e-02, -2.7781e-02, -7.0419e-02,  2.7823e-02,  8.7757e-02,\n",
       "                        3.7622e-02,  1.7285e-01, -2.7001e-02, -1.1047e-01, -4.3108e-02,\n",
       "                        1.0134e-02, -4.3735e-02,  3.4350e-02, -5.7908e-03, -5.7150e-02,\n",
       "                       -5.9686e-02, -1.0685e-01, -6.9004e-02, -4.6310e-02, -8.1404e-03,\n",
       "                       -2.6352e-02,  2.1312e-02, -2.2087e-02,  2.7808e-02, -8.7323e-03,\n",
       "                       -1.5194e-01,  1.5508e-02, -1.0190e-01, -2.5658e-02, -2.2058e-02,\n",
       "                       -1.5284e-02, -5.8251e-02,  2.5692e-02, -3.7294e-02,  9.5557e-03,\n",
       "                        4.1354e-03, -1.2920e-02,  1.6366e-01, -6.8242e-02, -7.2906e-02,\n",
       "                       -8.3248e-02, -5.3856e-02, -5.3554e-02, -1.0629e-01, -2.3517e-02,\n",
       "                        8.6463e-02,  4.2385e-02, -2.3400e-02, -1.3817e-01,  3.5840e-02,\n",
       "                        2.9935e-02,  2.4701e-02,  1.9099e-01, -1.3574e-01,  2.0110e-02,\n",
       "                       -9.3549e-02, -1.2271e-02,  5.4447e-02,  2.3875e-02, -2.8351e-02,\n",
       "                       -4.2061e-02, -5.1659e-02, -2.9618e-02,  9.4117e-02, -4.7408e-02,\n",
       "                       -3.8553e-02,  3.0420e-03,  7.1083e-02, -2.8587e-02, -3.5714e-02,\n",
       "                       -1.4762e-02,  1.9850e-01, -7.4615e-02, -1.5913e-02,  1.2281e-02,\n",
       "                       -9.5210e-02,  3.6647e-02,  2.5055e-02,  5.6619e-02, -6.9181e-02,\n",
       "                       -4.0907e-02,  2.8011e-02, -1.2952e-02, -1.8025e-02,  2.6118e-02,\n",
       "                       -9.8092e-02, -1.2486e-01, -5.9991e-02, -1.8379e-02, -1.1723e-01,\n",
       "                        1.1920e-02,  4.9763e-02,  2.7760e-03, -8.2271e-02,  5.6044e-02,\n",
       "                       -6.7871e-03,  1.3714e-02,  4.3061e-02,  1.4171e-02, -1.3684e-02,\n",
       "                       -4.2079e-02,  7.0073e-02,  5.4730e-02, -8.0323e-02, -2.2390e-02,\n",
       "                        3.7032e-02, -1.2499e-02, -6.7944e-02, -1.3903e-02, -7.9986e-02,\n",
       "                        6.6054e-02, -7.6216e-02,  9.9785e-03, -6.1433e-02, -1.0851e-01,\n",
       "                       -1.0671e-01,  1.4759e-01,  7.5942e-02, -6.0600e-02, -8.1060e-02,\n",
       "                        7.3416e-02,  7.0150e-02, -1.8487e-02, -1.0271e-02,  2.5700e-02,\n",
       "                        5.0532e-03,  2.1038e-02,  7.8533e-02, -4.1422e-02,  4.8665e-02,\n",
       "                        2.7327e-02,  6.6139e-02,  4.6419e-02, -2.7914e-02, -1.1375e-01,\n",
       "                       -7.4230e-02,  7.5116e-03,  4.0763e-02,  5.3219e-02,  1.5398e-02,\n",
       "                        4.9468e-02,  5.0070e-02, -7.0604e-02, -2.6092e-02, -9.0963e-02,\n",
       "                       -6.9687e-02, -6.0220e-02,  8.4856e-05, -8.7988e-02, -5.3616e-02,\n",
       "                       -3.9097e-03,  1.4956e-04,  1.1559e-01,  8.3216e-02, -1.0299e-02,\n",
       "                        4.0793e-03,  2.7449e-02, -5.6702e-02, -1.8560e-02,  3.3379e-03,\n",
       "                       -8.6147e-02,  1.2080e-02, -5.5390e-02, -5.8839e-02,  2.0483e-02,\n",
       "                       -4.6912e-02, -8.4791e-02, -6.0362e-02, -8.4305e-03,  1.1429e-02,\n",
       "                       -4.9148e-02,  5.0826e-02, -6.8059e-02,  5.0854e-02, -8.4793e-02,\n",
       "                        3.4702e-02, -2.2218e-02, -4.3895e-03, -7.2979e-02,  7.6041e-02,\n",
       "                       -7.8306e-02,  1.2555e-02, -6.5580e-02, -5.0994e-03,  7.5576e-02,\n",
       "                       -5.0239e-02,  3.9536e-02,  6.9237e-02, -1.2575e-02,  1.7902e-02,\n",
       "                        3.6402e-03,  1.5234e-02, -1.4263e-02,  7.5878e-03,  1.3783e-03,\n",
       "                        7.4330e-02,  7.8503e-02,  1.6500e-02, -8.8596e-02, -1.2603e-01,\n",
       "                       -7.6953e-02,  1.3345e-02, -4.4949e-02,  4.9305e-02,  3.8087e-02,\n",
       "                       -5.4993e-02,  9.0530e-02,  8.3561e-02,  8.1224e-02, -8.2714e-02,\n",
       "                       -9.6400e-03,  6.5208e-02, -1.0171e-03, -5.9076e-03, -8.2885e-02,\n",
       "                       -3.7236e-02, -7.6520e-02,  7.0521e-02,  6.7209e-02, -9.2149e-03,\n",
       "                        1.5528e-02, -9.9937e-02,  7.7062e-02,  3.3256e-02, -2.7790e-02,\n",
       "                        1.3559e-02, -4.5973e-02,  3.8894e-02,  2.3290e-02, -8.2612e-02,\n",
       "                       -2.9225e-02, -4.8818e-02, -1.2082e-02,  5.0634e-02,  1.5318e-02,\n",
       "                       -4.3339e-02, -5.8833e-02,  6.2079e-02, -5.1705e-02,  2.8720e-02,\n",
       "                        6.9428e-02,  6.1702e-02,  4.2138e-02,  5.8668e-02, -6.7971e-02,\n",
       "                        3.2054e-01,  6.4018e-02, -7.8739e-02,  4.4413e-02,  1.9672e-01,\n",
       "                        2.2297e-01,  2.8351e-01,  2.1688e-01,  1.3832e-01,  4.9452e-02,\n",
       "                        3.4393e-01,  5.2693e-02,  1.5029e-01,  1.5772e-01,  3.8269e-02,\n",
       "                       -1.2457e-02,  1.8656e-01, -5.6763e-02,  2.3686e-01,  2.5545e-01,\n",
       "                        1.3431e-01,  1.1911e-01, -2.3971e-02,  2.8896e-01,  5.1090e-01,\n",
       "                        3.4806e-01,  2.4709e-01,  1.3937e-01,  2.6113e-01,  3.1558e-01,\n",
       "                        1.4162e-01,  2.5170e-01, -5.8054e-02,  2.5967e-01,  3.4476e-01,\n",
       "                       -6.1784e-02,  3.7020e-02,  2.2838e-01,  1.9395e-01,  2.5379e-01,\n",
       "                        2.2937e-02,  2.3444e-01,  1.8525e-02,  2.9649e-01,  1.6722e-01,\n",
       "                        1.7505e-03,  2.8092e-01,  7.9346e-02, -4.1773e-02,  2.9285e-01,\n",
       "                        1.4612e-01,  3.3590e-01,  3.7125e-01,  3.5092e-01,  9.6163e-02,\n",
       "                        2.1342e-01, -1.3226e-02,  4.4776e-02,  1.2513e-01,  2.0117e-01,\n",
       "                        1.3394e-01,  1.9706e-01,  2.0335e-01,  4.8769e-02,  1.8868e-01,\n",
       "                        2.4917e-01,  3.8818e-01,  1.7806e-01,  1.5709e-01,  2.8293e-01,\n",
       "                        2.4471e-01,  1.7857e-01,  1.2988e-01,  1.9507e-01,  2.3005e-01,\n",
       "                        5.0575e-02,  1.1752e-01, -2.3874e-02, -6.4174e-02,  9.1197e-03,\n",
       "                        9.4871e-02,  4.3649e-02, -4.3450e-02,  1.7458e-01,  2.4979e-01,\n",
       "                        1.2641e-01,  1.2291e-01,  3.3428e-02,  3.2869e-01,  2.4882e-02,\n",
       "                        2.5668e-01,  1.9134e-01,  4.0933e-01,  3.3410e-01,  1.7164e-01,\n",
       "                        1.8313e-01,  2.6326e-01,  7.4358e-02,  2.3956e-01,  2.1116e-01,\n",
       "                        2.9999e-01,  1.4836e-01, -6.5815e-03,  1.3278e-01,  9.7997e-03,\n",
       "                        1.4670e-01,  1.0832e-01,  7.8335e-03,  2.2013e-01,  1.5691e-01,\n",
       "                        4.8246e-01,  2.4006e-01, -1.7497e-02,  7.4309e-02,  2.5592e-01,\n",
       "                        1.7304e-01, -9.1661e-04,  9.7051e-03,  3.6213e-01,  1.5774e-01,\n",
       "                        2.0499e-01,  6.2137e-02,  9.7482e-02,  1.0578e-01,  4.1293e-02,\n",
       "                        1.4097e-01,  1.3528e-01])),\n",
       "              ('lstm.bias_hh_l0',\n",
       "               tensor([ 2.0718e-01,  4.0152e-01,  1.5768e-01,  2.0612e-01,  1.0894e-01,\n",
       "                        1.4810e-01,  3.3950e-01,  1.7520e-01,  3.0122e-01,  2.7053e-01,\n",
       "                        2.9356e-01,  2.4682e-01,  1.4737e-01,  1.9059e-01,  1.2044e-01,\n",
       "                        1.0098e-01,  2.2605e-01,  1.4467e-01,  2.6363e-01,  1.6044e-01,\n",
       "                        1.4487e-01,  1.6594e-01,  1.4529e-01,  1.0230e-01,  3.5030e-01,\n",
       "                        3.4092e-01,  3.7185e-01,  4.2874e-01,  2.2200e-01,  2.1127e-01,\n",
       "                        2.7798e-01,  2.5589e-01,  2.7153e-01,  2.7333e-01,  3.5607e-01,\n",
       "                        4.7949e-01,  9.0555e-02,  1.1752e-01,  2.0014e-01,  1.5208e-01,\n",
       "                        2.5196e-01,  6.5198e-02,  2.6046e-01,  1.9319e-01,  1.7831e-01,\n",
       "                        2.3414e-01,  1.9641e-01,  2.0780e-01,  3.6701e-02,  1.5826e-01,\n",
       "                        1.5156e-01,  1.6483e-01,  3.7131e-01,  4.0181e-01,  2.2406e-01,\n",
       "                        2.2903e-01,  1.7919e-01,  1.3360e-01,  2.0717e-01,  3.0120e-01,\n",
       "                        2.3121e-01,  8.8822e-02,  2.3669e-01,  2.0453e-01,  6.3037e-02,\n",
       "                        1.2980e-01,  2.6227e-01,  2.4830e-01,  1.8366e-01,  1.4550e-01,\n",
       "                        3.3203e-01,  2.0760e-01,  1.1129e-01,  2.2857e-01,  2.6622e-01,\n",
       "                        2.9712e-01,  2.7291e-01,  9.6642e-02,  5.7123e-02,  3.8736e-01,\n",
       "                        2.4153e-01,  1.7227e-01,  2.2735e-01,  1.9642e-01,  3.0343e-01,\n",
       "                        3.7879e-01,  1.9653e-01,  2.0871e-01,  2.0189e-01,  1.6237e-01,\n",
       "                        3.5722e-02,  1.1291e-01,  1.2370e-01,  4.2062e-01,  1.3285e-01,\n",
       "                        1.7715e-01,  2.8234e-01,  2.1233e-01,  9.6157e-03,  4.6391e-01,\n",
       "                        8.4208e-02,  1.8748e-01,  2.8752e-01,  1.1326e-01,  1.7545e-01,\n",
       "                        2.1131e-01,  1.2671e-01,  1.8602e-01,  2.6588e-01,  3.1464e-01,\n",
       "                        1.6803e-01,  3.4705e-01,  2.7157e-01,  1.5011e-01,  1.1938e-01,\n",
       "                        4.4605e-01,  1.9436e-01,  4.3141e-02,  9.7024e-02,  3.3823e-01,\n",
       "                        1.8873e-01,  2.4600e-01,  1.2994e-01,  1.9186e-01,  1.0919e-01,\n",
       "                        2.2741e-01,  1.2282e-01,  2.9050e-01,  1.1512e-01, -1.5551e-02,\n",
       "                        4.8226e-03,  1.0399e-01,  4.1627e-02,  5.2379e-02, -6.8810e-03,\n",
       "                       -1.0401e-01, -5.9555e-03, -5.1807e-03, -6.9035e-02, -7.5956e-02,\n",
       "                       -9.2199e-02,  6.8781e-02,  2.4195e-02, -3.2279e-02,  5.0098e-02,\n",
       "                       -3.7572e-02,  1.1781e-01, -5.8147e-02, -7.2087e-02, -5.1379e-02,\n",
       "                        7.5081e-02,  4.9435e-02, -7.0062e-02, -4.2003e-02, -8.0714e-02,\n",
       "                       -4.0257e-02, -2.3132e-02, -1.0066e-01,  6.1100e-02, -7.4389e-02,\n",
       "                       -1.0363e-01,  1.7117e-02,  2.2805e-02,  1.1172e-02,  3.5920e-02,\n",
       "                       -1.5386e-01, -7.7395e-02, -1.0817e-01, -1.4340e-01, -5.1803e-02,\n",
       "                        5.2945e-03, -6.0841e-02, -2.1854e-02, -1.5673e-01,  6.7791e-02,\n",
       "                        4.2132e-03,  3.5669e-02,  2.7761e-02,  7.0649e-02, -3.5894e-03,\n",
       "                       -1.1249e-01, -1.3855e-01, -1.1386e-01, -1.0046e-01,  2.7901e-02,\n",
       "                        1.7354e-01, -1.5716e-02, -6.3607e-02, -1.8074e-02, -6.2967e-02,\n",
       "                       -5.3522e-02, -7.4027e-02,  1.1805e-01, -6.0376e-02, -6.5660e-03,\n",
       "                       -1.6796e-02, -7.3896e-02, -5.3259e-03, -9.3287e-02, -4.2942e-02,\n",
       "                        6.2832e-02,  4.0705e-02, -7.5395e-03, -4.7745e-02,  6.6937e-03,\n",
       "                        2.9706e-02, -5.5066e-02,  6.5522e-02, -6.2940e-02,  2.3620e-02,\n",
       "                       -6.5588e-02,  8.7054e-02, -9.3973e-02, -1.4299e-01, -5.2606e-02,\n",
       "                       -1.4807e-02,  3.4559e-02,  1.9574e-02,  2.3464e-02, -6.3253e-02,\n",
       "                       -5.6626e-02, -9.8686e-02, -5.7630e-02, -6.9526e-02, -1.5237e-02,\n",
       "                        4.8441e-02, -1.3535e-01, -6.7540e-02, -4.5629e-02, -3.3266e-02,\n",
       "                        1.2950e-02,  7.5268e-02, -8.5733e-02, -8.7989e-02,  9.2797e-02,\n",
       "                       -1.0801e-01, -8.1367e-02,  3.7849e-03, -2.1990e-02, -8.6985e-02,\n",
       "                        1.4988e-02,  6.7860e-02,  6.4758e-03, -5.6796e-02, -1.0247e-01,\n",
       "                        5.2895e-02,  4.2922e-02, -1.7024e-02,  2.3963e-02,  5.0841e-02,\n",
       "                       -6.2241e-02, -1.9530e-02, -1.0299e-01, -3.6824e-02, -9.6220e-02,\n",
       "                       -9.7737e-02,  7.1897e-02, -7.1814e-02, -7.7860e-03, -1.2880e-01,\n",
       "                        8.9124e-02, -7.6212e-02, -5.8794e-02,  1.3411e-02, -6.1108e-02,\n",
       "                        6.5282e-02, -1.7259e-02, -1.6842e-02, -1.6487e-02,  6.2153e-02,\n",
       "                        1.7636e-02,  8.4137e-02,  7.7410e-02,  4.0014e-02, -9.1588e-02,\n",
       "                       -3.8498e-03, -4.5756e-04,  1.4509e-02, -1.0187e-01, -5.7520e-02,\n",
       "                       -3.4505e-02, -3.1339e-02, -2.2741e-02, -2.3276e-02,  4.1758e-02,\n",
       "                       -7.4124e-02,  3.3899e-02,  2.9751e-02, -4.5195e-03,  8.1889e-02,\n",
       "                       -1.0351e-02,  2.5200e-02, -3.1276e-02,  9.0724e-02, -3.7994e-02,\n",
       "                       -3.3197e-02,  6.9193e-02,  8.8397e-02, -2.2262e-02, -4.2907e-02,\n",
       "                       -3.1946e-02, -2.9034e-02, -6.4586e-02,  8.7089e-02,  4.0652e-02,\n",
       "                       -1.4825e-01,  7.4389e-03, -9.0951e-03,  5.8880e-02, -7.4002e-03,\n",
       "                       -3.3207e-02,  1.7855e-02, -2.8301e-02,  5.4309e-02,  1.1951e-02,\n",
       "                        5.5782e-02,  4.7671e-02, -4.1380e-02, -4.9871e-02, -1.6181e-02,\n",
       "                       -4.9818e-02,  1.0167e-02, -4.0439e-02, -6.4162e-02,  3.4898e-02,\n",
       "                       -1.9224e-02, -7.4996e-02,  2.7109e-03, -8.1024e-02,  7.1759e-02,\n",
       "                       -7.3274e-02,  2.2418e-02,  2.4544e-02, -2.7923e-02, -6.0863e-02,\n",
       "                        1.3227e-01,  6.3792e-02,  8.1221e-02, -2.4866e-02, -5.3730e-02,\n",
       "                       -2.1802e-02,  7.1302e-02,  3.9097e-02,  8.6870e-02,  3.5454e-02,\n",
       "                        6.0797e-02,  1.0823e-01,  2.0834e-02, -1.8967e-02,  6.6170e-02,\n",
       "                       -1.7990e-02,  2.0741e-02, -1.1769e-02, -1.6862e-02,  3.3171e-03,\n",
       "                       -3.0402e-02,  3.2468e-02,  3.4080e-02,  7.3927e-02, -7.0349e-02,\n",
       "                       -2.2214e-02, -6.7639e-02,  7.0167e-02,  6.4953e-02,  5.0426e-02,\n",
       "                       -1.4779e-02,  1.6768e-02, -7.1058e-02,  2.1666e-02,  1.9655e-02,\n",
       "                       -7.2240e-02, -1.0789e-02, -1.5652e-02,  9.1450e-02, -5.7141e-02,\n",
       "                        6.7215e-02,  5.7382e-02,  8.1064e-02, -3.5206e-02,  3.6718e-02,\n",
       "                        9.9921e-03,  2.2632e-05,  1.3877e-02, -9.0779e-02,  1.6647e-02,\n",
       "                        3.7277e-01,  5.3730e-02,  7.1160e-02,  1.2887e-02,  1.0232e-01,\n",
       "                        2.4064e-01,  2.6874e-01,  7.9840e-02,  6.9659e-02,  1.5039e-01,\n",
       "                        2.6920e-01,  2.9492e-02,  5.7286e-02,  9.7054e-02,  3.6016e-02,\n",
       "                        5.3983e-02,  1.4777e-01, -2.2673e-02,  1.9809e-01,  1.4969e-01,\n",
       "                        4.7062e-02,  1.5828e-01,  3.7350e-02,  3.3380e-01,  3.9051e-01,\n",
       "                        2.8979e-01,  2.1204e-01,  1.7015e-01,  2.8952e-01,  2.4599e-01,\n",
       "                        8.1078e-02,  3.1038e-01,  2.8439e-02,  3.5825e-01,  2.6806e-01,\n",
       "                       -2.0461e-02,  1.8393e-01,  2.0336e-01,  2.0744e-01,  2.0256e-01,\n",
       "                        7.0638e-02,  1.9288e-01,  4.3676e-02,  2.7217e-01,  1.8167e-01,\n",
       "                        1.0081e-01,  3.1154e-01,  1.2997e-01,  8.8694e-02,  2.7702e-01,\n",
       "                        1.3518e-01,  3.3175e-01,  2.2535e-01,  3.7823e-01,  1.1831e-01,\n",
       "                        2.0877e-01, -2.6435e-02,  9.9912e-02,  1.9479e-01,  2.4194e-01,\n",
       "                        1.9331e-01,  2.5538e-01,  2.8924e-01, -1.9278e-02,  1.8831e-01,\n",
       "                        1.8242e-01,  3.7809e-01,  2.5747e-01,  1.6658e-01,  2.2501e-01,\n",
       "                        2.0710e-01,  3.1451e-02,  4.3714e-02,  1.6436e-01,  1.6920e-01,\n",
       "                        3.1479e-02,  3.2083e-02,  1.2919e-01, -5.7005e-02,  9.5778e-02,\n",
       "                        1.7905e-01,  1.5032e-01,  6.9411e-02,  8.8446e-02,  2.8760e-01,\n",
       "                        1.5827e-01,  2.7372e-01,  5.1245e-02,  3.9887e-01,  2.8849e-02,\n",
       "                        2.3430e-01,  7.3956e-02,  3.8779e-01,  1.9750e-01,  2.1111e-01,\n",
       "                        1.8628e-01,  2.5409e-01,  1.1423e-02,  2.3404e-01,  1.5457e-01,\n",
       "                        3.2810e-01,  1.0481e-01,  1.0249e-02,  1.7327e-01, -6.7645e-04,\n",
       "                        2.3602e-01,  8.5659e-02,  1.0438e-02,  2.5205e-01,  1.3224e-01,\n",
       "                        3.2895e-01,  1.8707e-01, -4.3286e-02,  5.5136e-02,  1.3746e-01,\n",
       "                        7.4355e-02, -7.6515e-02,  1.7889e-01,  3.4793e-01,  2.9678e-01,\n",
       "                        1.7369e-01,  1.7060e-01,  1.7796e-01,  4.7712e-02,  1.6036e-01,\n",
       "                        1.7859e-01,  1.2255e-01])),\n",
       "              ('lstm.weight_ih_l1',\n",
       "               tensor([[-1.0249e-01,  8.0582e-03,  9.3583e-02,  ...,  2.6974e-01,\n",
       "                         1.2176e-01, -9.7881e-02],\n",
       "                       [ 8.8368e-02,  6.6143e-02, -2.8702e-02,  ...,  4.0928e-02,\n",
       "                        -1.2375e-02, -1.4745e-01],\n",
       "                       [ 3.5614e-02,  8.6855e-02, -1.8541e-01,  ...,  4.9190e-02,\n",
       "                        -2.2581e-01,  1.6110e-01],\n",
       "                       ...,\n",
       "                       [ 1.7836e-01, -1.6991e-01, -2.7153e-01,  ...,  2.6410e-01,\n",
       "                         2.4462e-01,  9.7620e-02],\n",
       "                       [ 3.1342e-01, -4.4565e-01, -7.4926e-02,  ..., -2.0427e-01,\n",
       "                        -6.0001e-01, -8.7386e-02],\n",
       "                       [ 8.6846e-02, -3.0655e-01,  1.7694e-03,  ...,  1.4276e-05,\n",
       "                         1.1951e-01,  4.3234e-02]])),\n",
       "              ('lstm.weight_hh_l1',\n",
       "               tensor([[-1.3881e-01, -1.5997e-01, -4.4578e-02,  ..., -2.1288e-01,\n",
       "                        -1.8141e-01,  1.2210e-01],\n",
       "                       [-9.3665e-02,  3.6099e-02,  7.0414e-02,  ...,  4.1351e-02,\n",
       "                        -4.3248e-02,  2.2327e-02],\n",
       "                       [-3.8591e-05,  6.4240e-02, -1.5815e-02,  ..., -3.0666e-01,\n",
       "                        -6.4346e-02,  5.0141e-02],\n",
       "                       ...,\n",
       "                       [ 2.3346e-01, -1.9079e-01,  1.3663e-01,  ...,  2.0183e-01,\n",
       "                        -1.0313e-02,  4.6820e-02],\n",
       "                       [-7.1669e-02,  2.3648e-01, -2.1030e-01,  ..., -2.4767e-01,\n",
       "                        -1.2495e-01, -2.2565e-01],\n",
       "                       [ 3.1226e-01,  1.7787e-01, -2.7995e-01,  ...,  2.8997e-02,\n",
       "                        -9.4551e-02, -1.2727e-01]])),\n",
       "              ('lstm.bias_ih_l1',\n",
       "               tensor([ 1.0639e-01,  1.3282e-01,  1.0066e-01,  1.3082e-01,  6.9309e-02,\n",
       "                        9.9500e-02, -2.6775e-02, -3.2457e-02,  2.0711e-01,  1.4827e-01,\n",
       "                       -6.7663e-03,  1.3181e-01,  7.7826e-02,  1.0989e-01, -4.4830e-02,\n",
       "                        5.0159e-02,  9.5815e-02,  9.9013e-02,  2.1595e-01,  9.5667e-02,\n",
       "                        3.3299e-02,  6.1105e-02,  4.0559e-02, -4.5964e-02, -1.3986e-02,\n",
       "                        1.0014e-01,  1.5363e-01,  2.7499e-02,  8.8624e-03, -2.5168e-02,\n",
       "                        2.4706e-01,  1.0318e-01,  1.0576e-02,  8.8807e-02,  6.8415e-03,\n",
       "                        2.1859e-03,  1.1425e-01,  1.6073e-01, -6.6244e-02,  1.2074e-01,\n",
       "                        1.1236e-01,  6.2923e-03,  8.9714e-02, -8.6801e-02,  1.9491e-01,\n",
       "                        1.4549e-01,  6.1102e-02, -2.4473e-02,  1.9196e-01,  1.8266e-01,\n",
       "                       -2.9622e-02,  1.5507e-02, -2.8773e-02,  9.6230e-02,  2.3406e-01,\n",
       "                        1.6191e-01,  1.3592e-01,  9.6384e-02, -3.9184e-02,  1.7360e-01,\n",
       "                        8.8715e-02,  4.9989e-02,  4.5585e-03,  1.1188e-01,  2.1740e-01,\n",
       "                        1.2719e-01,  1.0569e-01,  1.1372e-01,  4.4992e-02,  4.7826e-02,\n",
       "                       -1.3799e-02,  8.6553e-02, -7.3534e-02, -7.4501e-02,  1.3235e-01,\n",
       "                        1.3348e-01,  2.7258e-02,  1.4811e-01,  6.7372e-02,  3.5463e-02,\n",
       "                        2.8012e-02,  1.2791e-02, -1.6436e-02,  1.0546e-01,  1.0672e-01,\n",
       "                        8.1329e-02,  8.3114e-02,  1.5302e-01,  2.7845e-02,  1.4184e-01,\n",
       "                        6.3488e-02, -5.0426e-03,  4.6987e-02,  2.0194e-02,  9.8043e-02,\n",
       "                        8.1450e-02,  2.3016e-02,  1.6951e-01,  3.5222e-02,  4.4614e-02,\n",
       "                        1.8443e-01,  2.9620e-02,  4.1146e-02,  1.5786e-01,  1.0959e-01,\n",
       "                        1.4939e-01,  6.4671e-02,  1.1238e-01,  4.9085e-02,  3.4122e-02,\n",
       "                        1.0762e-01, -8.0470e-03,  2.3067e-01,  1.2558e-01,  1.2964e-01,\n",
       "                        1.3041e-01, -8.8403e-03, -9.8096e-03,  2.1130e-02,  1.8259e-01,\n",
       "                        1.5452e-01,  9.3770e-02,  1.0133e-01,  3.2214e-01,  3.4851e-03,\n",
       "                       -2.9410e-02,  7.6055e-02,  2.7355e-02,  1.3433e-02,  1.1245e-02,\n",
       "                       -4.6083e-02, -5.7507e-02,  3.5633e-03, -5.6791e-02,  7.4853e-03,\n",
       "                       -6.0236e-02,  9.0404e-02, -1.1632e-01, -4.2428e-02,  9.8034e-02,\n",
       "                        2.8407e-03,  4.5032e-02,  1.1591e-01, -7.5929e-02,  1.5305e-01,\n",
       "                       -8.7637e-02,  6.6399e-02,  1.2351e-02,  2.1254e-02,  2.2867e-02,\n",
       "                        5.3835e-02, -7.8169e-02,  6.8247e-02, -3.8328e-02,  1.1432e-02,\n",
       "                       -6.6961e-02, -3.5521e-02,  1.0667e-01, -5.4763e-02,  7.4886e-02,\n",
       "                       -1.5313e-02,  4.8176e-02,  3.5620e-02,  2.5455e-02, -4.0525e-02,\n",
       "                       -3.1394e-02, -5.5612e-02,  9.2763e-02, -3.9128e-02,  1.2881e-02,\n",
       "                        1.1057e-01,  1.5780e-02,  9.3511e-03, -6.5201e-02, -7.1418e-03,\n",
       "                       -7.2396e-02,  1.0191e-02,  6.8700e-02, -2.1605e-02,  2.2279e-02,\n",
       "                       -1.7857e-02, -4.8893e-02,  7.9284e-02,  4.4984e-02,  8.4519e-02,\n",
       "                        5.3167e-02, -6.7494e-02,  2.7860e-02, -1.4199e-03,  1.1271e-01,\n",
       "                       -4.2735e-03, -8.7512e-02,  5.8952e-02,  9.9207e-03,  2.7662e-02,\n",
       "                        2.4650e-02, -9.3592e-02,  9.3969e-02,  4.7661e-02,  4.2865e-02,\n",
       "                       -5.4065e-02,  3.2779e-02,  1.1370e-01, -6.7543e-02,  7.2777e-02,\n",
       "                        1.1668e-01, -9.3504e-02, -9.4958e-02, -6.2661e-03, -5.2844e-02,\n",
       "                       -1.1058e-01,  5.4826e-02,  4.9871e-02, -4.0610e-02,  1.6878e-02,\n",
       "                        3.1110e-02,  9.1997e-03, -1.3293e-02, -1.0650e-01, -1.3980e-02,\n",
       "                        1.2890e-01,  7.1022e-02, -6.8764e-02, -1.2117e-02,  6.6994e-02,\n",
       "                        4.4704e-02, -9.4054e-03, -5.2711e-02,  7.5204e-02,  6.5509e-02,\n",
       "                        8.9744e-03,  1.0605e-01, -4.6832e-02,  2.8365e-02, -4.3297e-02,\n",
       "                       -1.3900e-02, -7.3230e-02,  2.2227e-02,  8.9034e-02, -8.1379e-02,\n",
       "                        2.4823e-02, -8.5042e-02,  5.0812e-02, -1.0911e-02, -1.8969e-02,\n",
       "                       -1.5316e-02, -2.2464e-02,  6.4403e-02,  5.5810e-02, -3.0841e-02,\n",
       "                        3.4554e-02,  2.4346e-02,  9.3320e-03, -7.3023e-02, -4.1751e-02,\n",
       "                       -1.9785e-02, -7.4046e-04,  8.4799e-03,  5.7930e-02, -6.9858e-03,\n",
       "                        1.3865e-02,  6.1330e-03, -5.0302e-02, -8.7292e-02, -1.0476e-02,\n",
       "                        2.0018e-02,  6.9315e-03,  2.5805e-02, -1.5099e-01,  2.5559e-02,\n",
       "                        3.0694e-03, -8.4263e-03,  4.9189e-02, -3.5986e-02, -5.7627e-02,\n",
       "                        3.9273e-03,  1.1300e-01,  7.3400e-02,  8.8591e-02,  6.4414e-02,\n",
       "                       -6.4583e-02,  8.8117e-03,  3.1275e-02,  1.1962e-01, -2.1093e-02,\n",
       "                       -1.1513e-01, -7.0148e-02,  1.3195e-01, -1.3858e-01, -2.8174e-02,\n",
       "                       -9.8662e-02,  8.1979e-02, -1.2373e-01, -1.0113e-01,  1.2548e-02,\n",
       "                       -1.0479e-01,  7.2069e-02,  9.3044e-02, -1.1517e-01,  1.0809e-02,\n",
       "                        1.3397e-01, -6.1757e-04, -1.3381e-01,  1.5350e-03,  9.8451e-02,\n",
       "                        4.7677e-02,  1.9860e-02,  8.6559e-02, -5.8655e-02, -5.3328e-02,\n",
       "                        7.7517e-02,  1.4604e-01,  1.1572e-01, -8.1255e-02,  3.3763e-02,\n",
       "                       -3.7022e-02, -4.3120e-02,  1.6832e-01,  6.0934e-02, -4.7318e-02,\n",
       "                       -8.4043e-02,  2.2930e-02,  3.8838e-02, -5.7429e-02, -1.0269e-01,\n",
       "                        8.0262e-02, -3.8922e-02, -3.6538e-02,  1.2498e-02,  1.0304e-01,\n",
       "                       -9.1980e-02,  7.8767e-02, -1.0176e-01, -7.6470e-02, -1.0428e-01,\n",
       "                       -2.1356e-02,  2.1600e-02, -6.9733e-02, -3.8942e-03, -2.6085e-03,\n",
       "                        1.0462e-02,  1.6521e-02, -1.0753e-01, -1.0436e-01, -1.1793e-01,\n",
       "                       -2.5244e-04, -3.0934e-02,  7.6998e-03, -7.3566e-04,  1.4022e-01,\n",
       "                        6.2889e-02,  3.0053e-02,  4.0530e-02, -7.7728e-02, -4.3921e-03,\n",
       "                        8.0058e-02,  5.6131e-02,  4.9486e-02, -1.2265e-01,  4.5220e-02,\n",
       "                       -1.1353e-01, -5.2370e-02, -6.6407e-02, -4.6067e-02, -9.6783e-02,\n",
       "                        6.4799e-03,  1.1794e-01,  1.5342e-02, -5.6431e-03,  5.1993e-02,\n",
       "                       -7.7446e-02, -1.0626e-01,  6.9396e-02,  4.8434e-02,  2.6627e-02,\n",
       "                        8.0306e-02, -2.8367e-02, -1.3101e-01,  6.6463e-02, -1.0173e-01,\n",
       "                        6.7248e-02, -7.6409e-02, -2.1843e-02,  6.3533e-02, -2.2208e-02,\n",
       "                       -1.4514e-01,  1.4721e-01, -2.6902e-02,  5.0575e-02,  1.5856e-01,\n",
       "                        2.6433e-02,  2.7673e-02,  1.6390e-02,  1.9166e-01,  6.2588e-02,\n",
       "                        5.7704e-02,  2.1878e-02,  1.0412e-02,  4.8391e-02,  1.0390e-02,\n",
       "                        1.1906e-01,  1.1003e-01,  1.2671e-01, -2.4193e-04, -6.3000e-02,\n",
       "                        9.3630e-02,  9.6892e-03,  1.2090e-01,  2.2356e-02,  7.1965e-02,\n",
       "                        7.2986e-02,  7.4240e-02,  7.3901e-02,  1.5673e-01,  9.6885e-02,\n",
       "                        1.0236e-01, -1.5148e-03,  1.4878e-01,  1.2998e-01, -1.7761e-02,\n",
       "                       -9.2247e-02,  4.5124e-02,  2.8113e-01,  3.0590e-02,  8.7291e-03,\n",
       "                        4.1577e-02,  5.6113e-02,  3.7083e-02,  1.2046e-01,  1.1021e-02,\n",
       "                        8.4190e-02,  1.3140e-01, -8.7816e-03,  1.3368e-01,  1.4823e-02,\n",
       "                       -1.8510e-02,  5.8174e-02,  8.0412e-02,  9.6879e-02,  5.0708e-02,\n",
       "                        1.0350e-01,  8.3372e-02,  8.2640e-02,  8.0436e-02,  1.1970e-01,\n",
       "                        1.2531e-02, -2.5386e-03,  4.4917e-02, -8.6036e-02,  1.4414e-01,\n",
       "                        6.0980e-03,  2.7278e-02,  3.1598e-02, -1.6234e-02,  8.6176e-02,\n",
       "                       -6.9863e-02, -1.5724e-02,  1.1168e-03,  1.5142e-02,  4.0303e-02,\n",
       "                        5.1681e-02,  1.2838e-01,  2.9314e-02,  9.4288e-02,  1.3175e-01,\n",
       "                        6.9282e-02,  1.8509e-01, -7.0569e-02,  4.7013e-02, -7.5543e-02,\n",
       "                        5.5272e-02,  5.2120e-02,  1.0025e-01,  2.3331e-02,  5.6030e-02,\n",
       "                        1.7367e-02,  4.3179e-02,  8.2375e-02,  2.2155e-01,  1.5764e-01,\n",
       "                        5.9233e-02,  1.0039e-02,  3.6763e-02,  1.7069e-02,  8.4863e-02,\n",
       "                       -1.4874e-02,  1.5517e-01,  8.8009e-02, -6.4979e-02,  9.2177e-02,\n",
       "                        8.7600e-02, -1.5729e-02,  1.1070e-01, -1.5247e-02,  1.4282e-03,\n",
       "                        1.4071e-01,  7.3764e-02,  3.9826e-02,  8.0324e-03,  1.0855e-01,\n",
       "                        8.9343e-02,  6.9954e-02,  1.1754e-01,  7.8928e-02,  1.5461e-02,\n",
       "                        2.5704e-02,  1.9725e-02,  1.8458e-01,  8.7418e-02,  1.5165e-02,\n",
       "                        4.6350e-02, -1.8205e-02])),\n",
       "              ('lstm.bias_hh_l1',\n",
       "               tensor([ 9.0306e-02,  1.7094e-02,  9.9887e-02,  1.3951e-01, -2.4204e-02,\n",
       "                        7.5119e-02,  1.2490e-02, -7.5981e-02,  7.4807e-02,  8.9808e-02,\n",
       "                        1.1386e-02,  1.9616e-01,  1.9025e-01,  8.6262e-02,  1.0716e-01,\n",
       "                        1.3281e-01,  1.6355e-01,  9.2094e-02,  2.6940e-01,  1.7558e-01,\n",
       "                       -1.6213e-03,  3.3033e-02, -1.0680e-02, -6.1519e-02,  2.8608e-02,\n",
       "                        5.4030e-02,  2.9198e-01,  5.1064e-02,  6.9023e-02,  2.5690e-02,\n",
       "                        3.1195e-01,  2.8038e-02,  1.0539e-01,  1.8413e-01,  1.2665e-01,\n",
       "                       -1.4313e-02,  8.8938e-02,  5.8857e-02,  4.4960e-05,  1.7191e-01,\n",
       "                        1.3345e-01,  5.2106e-02,  6.2158e-02,  6.6917e-02,  1.9181e-01,\n",
       "                        4.3943e-02,  1.0912e-01,  8.8503e-02,  1.9051e-01,  1.5170e-01,\n",
       "                        4.4271e-02,  3.1149e-02,  2.2710e-02, -5.0018e-02,  9.5905e-02,\n",
       "                        1.7287e-01,  4.7837e-02,  6.1251e-02,  6.8254e-02,  1.9776e-01,\n",
       "                        8.0977e-02,  1.5482e-01, -4.3901e-02,  2.0731e-01,  1.9729e-01,\n",
       "                        7.0020e-02,  7.6895e-02,  1.7132e-02,  1.0862e-01,  4.1669e-02,\n",
       "                       -1.1248e-01,  2.1168e-02,  5.1436e-03,  2.2823e-02,  1.4953e-01,\n",
       "                        3.2369e-02,  5.1452e-03,  1.1906e-01,  6.2421e-03,  1.1122e-01,\n",
       "                        3.4528e-02,  2.8915e-02,  1.1302e-01,  1.1406e-02,  2.8919e-02,\n",
       "                        2.7122e-03, -4.2933e-02,  1.3773e-01,  1.4910e-02,  8.5982e-02,\n",
       "                        5.3385e-02,  8.0630e-02,  4.0816e-02,  6.6850e-02,  7.7324e-02,\n",
       "                        5.0772e-02,  1.2175e-01,  1.5615e-01,  8.0594e-02,  1.3559e-01,\n",
       "                        1.9067e-01,  3.6945e-02,  4.4024e-02,  1.8655e-01,  1.2542e-01,\n",
       "                        9.9489e-02, -1.0501e-02,  1.5375e-01,  7.5101e-02,  4.1732e-03,\n",
       "                        4.4820e-02, -3.4621e-02,  2.3395e-01,  3.1006e-02,  1.0785e-01,\n",
       "                        1.4979e-01,  2.3963e-02,  1.0751e-01,  1.5392e-01,  7.6880e-02,\n",
       "                        1.6231e-02,  2.1797e-01,  7.4439e-02,  2.3965e-01,  1.3026e-01,\n",
       "                        1.0675e-01,  1.2785e-02,  9.7901e-02, -5.3980e-02,  2.4616e-02,\n",
       "                       -4.6506e-03,  9.2958e-02,  5.4741e-02, -1.0377e-01,  1.3843e-02,\n",
       "                        7.3302e-02, -2.3642e-02, -2.4458e-02,  7.2765e-03, -5.5912e-02,\n",
       "                        1.8478e-02, -2.2397e-02,  2.9600e-02, -6.6793e-02,  9.9827e-02,\n",
       "                       -7.9160e-02, -6.2730e-02,  6.6882e-02,  3.9855e-02,  2.3983e-02,\n",
       "                        2.8045e-02,  3.5118e-02,  3.8061e-02, -5.5212e-02,  5.7792e-02,\n",
       "                        9.5215e-02,  9.9036e-02, -3.4899e-03, -3.7095e-02, -1.4484e-02,\n",
       "                       -8.8241e-03, -4.0716e-02, -7.7705e-02, -6.4039e-02,  1.1701e-01,\n",
       "                        2.7808e-02,  1.9943e-02,  2.3036e-02, -1.8360e-03,  8.4813e-03,\n",
       "                        3.3951e-02,  9.3297e-02,  1.1262e-01,  7.4185e-02,  1.1347e-01,\n",
       "                       -1.5961e-01,  7.3879e-03,  5.2363e-02, -7.3258e-04, -1.2160e-02,\n",
       "                        2.6051e-02, -2.5774e-02,  7.4535e-02, -6.0121e-02, -3.5479e-02,\n",
       "                        6.8602e-02, -1.0444e-01,  5.3559e-02,  1.9218e-02,  6.1321e-02,\n",
       "                       -9.2196e-02, -3.6621e-02, -3.3925e-04, -1.6873e-02,  2.9117e-02,\n",
       "                        4.2098e-02, -1.0505e-01,  6.4519e-02, -3.6517e-02,  8.3440e-02,\n",
       "                       -1.6121e-02,  4.6655e-02,  2.9838e-02,  2.9320e-02, -2.9377e-02,\n",
       "                        1.3611e-01,  1.8989e-02, -7.5662e-03, -1.5489e-02,  1.7455e-03,\n",
       "                       -5.9683e-02, -1.0456e-01,  7.1423e-02, -3.1532e-02,  4.0320e-03,\n",
       "                        4.4075e-02,  1.0703e-01,  8.0755e-02,  1.7926e-02,  1.7474e-02,\n",
       "                        4.3126e-02, -5.6794e-02,  6.2459e-02, -4.4897e-03, -1.1873e-02,\n",
       "                        9.0502e-02,  7.5967e-02, -6.9316e-02, -1.4325e-02, -3.3250e-02,\n",
       "                       -7.1117e-03,  6.5039e-02,  4.1021e-02,  4.1141e-02, -4.3281e-02,\n",
       "                        1.2447e-02,  2.1185e-02,  1.0232e-02,  4.6518e-02, -7.7944e-02,\n",
       "                        4.5760e-02, -1.2897e-02,  3.1095e-02, -2.2867e-02,  3.2585e-03,\n",
       "                        4.3312e-02,  6.4020e-02,  8.4034e-02, -2.8457e-02,  1.0061e-01,\n",
       "                        3.9762e-02, -5.2385e-03,  3.4452e-02, -8.4318e-02, -5.2230e-02,\n",
       "                        3.2967e-02,  9.2766e-02, -4.3392e-03, -8.0793e-02, -2.0458e-02,\n",
       "                       -2.9823e-02,  9.9981e-02,  2.2491e-02, -1.1943e-02, -1.0395e-01,\n",
       "                        2.3126e-02, -8.0992e-02,  6.1947e-02, -1.0388e-01,  1.0274e-02,\n",
       "                        1.1792e-01,  5.9597e-02,  1.0406e-01, -3.9341e-03, -7.5383e-02,\n",
       "                        2.9002e-02,  1.0447e-01, -8.4016e-03,  2.2541e-02,  6.0685e-02,\n",
       "                       -7.8032e-02, -9.7053e-02,  7.1960e-02,  1.3628e-01, -1.1388e-01,\n",
       "                        2.1975e-02, -9.0767e-02,  7.8348e-02,  3.1152e-02,  5.0044e-02,\n",
       "                       -8.7696e-04,  3.9077e-04, -7.3742e-02, -1.0593e-01, -3.7250e-02,\n",
       "                       -9.7150e-02, -2.0814e-03, -1.0614e-02, -9.3378e-02,  7.7040e-02,\n",
       "                        4.6710e-02, -5.1358e-02, -7.0404e-02,  2.4869e-02,  5.8008e-02,\n",
       "                        6.8007e-02, -6.0841e-02, -2.8308e-02,  3.1097e-02, -4.4227e-02,\n",
       "                        5.1487e-02,  4.3723e-02, -2.5640e-02, -3.0081e-02,  5.8131e-02,\n",
       "                       -1.1391e-01, -5.3845e-02,  7.9641e-03,  6.1820e-02,  1.0936e-01,\n",
       "                       -9.0915e-02, -6.7558e-02,  2.7749e-02, -9.7187e-02, -7.0104e-02,\n",
       "                        1.3425e-01, -9.5740e-02, -6.5165e-02,  6.8054e-02,  1.4181e-02,\n",
       "                       -1.5903e-02,  4.0440e-03, -9.7320e-02, -1.1877e-02, -3.7003e-02,\n",
       "                       -1.9924e-02,  8.9865e-02, -7.4259e-02,  5.9026e-02, -1.0815e-01,\n",
       "                        5.4196e-02,  3.6487e-02, -5.4250e-02,  1.2471e-02, -4.4188e-02,\n",
       "                        7.0113e-03,  5.9778e-02,  9.8156e-02, -4.9527e-02,  5.6446e-02,\n",
       "                       -6.1404e-02, -7.9071e-02,  3.7044e-02, -9.4924e-02,  9.1242e-02,\n",
       "                        9.6168e-02,  3.0422e-02,  6.1796e-02, -1.2430e-03,  1.3021e-01,\n",
       "                       -7.2644e-02,  1.5586e-02, -1.1097e-02, -3.0342e-02, -7.9509e-04,\n",
       "                       -1.0551e-02,  9.7955e-02,  1.1881e-03,  6.7032e-02,  2.8638e-02,\n",
       "                       -6.4725e-02,  4.2364e-03,  1.9591e-02,  4.3190e-02, -5.8845e-02,\n",
       "                       -5.0668e-02,  1.2903e-02, -1.0413e-02, -7.7973e-02,  1.7304e-02,\n",
       "                       -1.9310e-02,  2.8767e-02, -1.0435e-01,  2.9136e-02, -1.5012e-02,\n",
       "                        9.7570e-03,  2.2046e-02,  6.5974e-02, -2.0599e-02,  6.2948e-02,\n",
       "                        1.5437e-02,  2.9809e-03,  1.2297e-01,  1.6790e-01, -1.4244e-02,\n",
       "                        1.2648e-02, -5.9924e-03,  2.3884e-02,  5.1184e-02,  2.0203e-02,\n",
       "                        5.2735e-02, -1.0165e-03,  1.4482e-02,  6.4302e-02, -6.7812e-03,\n",
       "                        1.0808e-01,  8.8941e-02,  5.5377e-02, -1.3929e-02,  2.7497e-02,\n",
       "                        8.4092e-02, -6.5964e-02,  3.7056e-02,  1.0352e-01,  6.6277e-02,\n",
       "                        6.3377e-02,  1.4048e-01,  1.4715e-02,  8.7493e-02,  4.2208e-03,\n",
       "                       -3.4607e-02,  1.1637e-01,  1.9501e-01,  4.5639e-02,  2.1605e-02,\n",
       "                        4.8080e-03,  6.5626e-03, -3.4929e-02, -2.0457e-02,  4.3401e-02,\n",
       "                        1.1979e-01,  9.7205e-02,  1.0686e-01,  4.7647e-02,  9.8159e-03,\n",
       "                       -5.1674e-02,  1.1674e-01,  8.0217e-02,  2.2531e-02,  9.4657e-03,\n",
       "                        8.6200e-02,  3.5175e-02, -4.0458e-02,  2.7774e-02,  5.8423e-02,\n",
       "                        1.4231e-01, -2.4630e-02,  1.4332e-01,  7.3123e-02,  7.4745e-02,\n",
       "                        4.5100e-02,  5.8556e-02,  4.0418e-02,  9.2948e-02,  8.6930e-02,\n",
       "                        6.2950e-03,  1.2643e-01, -2.7886e-02,  1.2782e-01,  1.0357e-01,\n",
       "                       -3.8925e-02,  9.7516e-02, -3.1496e-02, -3.4639e-03,  2.1022e-01,\n",
       "                        9.9983e-02,  1.1520e-01,  8.4996e-02, -3.3186e-02,  5.2326e-02,\n",
       "                       -5.6765e-02,  1.0423e-01, -5.5266e-02,  5.8339e-02,  5.7338e-02,\n",
       "                       -1.5782e-02,  1.3789e-01,  1.2805e-02,  1.3667e-01,  1.0673e-01,\n",
       "                        2.4642e-02,  8.3314e-02,  5.6064e-03, -1.1038e-02,  8.9887e-02,\n",
       "                        1.4178e-01,  5.4801e-02,  1.1435e-02,  3.2871e-02,  1.5113e-02,\n",
       "                       -4.1913e-02,  1.1627e-01,  8.2639e-02,  8.7082e-02,  1.4304e-01,\n",
       "                        3.1432e-02,  1.6130e-03, -6.6696e-02,  1.8430e-02,  8.9554e-02,\n",
       "                        1.4830e-01,  4.2704e-02,  5.6304e-02,  4.5770e-02,  8.6187e-02,\n",
       "                        1.2852e-01,  2.6577e-02,  1.2249e-01,  8.5302e-02,  2.4447e-02,\n",
       "                       -5.7985e-02,  5.6544e-02])),\n",
       "              ('fc.weight',\n",
       "               tensor([[-0.2726, -0.2703, -0.2543,  ...,  0.1402,  0.1597, -0.2377],\n",
       "                       [-0.1147, -0.2239, -0.1432,  ...,  0.0700,  0.1353, -0.2953],\n",
       "                       [ 0.3954, -0.0170, -0.0392,  ..., -0.4218, -0.0041, -0.0519],\n",
       "                       ...,\n",
       "                       [ 0.1413, -0.5260, -0.1931,  ...,  0.0137,  0.3234, -0.0105],\n",
       "                       [ 0.0367, -0.5425, -0.1131,  ...,  0.1306,  0.3351, -0.0890],\n",
       "                       [ 0.0465, -0.4813, -0.1102,  ...,  0.0871,  0.2737,  0.0178]])),\n",
       "              ('fc.bias',\n",
       "               tensor([-0.0981,  0.0549,  0.0822, -0.0315, -0.1586, -0.1178,  0.0666, -0.0550,\n",
       "                       -0.2345,  0.0537, -0.0246, -0.0109,  0.0745,  0.0072, -0.0918, -0.0056,\n",
       "                       -0.0762,  0.0124, -0.1270, -0.0276, -0.2338, -0.1840, -0.0128, -0.0014,\n",
       "                       -0.1959, -0.0151,  0.0713, -0.0450, -0.0788, -0.0623, -0.0131,  0.0033,\n",
       "                       -0.0590,  0.0296,  0.0270,  0.0580, -0.0320,  0.0615, -0.1459,  0.0302,\n",
       "                       -0.0439, -0.0701, -0.1711,  0.0988,  0.0211, -0.0610,  0.1116, -0.0003,\n",
       "                       -0.0191,  0.0469,  0.0319, -0.2177, -0.1377, -0.0940,  0.0023,  0.0915,\n",
       "                       -0.0151,  0.0673, -0.0011, -0.0070, -0.0029, -0.2144, -0.0322, -0.0504,\n",
       "                        0.1062, -0.1039,  0.0074, -0.0874, -0.1800,  0.0046, -0.0700,  0.0535,\n",
       "                        0.0059, -0.2101,  0.0836, -0.0811, -0.1091,  0.0656, -0.1048,  0.0257,\n",
       "                        0.0305, -0.0795, -0.0312,  0.0870, -0.0362, -0.0730, -0.0205,  0.0670,\n",
       "                       -0.0157, -0.1280, -0.1300, -0.2434, -0.1371, -0.1086, -0.2034, -0.1722,\n",
       "                       -0.2519, -0.2672, -0.1683, -0.2151]))]),\n",
       " 'optimizer_state_dict': {'state': {0: {'step': 65292,\n",
       "    'exp_avg': tensor([[-1.9600e-09, -5.5087e-08, -1.1137e-06,  ...,  0.0000e+00,\n",
       "              0.0000e+00,  0.0000e+00],\n",
       "            [ 1.6334e-09, -8.5104e-09,  2.9660e-07,  ...,  0.0000e+00,\n",
       "              0.0000e+00,  0.0000e+00],\n",
       "            [ 4.5186e-09, -2.2112e-07, -4.9987e-06,  ...,  0.0000e+00,\n",
       "              0.0000e+00,  0.0000e+00],\n",
       "            ...,\n",
       "            [ 9.5022e-09,  1.9303e-06, -9.5410e-06,  ...,  0.0000e+00,\n",
       "              0.0000e+00,  0.0000e+00],\n",
       "            [-1.3435e-09,  2.4879e-08, -3.8030e-05,  ...,  0.0000e+00,\n",
       "              0.0000e+00,  0.0000e+00],\n",
       "            [-1.2617e-08,  1.0462e-07,  7.0606e-05,  ...,  0.0000e+00,\n",
       "              0.0000e+00,  0.0000e+00]]),\n",
       "    'exp_avg_sq': tensor([[2.5763e-14, 2.0837e-14, 3.1980e-11,  ..., 0.0000e+00, 0.0000e+00,\n",
       "             0.0000e+00],\n",
       "            [2.1658e-13, 2.2787e-13, 5.3258e-12,  ..., 0.0000e+00, 0.0000e+00,\n",
       "             0.0000e+00],\n",
       "            [4.2552e-13, 3.4441e-13, 2.7042e-09,  ..., 0.0000e+00, 0.0000e+00,\n",
       "             0.0000e+00],\n",
       "            ...,\n",
       "            [2.5594e-11, 1.7281e-11, 2.4363e-08,  ..., 0.0000e+00, 0.0000e+00,\n",
       "             0.0000e+00],\n",
       "            [6.8902e-12, 2.7891e-12, 8.3028e-09,  ..., 0.0000e+00, 0.0000e+00,\n",
       "             0.0000e+00],\n",
       "            [2.0123e-11, 2.4251e-12, 2.9569e-08,  ..., 0.0000e+00, 0.0000e+00,\n",
       "             0.0000e+00]])},\n",
       "   1: {'step': 65292,\n",
       "    'exp_avg': tensor([[-2.8785e-06,  2.4882e-06,  1.8644e-06,  ..., -1.8607e-06,\n",
       "             -3.2675e-06, -4.7751e-06],\n",
       "            [-3.6923e-06, -1.0741e-06, -1.7662e-06,  ..., -1.8798e-06,\n",
       "              3.0361e-06, -4.4613e-07],\n",
       "            [-2.1654e-05, -1.5190e-05,  2.0321e-05,  ...,  4.5585e-05,\n",
       "             -2.9910e-05,  5.8551e-05],\n",
       "            ...,\n",
       "            [ 5.6731e-05, -1.4367e-06, -3.4681e-05,  ...,  3.6995e-05,\n",
       "              8.6123e-05, -1.2273e-05],\n",
       "            [ 2.0461e-06,  1.1834e-06,  7.2488e-05,  ..., -1.0113e-05,\n",
       "              3.7162e-07, -1.9020e-05],\n",
       "            [-6.8949e-05, -1.8654e-05, -4.5487e-06,  ..., -1.2998e-05,\n",
       "             -1.7130e-05, -2.4324e-05]]),\n",
       "    'exp_avg_sq': tensor([[2.4357e-10, 1.0003e-10, 1.5779e-10,  ..., 2.1693e-10, 8.4059e-11,\n",
       "             1.0235e-10],\n",
       "            [2.1226e-10, 2.2801e-10, 1.8755e-10,  ..., 1.8475e-10, 1.6666e-10,\n",
       "             1.7291e-10],\n",
       "            [6.7147e-08, 3.8863e-08, 3.4935e-08,  ..., 3.2378e-08, 3.1870e-08,\n",
       "             4.3201e-08],\n",
       "            ...,\n",
       "            [2.5060e-07, 2.6093e-07, 2.2997e-07,  ..., 2.2460e-07, 1.3551e-07,\n",
       "             7.1601e-08],\n",
       "            [3.5838e-08, 1.9601e-08, 1.8777e-08,  ..., 2.0877e-08, 2.1195e-08,\n",
       "             1.9519e-08],\n",
       "            [7.6753e-08, 1.6528e-07, 1.0195e-07,  ..., 7.2069e-08, 7.7817e-08,\n",
       "             3.7650e-08]])},\n",
       "   2: {'step': 65292,\n",
       "    'exp_avg': tensor([-1.0338e-05, -8.7320e-06, -1.3258e-05,  1.4775e-05, -9.5775e-05,\n",
       "            -9.0899e-05,  1.8065e-05, -2.0106e-04,  6.3911e-05,  1.9389e-04,\n",
       "             1.9863e-05,  1.5433e-04, -7.2651e-05,  5.0583e-05, -2.3013e-04,\n",
       "             1.6854e-04,  2.0056e-05, -9.7314e-06, -9.8745e-06, -1.3716e-04,\n",
       "            -3.3882e-04, -1.1976e-04,  2.5748e-06, -6.9876e-05,  1.8693e-04,\n",
       "             9.6619e-05,  1.8885e-05, -3.1722e-05,  2.9870e-04, -4.6453e-04,\n",
       "             1.1310e-05,  1.6299e-05,  4.2800e-06,  6.1257e-06,  1.8509e-05,\n",
       "             1.6638e-05,  6.1627e-05, -4.9635e-05, -5.7876e-05, -2.9088e-04,\n",
       "             8.4794e-05,  6.9245e-05,  3.5298e-05, -1.8211e-05, -4.5943e-05,\n",
       "             5.1056e-05, -7.9910e-05,  4.3583e-05, -1.6858e-04, -2.1418e-05,\n",
       "            -1.2769e-05,  9.2999e-05,  1.8691e-05,  2.1064e-05,  1.2848e-04,\n",
       "            -2.2918e-05,  4.1121e-05,  2.2620e-06,  3.1731e-04,  1.8579e-04,\n",
       "            -5.6996e-05, -3.2711e-06, -5.3713e-05, -1.0572e-04, -2.6905e-05,\n",
       "             1.4737e-04, -5.1475e-06,  1.4979e-04, -1.2024e-04, -2.8573e-04,\n",
       "             2.2464e-05, -3.4823e-05,  7.4365e-05,  3.3881e-04,  7.3386e-05,\n",
       "            -3.7842e-05,  2.2964e-05,  4.5577e-06, -5.0278e-05, -8.6823e-06,\n",
       "             2.0382e-04, -1.0000e-04,  1.4100e-05, -2.4631e-06, -7.0394e-05,\n",
       "            -8.6341e-05, -3.6944e-05,  4.5042e-05, -5.3712e-05,  2.2678e-06,\n",
       "             5.4710e-05,  1.8751e-04, -3.8131e-05,  4.4218e-05,  4.2046e-05,\n",
       "            -3.0624e-04, -2.0678e-05,  2.3693e-04,  3.2893e-04,  5.2202e-06,\n",
       "             1.3993e-04,  1.0935e-04, -1.0498e-05,  8.7356e-05, -3.9971e-05,\n",
       "            -6.8191e-05,  1.6824e-06, -1.3602e-04, -1.8970e-05,  1.4442e-04,\n",
       "             5.6507e-05, -4.9154e-05,  2.9609e-05, -4.7907e-05, -6.3942e-05,\n",
       "            -1.6194e-04,  8.6717e-05, -2.5522e-04, -2.4808e-04,  6.1801e-05,\n",
       "             5.5161e-06, -9.1626e-05, -9.5847e-05, -2.2075e-05, -9.6915e-05,\n",
       "             4.1630e-05, -3.7657e-05, -1.4979e-04,  3.4508e-05,  4.5852e-05,\n",
       "             2.2081e-04,  5.3769e-04, -5.3799e-05,  2.5442e-04,  2.1838e-04,\n",
       "             4.3533e-04,  2.6789e-04,  3.1909e-04, -2.0315e-04,  1.5826e-04,\n",
       "             2.1971e-04,  2.4313e-04, -6.0007e-04,  1.0068e-04,  2.2217e-04,\n",
       "             2.9822e-04, -1.8320e-04,  9.8124e-04, -2.9070e-04, -6.7584e-04,\n",
       "             3.2963e-04, -4.4145e-04,  2.6083e-04,  4.5849e-05,  9.0070e-04,\n",
       "            -2.0785e-04,  7.3770e-04,  4.2300e-04,  4.8533e-04,  3.4116e-04,\n",
       "             2.9236e-04,  1.4335e-04, -1.0877e-04, -1.7202e-04,  1.9779e-04,\n",
       "            -9.5235e-06, -8.5360e-06,  5.9889e-04, -4.1091e-05,  1.6637e-04,\n",
       "             6.3560e-04, -2.6580e-04,  3.1932e-04,  7.0250e-04, -1.5891e-04,\n",
       "             1.0794e-03,  1.0945e-04, -1.4909e-04, -1.7929e-04,  4.0550e-05,\n",
       "             2.2746e-04, -1.6150e-04, -4.8589e-04, -1.5483e-04,  5.8994e-05,\n",
       "             1.4318e-04,  7.1524e-04,  9.3883e-04, -4.3789e-04,  9.4031e-04,\n",
       "            -1.6871e-04,  5.6748e-04, -1.9587e-04,  4.1856e-04, -2.4181e-04,\n",
       "             3.8474e-04, -5.3143e-05,  3.2423e-04,  7.1109e-04,  3.1997e-04,\n",
       "             7.0865e-04, -5.6677e-04,  5.6815e-04,  6.7008e-04, -4.6059e-04,\n",
       "            -2.4377e-04, -4.9866e-04, -6.8476e-04,  4.4600e-04,  1.0791e-04,\n",
       "            -1.9894e-04,  2.7630e-05, -4.4268e-04,  8.3918e-04,  5.1330e-04,\n",
       "            -5.0142e-04, -4.1993e-04, -2.5519e-05,  2.1077e-04,  2.2080e-04,\n",
       "            -3.6012e-04, -2.2234e-04,  3.6200e-04, -5.7203e-05,  2.4682e-04,\n",
       "            -5.9080e-04,  3.2016e-04, -2.2771e-04,  8.1738e-04,  5.8245e-05,\n",
       "            -1.7765e-04,  3.8109e-04,  1.7641e-04, -3.0679e-06, -4.9361e-05,\n",
       "             4.7597e-04, -6.7022e-04,  2.3677e-04, -2.6326e-05,  3.9724e-04,\n",
       "             2.2788e-04, -4.8975e-04,  7.3271e-04,  1.8459e-04,  1.2980e-04,\n",
       "            -4.6802e-04,  2.6916e-04,  2.7369e-04, -1.6531e-04,  5.8548e-04,\n",
       "            -4.3916e-04,  6.3227e-05, -3.2079e-04,  3.3586e-04, -3.3885e-05,\n",
       "            -3.0972e-04,  1.6071e-04, -7.4181e-03,  3.9775e-03, -1.5104e-03,\n",
       "            -2.4973e-04,  1.8065e-03,  1.4848e-03,  3.7443e-03,  6.3812e-04,\n",
       "             6.1757e-03, -8.8004e-04, -4.0219e-03,  1.0168e-03, -5.3069e-04,\n",
       "             4.8343e-04, -3.3185e-03,  2.5659e-04, -5.6938e-03,  2.3364e-03,\n",
       "             1.0902e-02,  5.5073e-03, -4.0381e-03,  4.3076e-03,  2.1289e-03,\n",
       "            -1.4376e-02, -7.6192e-03, -1.4568e-02, -1.0612e-03,  1.8213e-03,\n",
       "             8.4249e-03, -2.9100e-03, -7.1668e-03, -4.5492e-03, -4.3083e-05,\n",
       "            -2.0440e-03, -5.8116e-03,  1.3792e-03, -5.2945e-04,  1.8932e-02,\n",
       "             2.8539e-03, -1.1298e-02, -4.5653e-03,  1.1120e-02,  5.0795e-03,\n",
       "             6.4514e-03,  2.1464e-03, -3.8761e-03,  1.9112e-02,  2.9781e-05,\n",
       "             2.8908e-04,  5.5872e-03,  2.9633e-03, -2.2905e-03, -1.5837e-03,\n",
       "            -2.7409e-03, -2.1614e-03,  3.1061e-04,  8.5501e-04, -3.6935e-03,\n",
       "             1.6531e-02, -3.5348e-03, -7.1712e-03,  3.4235e-03,  5.6030e-03,\n",
       "             3.1134e-04,  1.0897e-02, -5.4585e-03,  1.1823e-02, -1.3920e-03,\n",
       "             3.2801e-03,  1.7129e-02,  1.3773e-03, -3.4970e-03,  1.1971e-02,\n",
       "            -1.5894e-03, -1.0789e-02,  1.3998e-03,  2.7195e-03, -1.3888e-03,\n",
       "            -1.9989e-03,  6.8237e-03, -1.1653e-03,  5.2175e-04, -3.0836e-05,\n",
       "            -1.7771e-03,  1.6326e-03, -8.5389e-04,  2.8760e-03, -3.8109e-03,\n",
       "            -6.7458e-03, -3.4026e-04,  1.0434e-02, -1.7605e-03, -1.2297e-02,\n",
       "            -7.1637e-03,  1.2848e-03,  2.1254e-03,  8.7700e-03, -1.0357e-02,\n",
       "             3.1808e-03,  2.4263e-03,  1.2919e-03,  6.1208e-03, -3.2585e-03,\n",
       "             3.4106e-03,  3.7624e-04, -4.1274e-03, -8.4191e-04, -5.8613e-04,\n",
       "             6.0910e-04, -7.5610e-04,  1.4459e-03, -3.5347e-04,  1.3673e-03,\n",
       "            -6.4740e-03,  8.1363e-03, -7.0673e-04, -2.0948e-03,  4.3368e-03,\n",
       "             3.5538e-03, -3.3790e-03, -6.5611e-03,  6.1884e-03,  1.0256e-02,\n",
       "            -6.2280e-03, -4.2710e-03, -4.5409e-05,  7.5929e-03,  7.6355e-04,\n",
       "             2.7602e-05,  1.3456e-04,  4.5379e-04, -6.1965e-04, -1.8138e-04,\n",
       "             2.4740e-05, -1.4195e-04,  1.1391e-04,  1.8478e-04,  4.8965e-05,\n",
       "             1.2564e-04, -9.0152e-05,  3.5586e-04, -5.6811e-04,  3.1859e-05,\n",
       "             5.5019e-04, -2.0699e-05,  1.9581e-05, -2.0405e-05,  9.5854e-05,\n",
       "            -4.2020e-04, -2.9730e-05, -5.0398e-04,  1.8493e-04, -9.1665e-05,\n",
       "             1.4967e-05, -2.1484e-04,  7.3894e-04, -7.1772e-05,  6.8962e-06,\n",
       "             5.7667e-05,  4.4467e-05,  1.5248e-04,  1.4114e-04, -5.1069e-05,\n",
       "             2.9136e-04, -2.6841e-04,  1.8892e-04,  2.0643e-05,  9.4430e-05,\n",
       "             6.7324e-05, -2.7091e-06, -2.0579e-04, -3.5928e-05, -1.5019e-06,\n",
       "            -3.8082e-04,  2.1266e-05, -9.6103e-05, -6.7374e-04, -1.9975e-05,\n",
       "             1.7593e-04,  5.8488e-05, -8.0532e-05,  5.9261e-05, -3.3582e-04,\n",
       "            -3.4610e-05,  8.9488e-04,  7.4582e-04,  4.5017e-04,  5.6070e-05,\n",
       "             4.5047e-05,  8.1053e-05,  6.3445e-05, -8.6713e-04,  2.2653e-04,\n",
       "             1.4629e-06, -4.8167e-05, -1.0255e-04, -1.0787e-04,  7.6565e-06,\n",
       "            -9.0846e-05,  1.5856e-04,  2.8067e-04, -1.1695e-04, -6.6345e-05,\n",
       "             4.7945e-05, -2.0330e-05, -4.1283e-06, -4.2008e-04,  7.2780e-04,\n",
       "            -2.6203e-04,  2.5099e-04,  1.9756e-04, -1.3673e-04, -2.2939e-04,\n",
       "             1.5661e-05, -1.9076e-04, -4.5033e-04,  7.7141e-05,  1.9939e-04,\n",
       "             9.9056e-05, -1.5350e-04, -2.7592e-05,  2.0380e-05, -1.2795e-04,\n",
       "             1.7016e-04,  7.3172e-05,  1.5923e-04, -7.3426e-05,  1.5828e-04,\n",
       "            -2.1395e-05, -3.3716e-05,  8.0068e-05,  5.0773e-05, -2.7243e-04,\n",
       "             2.2711e-04, -1.5772e-04, -5.0305e-04,  8.7976e-06,  1.1088e-04,\n",
       "             3.4796e-05,  1.3624e-05, -4.5158e-04, -7.5233e-06,  1.4384e-04,\n",
       "             8.2758e-05, -9.4703e-04, -4.7666e-04,  3.7977e-05, -9.6522e-05,\n",
       "             2.4963e-05, -1.4853e-04, -3.9483e-05, -2.4979e-04,  1.9029e-04,\n",
       "            -2.1655e-05, -2.4823e-04]),\n",
       "    'exp_avg_sq': tensor([3.6686e-09, 2.2114e-09, 4.2191e-07, 3.7881e-08, 2.5806e-07, 2.3256e-07,\n",
       "            1.0163e-07, 3.4960e-07, 1.3114e-07, 2.1933e-07, 2.9471e-08, 2.0587e-07,\n",
       "            5.6045e-07, 1.3042e-07, 3.6525e-07, 1.8664e-06, 1.7129e-08, 2.3245e-07,\n",
       "            9.9829e-09, 7.1030e-07, 7.3618e-07, 5.7144e-08, 2.9726e-07, 4.6182e-07,\n",
       "            1.4607e-07, 4.5449e-07, 1.1631e-07, 5.9957e-08, 4.8691e-07, 8.0273e-07,\n",
       "            1.2223e-07, 1.3148e-07, 1.2665e-08, 4.5647e-09, 1.4591e-07, 3.4635e-08,\n",
       "            6.4978e-08, 2.1754e-07, 7.0664e-07, 4.1241e-07, 9.2709e-08, 3.4782e-07,\n",
       "            5.1604e-07, 1.2603e-07, 2.3437e-07, 2.6302e-08, 3.2880e-07, 5.8953e-08,\n",
       "            3.1303e-07, 1.1909e-08, 4.4441e-07, 1.6685e-07, 1.8561e-08, 2.9479e-08,\n",
       "            9.1770e-08, 1.0390e-07, 8.8086e-08, 2.6991e-08, 5.7551e-07, 3.6955e-07,\n",
       "            7.7887e-07, 2.1035e-07, 1.1190e-07, 2.1107e-07, 5.2548e-09, 1.7540e-07,\n",
       "            2.3230e-08, 2.8776e-07, 1.6539e-07, 8.9060e-07, 2.0287e-07, 2.6335e-07,\n",
       "            3.4053e-07, 1.5501e-06, 1.1251e-07, 2.7581e-07, 3.7735e-08, 2.1274e-07,\n",
       "            4.1117e-07, 1.5672e-09, 1.5810e-07, 7.5880e-08, 9.5180e-08, 2.2773e-09,\n",
       "            4.3428e-08, 6.8135e-08, 1.3274e-07, 5.3321e-07, 8.3288e-08, 5.5580e-07,\n",
       "            1.1849e-06, 5.4859e-07, 6.0554e-07, 1.2707e-07, 2.8951e-07, 1.2950e-06,\n",
       "            3.3469e-08, 2.8100e-07, 1.1198e-06, 1.1672e-08, 2.7784e-07, 1.4926e-07,\n",
       "            7.5881e-08, 9.4780e-07, 5.7425e-08, 8.4406e-08, 2.8481e-07, 3.8246e-07,\n",
       "            1.9820e-08, 3.2227e-07, 2.0837e-07, 1.0112e-07, 2.7720e-07, 1.8634e-08,\n",
       "            3.3577e-07, 2.1580e-07, 2.2732e-07, 6.6628e-07, 7.5202e-07, 1.0102e-07,\n",
       "            8.8837e-08, 5.4226e-08, 6.1524e-07, 2.7465e-07, 4.3255e-07, 2.5270e-07,\n",
       "            2.3642e-07, 1.1520e-07, 4.9354e-07, 1.7103e-06, 1.8359e-06, 1.9875e-06,\n",
       "            2.7881e-06, 1.4308e-06, 2.4343e-06, 4.1576e-06, 2.1509e-06, 2.2513e-06,\n",
       "            2.2078e-06, 3.9079e-06, 2.6322e-06, 3.9569e-06, 1.5133e-06, 4.2579e-06,\n",
       "            1.0191e-06, 2.7842e-06, 1.8220e-06, 3.0215e-06, 1.9259e-06, 2.4160e-06,\n",
       "            2.2105e-06, 2.2392e-06, 3.6559e-06, 2.7150e-06, 4.2639e-06, 2.0047e-06,\n",
       "            1.9110e-06, 4.6686e-06, 2.4667e-06, 1.1284e-06, 1.1062e-06, 1.2775e-06,\n",
       "            4.3169e-06, 1.2698e-06, 1.8841e-06, 1.1659e-06, 6.3298e-06, 1.1832e-06,\n",
       "            2.9758e-06, 1.9377e-06, 5.9861e-06, 2.9487e-06, 1.1865e-06, 1.6613e-06,\n",
       "            1.9350e-06, 3.9300e-06, 1.4442e-06, 6.6425e-07, 3.0422e-06, 1.4496e-06,\n",
       "            1.4087e-06, 1.1417e-06, 2.1680e-06, 3.0983e-06, 1.2633e-06, 9.5910e-07,\n",
       "            4.5012e-06, 8.3002e-06, 1.6780e-06, 3.8050e-06, 1.6244e-06, 2.1416e-06,\n",
       "            2.2736e-07, 2.5385e-06, 2.1235e-06, 2.5520e-06, 1.1674e-06, 2.7643e-06,\n",
       "            5.0734e-06, 4.3231e-06, 2.5791e-06, 2.0734e-05, 2.7392e-06, 2.5687e-06,\n",
       "            3.3205e-06, 2.1932e-06, 4.0250e-06, 1.7859e-06, 3.0064e-06, 1.3263e-06,\n",
       "            2.9813e-06, 1.1957e-07, 1.3383e-06, 3.5855e-06, 3.1838e-06, 1.7797e-06,\n",
       "            1.6148e-06, 2.9734e-06, 3.4869e-06, 2.6141e-06, 4.6217e-06, 4.6338e-06,\n",
       "            2.7550e-06, 1.8720e-06, 1.0892e-06, 1.7849e-06, 1.5481e-06, 4.6484e-06,\n",
       "            2.2420e-06, 1.7036e-06, 1.5053e-06, 3.6001e-06, 2.0016e-06, 1.3065e-06,\n",
       "            1.8127e-06, 3.9752e-06, 3.8543e-06, 1.8685e-06, 1.3928e-06, 3.4605e-06,\n",
       "            2.0342e-06, 1.1699e-06, 1.8127e-06, 3.0984e-06, 1.1609e-06, 4.2516e-06,\n",
       "            2.2220e-06, 2.0566e-06, 1.2432e-06, 1.3093e-06, 3.3100e-06, 1.4008e-06,\n",
       "            3.2123e-06, 2.4732e-06, 7.7934e-07, 1.9738e-06, 1.8861e-06, 3.8455e-04,\n",
       "            2.4781e-04, 1.2490e-05, 8.9563e-05, 2.0237e-04, 8.0638e-04, 4.2700e-04,\n",
       "            5.9640e-04, 2.2612e-04, 8.7100e-05, 5.2730e-04, 2.8609e-04, 2.0490e-04,\n",
       "            2.2342e-04, 8.2800e-04, 8.1541e-06, 3.6565e-04, 2.1430e-05, 5.8379e-04,\n",
       "            5.5338e-04, 8.5901e-05, 2.2460e-04, 1.5266e-04, 1.5321e-03, 6.5377e-04,\n",
       "            6.8677e-04, 3.0527e-04, 2.8477e-04, 6.9686e-04, 7.1372e-04, 2.3817e-04,\n",
       "            3.2504e-04, 6.0107e-06, 1.3788e-03, 2.9682e-04, 2.5962e-05, 1.2409e-04,\n",
       "            2.0594e-03, 2.1820e-04, 7.0879e-04, 1.5674e-04, 1.3563e-03, 1.1396e-04,\n",
       "            3.4192e-04, 2.3339e-04, 1.4442e-04, 1.1072e-03, 4.3490e-04, 2.2343e-06,\n",
       "            8.3108e-04, 1.6907e-04, 5.2102e-04, 2.0280e-04, 7.3318e-04, 1.4954e-04,\n",
       "            2.7894e-04, 5.3224e-06, 3.4813e-04, 3.3988e-03, 4.7187e-04, 3.7380e-04,\n",
       "            2.6906e-04, 1.1312e-04, 7.8713e-07, 5.2323e-04, 4.3466e-04, 5.2161e-04,\n",
       "            3.8296e-04, 7.5006e-04, 1.9706e-03, 1.4398e-03, 3.1516e-04, 3.4761e-03,\n",
       "            1.3554e-04, 6.4900e-04, 1.1808e-04, 2.3262e-04, 1.6919e-04, 1.1958e-05,\n",
       "            4.8718e-04, 3.6956e-04, 7.7046e-05, 3.4941e-07, 3.8754e-04, 3.5875e-04,\n",
       "            4.4794e-04, 5.1378e-04, 1.9689e-04, 4.4260e-04, 5.7397e-05, 6.6734e-04,\n",
       "            6.4032e-04, 7.2968e-04, 4.4114e-04, 3.0681e-04, 1.8749e-04, 5.8185e-04,\n",
       "            2.8619e-04, 5.4196e-04, 5.1029e-04, 4.0055e-04, 2.0445e-04, 2.5730e-04,\n",
       "            2.3364e-04, 6.5648e-05, 2.9506e-04, 4.3393e-04, 8.9549e-05, 6.0978e-04,\n",
       "            3.5490e-04, 8.1189e-04, 2.8546e-04, 1.0447e-05, 4.1452e-04, 3.9781e-04,\n",
       "            2.0816e-04, 5.8840e-05, 3.2359e-04, 5.6089e-04, 2.7331e-04, 3.5502e-04,\n",
       "            7.8984e-04, 4.8911e-04, 3.7948e-04, 2.7251e-04, 2.4949e-04, 4.9855e-04,\n",
       "            4.7034e-06, 1.8688e-07, 5.9232e-07, 7.0310e-06, 5.0177e-06, 3.4399e-07,\n",
       "            1.7362e-07, 1.6940e-07, 4.3506e-07, 1.3116e-06, 2.3810e-06, 2.0995e-07,\n",
       "            1.9922e-06, 1.9565e-06, 2.2064e-06, 1.9355e-06, 2.5452e-06, 1.1243e-06,\n",
       "            4.7326e-06, 2.9119e-07, 6.9239e-07, 2.0111e-06, 1.6521e-06, 2.2261e-06,\n",
       "            2.8971e-07, 1.5316e-07, 1.4706e-07, 3.4064e-07, 1.0971e-06, 1.9328e-07,\n",
       "            1.2238e-07, 8.6950e-07, 1.6509e-07, 2.9434e-06, 3.1160e-07, 5.4940e-07,\n",
       "            4.5991e-06, 9.4782e-07, 2.7146e-07, 2.1829e-07, 2.1068e-07, 1.0562e-06,\n",
       "            3.8459e-07, 2.5948e-06, 1.7275e-07, 5.1410e-07, 1.4283e-06, 2.5661e-07,\n",
       "            3.8509e-07, 7.7057e-06, 2.1512e-07, 7.2185e-07, 2.7456e-07, 1.5581e-07,\n",
       "            4.2357e-08, 2.1498e-06, 5.2958e-07, 1.5002e-05, 2.3609e-06, 7.8297e-07,\n",
       "            4.3108e-07, 1.3900e-06, 1.8180e-07, 6.4253e-07, 4.0462e-06, 3.6790e-07,\n",
       "            2.7819e-07, 7.0853e-08, 3.4129e-07, 7.8507e-07, 1.4538e-07, 3.1228e-07,\n",
       "            1.4141e-06, 3.2460e-06, 1.8198e-06, 5.7377e-07, 2.1641e-06, 1.5161e-06,\n",
       "            3.4242e-06, 5.7733e-06, 2.1476e-06, 3.8429e-07, 2.9872e-06, 5.8801e-06,\n",
       "            2.9112e-07, 4.3496e-07, 3.0318e-07, 2.9272e-07, 1.1668e-06, 5.5122e-08,\n",
       "            5.0697e-06, 3.4652e-07, 1.4436e-06, 1.2826e-07, 2.5729e-07, 1.0553e-06,\n",
       "            3.9644e-07, 1.2591e-07, 1.5288e-06, 5.6312e-07, 2.0300e-07, 4.3708e-08,\n",
       "            6.6469e-07, 2.6970e-06, 1.1861e-06, 2.4317e-06, 5.0020e-07, 2.9307e-06,\n",
       "            5.7081e-06, 3.1538e-07, 5.8896e-07, 4.6791e-08, 5.5599e-07, 4.7297e-06,\n",
       "            4.4683e-07, 1.0591e-06, 4.0083e-07, 6.8550e-06, 1.2301e-06, 3.3204e-07,\n",
       "            2.6506e-07, 1.4117e-07, 8.4014e-07, 2.5192e-07, 1.8525e-06, 2.0136e-06,\n",
       "            2.3765e-07, 8.9873e-07])},\n",
       "   3: {'step': 65292,\n",
       "    'exp_avg': tensor([-1.0338e-05, -8.7320e-06, -1.3258e-05,  1.4775e-05, -9.5775e-05,\n",
       "            -9.0899e-05,  1.8065e-05, -2.0106e-04,  6.3911e-05,  1.9389e-04,\n",
       "             1.9863e-05,  1.5433e-04, -7.2651e-05,  5.0583e-05, -2.3013e-04,\n",
       "             1.6854e-04,  2.0056e-05, -9.7314e-06, -9.8745e-06, -1.3716e-04,\n",
       "            -3.3882e-04, -1.1976e-04,  2.5748e-06, -6.9876e-05,  1.8693e-04,\n",
       "             9.6619e-05,  1.8885e-05, -3.1722e-05,  2.9870e-04, -4.6453e-04,\n",
       "             1.1310e-05,  1.6299e-05,  4.2800e-06,  6.1257e-06,  1.8509e-05,\n",
       "             1.6638e-05,  6.1627e-05, -4.9635e-05, -5.7876e-05, -2.9088e-04,\n",
       "             8.4794e-05,  6.9245e-05,  3.5298e-05, -1.8211e-05, -4.5943e-05,\n",
       "             5.1056e-05, -7.9910e-05,  4.3583e-05, -1.6858e-04, -2.1418e-05,\n",
       "            -1.2769e-05,  9.2999e-05,  1.8691e-05,  2.1064e-05,  1.2848e-04,\n",
       "            -2.2918e-05,  4.1121e-05,  2.2620e-06,  3.1731e-04,  1.8579e-04,\n",
       "            -5.6996e-05, -3.2711e-06, -5.3713e-05, -1.0572e-04, -2.6905e-05,\n",
       "             1.4737e-04, -5.1475e-06,  1.4979e-04, -1.2024e-04, -2.8573e-04,\n",
       "             2.2464e-05, -3.4823e-05,  7.4365e-05,  3.3881e-04,  7.3386e-05,\n",
       "            -3.7842e-05,  2.2964e-05,  4.5577e-06, -5.0278e-05, -8.6823e-06,\n",
       "             2.0382e-04, -1.0000e-04,  1.4100e-05, -2.4631e-06, -7.0394e-05,\n",
       "            -8.6341e-05, -3.6944e-05,  4.5042e-05, -5.3712e-05,  2.2678e-06,\n",
       "             5.4710e-05,  1.8751e-04, -3.8131e-05,  4.4218e-05,  4.2046e-05,\n",
       "            -3.0624e-04, -2.0678e-05,  2.3693e-04,  3.2893e-04,  5.2202e-06,\n",
       "             1.3993e-04,  1.0935e-04, -1.0498e-05,  8.7356e-05, -3.9971e-05,\n",
       "            -6.8191e-05,  1.6824e-06, -1.3602e-04, -1.8970e-05,  1.4442e-04,\n",
       "             5.6507e-05, -4.9154e-05,  2.9609e-05, -4.7907e-05, -6.3942e-05,\n",
       "            -1.6194e-04,  8.6717e-05, -2.5522e-04, -2.4808e-04,  6.1801e-05,\n",
       "             5.5161e-06, -9.1626e-05, -9.5847e-05, -2.2075e-05, -9.6915e-05,\n",
       "             4.1630e-05, -3.7657e-05, -1.4979e-04,  3.4508e-05,  4.5852e-05,\n",
       "             2.2081e-04,  5.3769e-04, -5.3799e-05,  2.5442e-04,  2.1838e-04,\n",
       "             4.3533e-04,  2.6789e-04,  3.1909e-04, -2.0315e-04,  1.5826e-04,\n",
       "             2.1971e-04,  2.4313e-04, -6.0007e-04,  1.0068e-04,  2.2217e-04,\n",
       "             2.9822e-04, -1.8320e-04,  9.8124e-04, -2.9070e-04, -6.7584e-04,\n",
       "             3.2963e-04, -4.4145e-04,  2.6083e-04,  4.5849e-05,  9.0070e-04,\n",
       "            -2.0785e-04,  7.3770e-04,  4.2300e-04,  4.8533e-04,  3.4116e-04,\n",
       "             2.9236e-04,  1.4335e-04, -1.0877e-04, -1.7202e-04,  1.9779e-04,\n",
       "            -9.5235e-06, -8.5360e-06,  5.9889e-04, -4.1091e-05,  1.6637e-04,\n",
       "             6.3560e-04, -2.6580e-04,  3.1932e-04,  7.0250e-04, -1.5891e-04,\n",
       "             1.0794e-03,  1.0945e-04, -1.4909e-04, -1.7929e-04,  4.0550e-05,\n",
       "             2.2746e-04, -1.6150e-04, -4.8589e-04, -1.5483e-04,  5.8994e-05,\n",
       "             1.4318e-04,  7.1524e-04,  9.3883e-04, -4.3789e-04,  9.4031e-04,\n",
       "            -1.6871e-04,  5.6748e-04, -1.9587e-04,  4.1856e-04, -2.4181e-04,\n",
       "             3.8474e-04, -5.3143e-05,  3.2423e-04,  7.1109e-04,  3.1997e-04,\n",
       "             7.0865e-04, -5.6677e-04,  5.6815e-04,  6.7008e-04, -4.6059e-04,\n",
       "            -2.4377e-04, -4.9866e-04, -6.8476e-04,  4.4600e-04,  1.0791e-04,\n",
       "            -1.9894e-04,  2.7630e-05, -4.4268e-04,  8.3918e-04,  5.1330e-04,\n",
       "            -5.0142e-04, -4.1993e-04, -2.5519e-05,  2.1077e-04,  2.2080e-04,\n",
       "            -3.6012e-04, -2.2234e-04,  3.6200e-04, -5.7203e-05,  2.4682e-04,\n",
       "            -5.9080e-04,  3.2016e-04, -2.2771e-04,  8.1738e-04,  5.8245e-05,\n",
       "            -1.7765e-04,  3.8109e-04,  1.7641e-04, -3.0679e-06, -4.9361e-05,\n",
       "             4.7597e-04, -6.7022e-04,  2.3677e-04, -2.6326e-05,  3.9724e-04,\n",
       "             2.2788e-04, -4.8975e-04,  7.3271e-04,  1.8459e-04,  1.2980e-04,\n",
       "            -4.6802e-04,  2.6916e-04,  2.7369e-04, -1.6531e-04,  5.8548e-04,\n",
       "            -4.3916e-04,  6.3227e-05, -3.2079e-04,  3.3586e-04, -3.3885e-05,\n",
       "            -3.0972e-04,  1.6071e-04, -7.4181e-03,  3.9775e-03, -1.5104e-03,\n",
       "            -2.4973e-04,  1.8065e-03,  1.4848e-03,  3.7443e-03,  6.3812e-04,\n",
       "             6.1757e-03, -8.8004e-04, -4.0219e-03,  1.0168e-03, -5.3069e-04,\n",
       "             4.8343e-04, -3.3185e-03,  2.5659e-04, -5.6938e-03,  2.3364e-03,\n",
       "             1.0902e-02,  5.5073e-03, -4.0381e-03,  4.3076e-03,  2.1289e-03,\n",
       "            -1.4376e-02, -7.6192e-03, -1.4568e-02, -1.0612e-03,  1.8213e-03,\n",
       "             8.4249e-03, -2.9100e-03, -7.1668e-03, -4.5492e-03, -4.3083e-05,\n",
       "            -2.0440e-03, -5.8116e-03,  1.3792e-03, -5.2945e-04,  1.8932e-02,\n",
       "             2.8539e-03, -1.1298e-02, -4.5653e-03,  1.1120e-02,  5.0795e-03,\n",
       "             6.4514e-03,  2.1464e-03, -3.8761e-03,  1.9112e-02,  2.9781e-05,\n",
       "             2.8908e-04,  5.5872e-03,  2.9633e-03, -2.2905e-03, -1.5837e-03,\n",
       "            -2.7409e-03, -2.1614e-03,  3.1061e-04,  8.5501e-04, -3.6935e-03,\n",
       "             1.6531e-02, -3.5348e-03, -7.1712e-03,  3.4235e-03,  5.6030e-03,\n",
       "             3.1134e-04,  1.0897e-02, -5.4585e-03,  1.1823e-02, -1.3920e-03,\n",
       "             3.2801e-03,  1.7129e-02,  1.3773e-03, -3.4970e-03,  1.1971e-02,\n",
       "            -1.5894e-03, -1.0789e-02,  1.3998e-03,  2.7195e-03, -1.3888e-03,\n",
       "            -1.9989e-03,  6.8237e-03, -1.1653e-03,  5.2175e-04, -3.0836e-05,\n",
       "            -1.7771e-03,  1.6326e-03, -8.5389e-04,  2.8760e-03, -3.8109e-03,\n",
       "            -6.7458e-03, -3.4026e-04,  1.0434e-02, -1.7605e-03, -1.2297e-02,\n",
       "            -7.1637e-03,  1.2848e-03,  2.1254e-03,  8.7700e-03, -1.0357e-02,\n",
       "             3.1808e-03,  2.4263e-03,  1.2919e-03,  6.1208e-03, -3.2585e-03,\n",
       "             3.4106e-03,  3.7624e-04, -4.1274e-03, -8.4191e-04, -5.8613e-04,\n",
       "             6.0910e-04, -7.5610e-04,  1.4459e-03, -3.5347e-04,  1.3673e-03,\n",
       "            -6.4740e-03,  8.1363e-03, -7.0673e-04, -2.0948e-03,  4.3368e-03,\n",
       "             3.5538e-03, -3.3790e-03, -6.5611e-03,  6.1884e-03,  1.0256e-02,\n",
       "            -6.2280e-03, -4.2710e-03, -4.5409e-05,  7.5929e-03,  7.6355e-04,\n",
       "             2.7602e-05,  1.3456e-04,  4.5379e-04, -6.1965e-04, -1.8138e-04,\n",
       "             2.4740e-05, -1.4195e-04,  1.1391e-04,  1.8478e-04,  4.8965e-05,\n",
       "             1.2564e-04, -9.0152e-05,  3.5586e-04, -5.6811e-04,  3.1859e-05,\n",
       "             5.5019e-04, -2.0699e-05,  1.9581e-05, -2.0405e-05,  9.5854e-05,\n",
       "            -4.2020e-04, -2.9730e-05, -5.0398e-04,  1.8493e-04, -9.1665e-05,\n",
       "             1.4967e-05, -2.1484e-04,  7.3894e-04, -7.1772e-05,  6.8962e-06,\n",
       "             5.7667e-05,  4.4467e-05,  1.5248e-04,  1.4114e-04, -5.1069e-05,\n",
       "             2.9136e-04, -2.6841e-04,  1.8892e-04,  2.0643e-05,  9.4430e-05,\n",
       "             6.7324e-05, -2.7091e-06, -2.0579e-04, -3.5928e-05, -1.5019e-06,\n",
       "            -3.8082e-04,  2.1266e-05, -9.6103e-05, -6.7374e-04, -1.9975e-05,\n",
       "             1.7593e-04,  5.8488e-05, -8.0532e-05,  5.9261e-05, -3.3582e-04,\n",
       "            -3.4610e-05,  8.9488e-04,  7.4582e-04,  4.5017e-04,  5.6070e-05,\n",
       "             4.5047e-05,  8.1053e-05,  6.3445e-05, -8.6713e-04,  2.2653e-04,\n",
       "             1.4629e-06, -4.8167e-05, -1.0255e-04, -1.0787e-04,  7.6565e-06,\n",
       "            -9.0846e-05,  1.5856e-04,  2.8067e-04, -1.1695e-04, -6.6345e-05,\n",
       "             4.7945e-05, -2.0330e-05, -4.1283e-06, -4.2008e-04,  7.2780e-04,\n",
       "            -2.6203e-04,  2.5099e-04,  1.9756e-04, -1.3673e-04, -2.2939e-04,\n",
       "             1.5661e-05, -1.9076e-04, -4.5033e-04,  7.7141e-05,  1.9939e-04,\n",
       "             9.9056e-05, -1.5350e-04, -2.7592e-05,  2.0380e-05, -1.2795e-04,\n",
       "             1.7016e-04,  7.3172e-05,  1.5923e-04, -7.3426e-05,  1.5828e-04,\n",
       "            -2.1395e-05, -3.3716e-05,  8.0068e-05,  5.0773e-05, -2.7243e-04,\n",
       "             2.2711e-04, -1.5772e-04, -5.0305e-04,  8.7976e-06,  1.1088e-04,\n",
       "             3.4796e-05,  1.3624e-05, -4.5158e-04, -7.5233e-06,  1.4384e-04,\n",
       "             8.2758e-05, -9.4703e-04, -4.7666e-04,  3.7977e-05, -9.6522e-05,\n",
       "             2.4963e-05, -1.4853e-04, -3.9483e-05, -2.4979e-04,  1.9029e-04,\n",
       "            -2.1655e-05, -2.4823e-04]),\n",
       "    'exp_avg_sq': tensor([3.6686e-09, 2.2114e-09, 4.2191e-07, 3.7881e-08, 2.5806e-07, 2.3256e-07,\n",
       "            1.0163e-07, 3.4960e-07, 1.3114e-07, 2.1933e-07, 2.9471e-08, 2.0587e-07,\n",
       "            5.6045e-07, 1.3042e-07, 3.6525e-07, 1.8664e-06, 1.7129e-08, 2.3245e-07,\n",
       "            9.9829e-09, 7.1030e-07, 7.3618e-07, 5.7144e-08, 2.9726e-07, 4.6182e-07,\n",
       "            1.4607e-07, 4.5449e-07, 1.1631e-07, 5.9957e-08, 4.8691e-07, 8.0273e-07,\n",
       "            1.2223e-07, 1.3148e-07, 1.2665e-08, 4.5647e-09, 1.4591e-07, 3.4635e-08,\n",
       "            6.4978e-08, 2.1754e-07, 7.0664e-07, 4.1241e-07, 9.2709e-08, 3.4782e-07,\n",
       "            5.1604e-07, 1.2603e-07, 2.3437e-07, 2.6302e-08, 3.2880e-07, 5.8953e-08,\n",
       "            3.1303e-07, 1.1909e-08, 4.4441e-07, 1.6685e-07, 1.8561e-08, 2.9479e-08,\n",
       "            9.1770e-08, 1.0390e-07, 8.8086e-08, 2.6991e-08, 5.7551e-07, 3.6955e-07,\n",
       "            7.7887e-07, 2.1035e-07, 1.1190e-07, 2.1107e-07, 5.2548e-09, 1.7540e-07,\n",
       "            2.3230e-08, 2.8776e-07, 1.6539e-07, 8.9060e-07, 2.0287e-07, 2.6335e-07,\n",
       "            3.4053e-07, 1.5501e-06, 1.1251e-07, 2.7581e-07, 3.7735e-08, 2.1274e-07,\n",
       "            4.1117e-07, 1.5672e-09, 1.5810e-07, 7.5880e-08, 9.5180e-08, 2.2773e-09,\n",
       "            4.3428e-08, 6.8135e-08, 1.3274e-07, 5.3321e-07, 8.3288e-08, 5.5580e-07,\n",
       "            1.1849e-06, 5.4859e-07, 6.0554e-07, 1.2707e-07, 2.8951e-07, 1.2950e-06,\n",
       "            3.3469e-08, 2.8100e-07, 1.1198e-06, 1.1672e-08, 2.7784e-07, 1.4926e-07,\n",
       "            7.5881e-08, 9.4780e-07, 5.7425e-08, 8.4406e-08, 2.8481e-07, 3.8246e-07,\n",
       "            1.9820e-08, 3.2227e-07, 2.0837e-07, 1.0112e-07, 2.7720e-07, 1.8634e-08,\n",
       "            3.3577e-07, 2.1580e-07, 2.2732e-07, 6.6628e-07, 7.5202e-07, 1.0102e-07,\n",
       "            8.8837e-08, 5.4226e-08, 6.1524e-07, 2.7465e-07, 4.3255e-07, 2.5270e-07,\n",
       "            2.3642e-07, 1.1520e-07, 4.9354e-07, 1.7103e-06, 1.8359e-06, 1.9875e-06,\n",
       "            2.7881e-06, 1.4308e-06, 2.4343e-06, 4.1576e-06, 2.1509e-06, 2.2513e-06,\n",
       "            2.2078e-06, 3.9079e-06, 2.6322e-06, 3.9569e-06, 1.5133e-06, 4.2579e-06,\n",
       "            1.0191e-06, 2.7842e-06, 1.8220e-06, 3.0215e-06, 1.9259e-06, 2.4160e-06,\n",
       "            2.2105e-06, 2.2392e-06, 3.6559e-06, 2.7150e-06, 4.2639e-06, 2.0047e-06,\n",
       "            1.9110e-06, 4.6686e-06, 2.4667e-06, 1.1284e-06, 1.1062e-06, 1.2775e-06,\n",
       "            4.3169e-06, 1.2698e-06, 1.8841e-06, 1.1659e-06, 6.3298e-06, 1.1832e-06,\n",
       "            2.9758e-06, 1.9377e-06, 5.9861e-06, 2.9487e-06, 1.1865e-06, 1.6613e-06,\n",
       "            1.9350e-06, 3.9300e-06, 1.4442e-06, 6.6425e-07, 3.0422e-06, 1.4496e-06,\n",
       "            1.4087e-06, 1.1417e-06, 2.1680e-06, 3.0983e-06, 1.2633e-06, 9.5910e-07,\n",
       "            4.5012e-06, 8.3002e-06, 1.6780e-06, 3.8050e-06, 1.6244e-06, 2.1416e-06,\n",
       "            2.2736e-07, 2.5385e-06, 2.1235e-06, 2.5520e-06, 1.1674e-06, 2.7643e-06,\n",
       "            5.0734e-06, 4.3231e-06, 2.5791e-06, 2.0734e-05, 2.7392e-06, 2.5687e-06,\n",
       "            3.3205e-06, 2.1932e-06, 4.0250e-06, 1.7859e-06, 3.0064e-06, 1.3263e-06,\n",
       "            2.9813e-06, 1.1957e-07, 1.3383e-06, 3.5855e-06, 3.1838e-06, 1.7797e-06,\n",
       "            1.6148e-06, 2.9734e-06, 3.4869e-06, 2.6141e-06, 4.6217e-06, 4.6338e-06,\n",
       "            2.7550e-06, 1.8720e-06, 1.0892e-06, 1.7849e-06, 1.5481e-06, 4.6484e-06,\n",
       "            2.2420e-06, 1.7036e-06, 1.5053e-06, 3.6001e-06, 2.0016e-06, 1.3065e-06,\n",
       "            1.8127e-06, 3.9752e-06, 3.8543e-06, 1.8685e-06, 1.3928e-06, 3.4605e-06,\n",
       "            2.0342e-06, 1.1699e-06, 1.8127e-06, 3.0984e-06, 1.1609e-06, 4.2516e-06,\n",
       "            2.2220e-06, 2.0566e-06, 1.2432e-06, 1.3093e-06, 3.3100e-06, 1.4008e-06,\n",
       "            3.2123e-06, 2.4732e-06, 7.7934e-07, 1.9738e-06, 1.8861e-06, 3.8455e-04,\n",
       "            2.4781e-04, 1.2490e-05, 8.9563e-05, 2.0237e-04, 8.0638e-04, 4.2700e-04,\n",
       "            5.9640e-04, 2.2612e-04, 8.7100e-05, 5.2730e-04, 2.8609e-04, 2.0490e-04,\n",
       "            2.2342e-04, 8.2800e-04, 8.1541e-06, 3.6565e-04, 2.1430e-05, 5.8379e-04,\n",
       "            5.5338e-04, 8.5901e-05, 2.2460e-04, 1.5266e-04, 1.5321e-03, 6.5377e-04,\n",
       "            6.8677e-04, 3.0527e-04, 2.8477e-04, 6.9686e-04, 7.1372e-04, 2.3817e-04,\n",
       "            3.2504e-04, 6.0107e-06, 1.3788e-03, 2.9682e-04, 2.5962e-05, 1.2409e-04,\n",
       "            2.0594e-03, 2.1820e-04, 7.0879e-04, 1.5674e-04, 1.3563e-03, 1.1396e-04,\n",
       "            3.4192e-04, 2.3339e-04, 1.4442e-04, 1.1072e-03, 4.3490e-04, 2.2343e-06,\n",
       "            8.3108e-04, 1.6907e-04, 5.2102e-04, 2.0280e-04, 7.3318e-04, 1.4954e-04,\n",
       "            2.7894e-04, 5.3224e-06, 3.4813e-04, 3.3988e-03, 4.7187e-04, 3.7380e-04,\n",
       "            2.6906e-04, 1.1312e-04, 7.8713e-07, 5.2323e-04, 4.3466e-04, 5.2161e-04,\n",
       "            3.8296e-04, 7.5006e-04, 1.9706e-03, 1.4398e-03, 3.1516e-04, 3.4761e-03,\n",
       "            1.3554e-04, 6.4900e-04, 1.1808e-04, 2.3262e-04, 1.6919e-04, 1.1958e-05,\n",
       "            4.8718e-04, 3.6956e-04, 7.7046e-05, 3.4941e-07, 3.8754e-04, 3.5875e-04,\n",
       "            4.4794e-04, 5.1378e-04, 1.9689e-04, 4.4260e-04, 5.7397e-05, 6.6734e-04,\n",
       "            6.4032e-04, 7.2968e-04, 4.4114e-04, 3.0681e-04, 1.8749e-04, 5.8185e-04,\n",
       "            2.8619e-04, 5.4196e-04, 5.1029e-04, 4.0055e-04, 2.0445e-04, 2.5730e-04,\n",
       "            2.3364e-04, 6.5648e-05, 2.9506e-04, 4.3393e-04, 8.9549e-05, 6.0978e-04,\n",
       "            3.5490e-04, 8.1189e-04, 2.8546e-04, 1.0447e-05, 4.1452e-04, 3.9781e-04,\n",
       "            2.0816e-04, 5.8840e-05, 3.2359e-04, 5.6089e-04, 2.7331e-04, 3.5502e-04,\n",
       "            7.8984e-04, 4.8911e-04, 3.7948e-04, 2.7251e-04, 2.4949e-04, 4.9855e-04,\n",
       "            4.7034e-06, 1.8688e-07, 5.9232e-07, 7.0310e-06, 5.0177e-06, 3.4399e-07,\n",
       "            1.7362e-07, 1.6940e-07, 4.3506e-07, 1.3116e-06, 2.3810e-06, 2.0995e-07,\n",
       "            1.9922e-06, 1.9565e-06, 2.2064e-06, 1.9355e-06, 2.5452e-06, 1.1243e-06,\n",
       "            4.7326e-06, 2.9119e-07, 6.9239e-07, 2.0111e-06, 1.6521e-06, 2.2261e-06,\n",
       "            2.8971e-07, 1.5316e-07, 1.4706e-07, 3.4064e-07, 1.0971e-06, 1.9328e-07,\n",
       "            1.2238e-07, 8.6950e-07, 1.6509e-07, 2.9434e-06, 3.1160e-07, 5.4940e-07,\n",
       "            4.5991e-06, 9.4782e-07, 2.7146e-07, 2.1829e-07, 2.1068e-07, 1.0562e-06,\n",
       "            3.8459e-07, 2.5948e-06, 1.7275e-07, 5.1410e-07, 1.4283e-06, 2.5661e-07,\n",
       "            3.8509e-07, 7.7057e-06, 2.1512e-07, 7.2185e-07, 2.7456e-07, 1.5581e-07,\n",
       "            4.2357e-08, 2.1498e-06, 5.2958e-07, 1.5002e-05, 2.3609e-06, 7.8297e-07,\n",
       "            4.3108e-07, 1.3900e-06, 1.8180e-07, 6.4253e-07, 4.0462e-06, 3.6790e-07,\n",
       "            2.7819e-07, 7.0853e-08, 3.4129e-07, 7.8507e-07, 1.4538e-07, 3.1228e-07,\n",
       "            1.4141e-06, 3.2460e-06, 1.8198e-06, 5.7377e-07, 2.1641e-06, 1.5161e-06,\n",
       "            3.4242e-06, 5.7733e-06, 2.1476e-06, 3.8429e-07, 2.9872e-06, 5.8801e-06,\n",
       "            2.9112e-07, 4.3496e-07, 3.0318e-07, 2.9272e-07, 1.1668e-06, 5.5122e-08,\n",
       "            5.0697e-06, 3.4652e-07, 1.4436e-06, 1.2826e-07, 2.5729e-07, 1.0553e-06,\n",
       "            3.9644e-07, 1.2591e-07, 1.5288e-06, 5.6312e-07, 2.0300e-07, 4.3708e-08,\n",
       "            6.6469e-07, 2.6970e-06, 1.1861e-06, 2.4317e-06, 5.0020e-07, 2.9307e-06,\n",
       "            5.7081e-06, 3.1538e-07, 5.8896e-07, 4.6791e-08, 5.5599e-07, 4.7297e-06,\n",
       "            4.4683e-07, 1.0591e-06, 4.0083e-07, 6.8550e-06, 1.2301e-06, 3.3204e-07,\n",
       "            2.6506e-07, 1.4117e-07, 8.4014e-07, 2.5192e-07, 1.8525e-06, 2.0136e-06,\n",
       "            2.3765e-07, 8.9873e-07])},\n",
       "   4: {'step': 65292,\n",
       "    'exp_avg': tensor([[-1.1676e-04,  1.4700e-05, -3.4781e-05,  ...,  6.9629e-05,\n",
       "              1.9794e-05,  1.2912e-04],\n",
       "            [-1.3759e-11,  5.5885e-12, -2.9009e-12,  ..., -5.9940e-13,\n",
       "              2.7690e-12, -7.8412e-13],\n",
       "            [ 1.4762e-05,  9.7491e-06, -1.3571e-05,  ..., -3.0004e-06,\n",
       "              1.4340e-05, -1.8956e-06],\n",
       "            ...,\n",
       "            [-1.9277e-05,  1.2330e-04, -1.3265e-05,  ...,  4.9170e-05,\n",
       "             -4.0425e-05, -1.2227e-05],\n",
       "            [-1.5019e-04,  6.6673e-05,  1.0263e-04,  ..., -4.8592e-05,\n",
       "             -5.6148e-05, -5.4693e-05],\n",
       "            [ 3.1501e-05,  3.6576e-06,  1.0901e-05,  ...,  4.8936e-05,\n",
       "              2.1119e-05, -1.3474e-05]]),\n",
       "    'exp_avg_sq': tensor([[1.4709e-07, 7.4085e-08, 6.1643e-08,  ..., 8.6791e-08, 4.8595e-08,\n",
       "             6.4305e-08],\n",
       "            [1.0601e-12, 2.9162e-14, 5.9306e-14,  ..., 8.7174e-14, 8.1995e-14,\n",
       "             4.1267e-13],\n",
       "            [1.9361e-09, 2.1345e-09, 1.8009e-09,  ..., 2.1020e-09, 2.2254e-09,\n",
       "             8.4008e-10],\n",
       "            ...,\n",
       "            [8.1966e-08, 1.2055e-07, 9.9748e-08,  ..., 7.3693e-08, 8.0994e-08,\n",
       "             8.3345e-08],\n",
       "            [1.1936e-07, 8.7382e-08, 9.1764e-08,  ..., 1.1446e-07, 6.7746e-08,\n",
       "             9.0297e-08],\n",
       "            [3.0105e-07, 1.6789e-07, 2.0629e-07,  ..., 2.4036e-07, 1.1876e-07,\n",
       "             1.2663e-07]])},\n",
       "   5: {'step': 65292,\n",
       "    'exp_avg': tensor([[ 1.6624e-05,  9.1894e-06,  3.7023e-05,  ...,  1.5469e-04,\n",
       "             -5.5255e-05, -7.3239e-05],\n",
       "            [-1.4059e-12, -8.3383e-12, -1.1717e-12,  ..., -3.3863e-12,\n",
       "              5.2835e-12, -2.2990e-12],\n",
       "            [-1.1400e-06, -3.9035e-07,  1.4168e-05,  ..., -2.3223e-05,\n",
       "              1.4672e-06, -4.4297e-06],\n",
       "            ...,\n",
       "            [ 1.3319e-04, -1.8672e-07,  1.2505e-04,  ..., -9.4680e-05,\n",
       "             -1.1816e-04,  2.8316e-05],\n",
       "            [ 3.3727e-04, -1.2444e-04, -9.1165e-05,  ...,  1.4699e-04,\n",
       "              1.4907e-04, -2.7114e-04],\n",
       "            [-3.9710e-05, -4.9648e-05, -2.6358e-04,  ..., -2.7288e-05,\n",
       "              7.5577e-05,  2.0919e-04]]),\n",
       "    'exp_avg_sq': tensor([[2.3220e-07, 7.3211e-08, 2.8797e-07,  ..., 1.7694e-07, 1.0348e-07,\n",
       "             1.0402e-07],\n",
       "            [4.4955e-13, 5.4112e-13, 1.1453e-12,  ..., 1.5178e-14, 1.4224e-12,\n",
       "             1.3415e-12],\n",
       "            [7.1533e-09, 1.2224e-10, 2.3568e-09,  ..., 4.5695e-09, 4.9637e-11,\n",
       "             2.5664e-10],\n",
       "            ...,\n",
       "            [2.6139e-07, 1.1870e-07, 3.3826e-07,  ..., 2.2846e-07, 1.3879e-07,\n",
       "             1.0899e-07],\n",
       "            [3.6944e-07, 1.2675e-07, 4.1281e-07,  ..., 2.9250e-07, 1.8271e-07,\n",
       "             1.8981e-07],\n",
       "            [6.6734e-07, 3.0775e-07, 8.1108e-07,  ..., 5.3937e-07, 3.4326e-07,\n",
       "             5.0369e-07]])},\n",
       "   6: {'step': 65292,\n",
       "    'exp_avg': tensor([-7.7292e-05, -2.7988e-11,  4.2274e-06,  2.4025e-05,  6.2228e-10,\n",
       "             1.6664e-05,  2.4379e-04,  4.4331e-05, -1.1875e-06, -5.5282e-05,\n",
       "             1.4771e-04,  4.3649e-06,  6.7304e-08, -1.5361e-06,  1.6742e-06,\n",
       "             4.6818e-05,  2.4755e-07, -7.6671e-05,  4.4828e-08, -1.2177e-06,\n",
       "            -2.0669e-05, -4.3559e-05,  5.4104e-06, -1.2727e-04,  2.9628e-04,\n",
       "            -2.2579e-06,  1.2980e-07, -9.2598e-05, -9.6568e-07,  5.7359e-07,\n",
       "             2.4274e-08, -6.4534e-06, -5.0804e-06, -2.1144e-05,  9.8977e-05,\n",
       "            -3.5162e-06,  1.1957e-07,  8.3520e-07, -1.5440e-05, -4.2256e-08,\n",
       "            -1.5366e-04,  4.3962e-05, -5.4618e-10, -1.1480e-05, -9.3008e-09,\n",
       "            -2.9162e-05,  2.9118e-05,  3.6971e-07,  4.3273e-07,  3.0520e-07,\n",
       "            -6.9556e-05, -5.3043e-05, -2.6737e-06,  2.5009e-06, -4.0229e-07,\n",
       "            -6.9762e-07, -1.1114e-05, -1.0480e-04, -7.0057e-05, -3.9587e-08,\n",
       "            -1.4441e-04,  1.8939e-05,  6.9309e-05, -2.3558e-05, -3.1786e-08,\n",
       "             1.6728e-06,  6.6429e-05, -6.9761e-06,  1.4873e-07, -1.5818e-09,\n",
       "             1.3774e-05, -1.2602e-06, -6.5078e-05,  4.7743e-06,  1.5923e-06,\n",
       "            -1.9346e-06, -6.6085e-05, -1.0090e-10,  9.4032e-06, -1.6627e-04,\n",
       "             9.0529e-05,  2.7885e-04,  1.2073e-04,  4.5851e-05,  1.6730e-05,\n",
       "            -1.6047e-05,  2.1227e-06, -4.7066e-06,  1.0183e-04, -3.0511e-06,\n",
       "             7.0407e-05,  1.9284e-04, -1.0283e-09,  1.8759e-06, -1.1968e-04,\n",
       "             9.7047e-05, -3.9750e-10, -1.4555e-05, -4.5954e-08,  2.0207e-06,\n",
       "            -2.5228e-08, -2.6762e-05, -1.1501e-04, -1.0758e-08,  2.0701e-07,\n",
       "            -1.6255e-07,  2.3665e-05, -2.9242e-06, -1.6571e-05, -6.6620e-11,\n",
       "             1.6492e-05, -3.5663e-06, -1.1583e-06,  2.0686e-05, -3.7095e-05,\n",
       "             4.5777e-07, -6.3421e-06, -1.4865e-04, -6.2937e-06, -8.8050e-05,\n",
       "             2.2793e-08, -1.0958e-07,  1.0764e-06,  1.0134e-07,  1.5591e-05,\n",
       "             4.0480e-05, -9.9686e-05, -6.6085e-07, -6.2736e-06, -3.3982e-10,\n",
       "            -2.6505e-04,  3.3665e-05,  1.2099e-09, -4.2185e-04,  3.2900e-04,\n",
       "             3.5558e-04, -1.0991e-04,  8.9748e-04,  2.1006e-04, -3.0962e-05,\n",
       "             2.6215e-05,  2.3416e-04,  1.6966e-04, -2.8143e-04,  7.6842e-05,\n",
       "             2.7841e-04,  2.0112e-04,  8.6512e-06,  5.7761e-05, -1.0665e-04,\n",
       "            -8.9261e-05, -3.2802e-04,  3.4742e-04,  1.5057e-04,  9.9964e-06,\n",
       "             3.5944e-04, -3.3414e-04,  1.6634e-04,  2.7105e-04, -1.5395e-05,\n",
       "             3.4765e-05, -9.6289e-04, -3.5347e-04,  2.0466e-04, -1.0992e-04,\n",
       "             5.9248e-04,  2.4187e-04, -5.2511e-04, -2.5369e-04,  3.0143e-04,\n",
       "            -2.3962e-09, -8.7225e-05,  2.5274e-04, -4.7143e-04,  1.1949e-05,\n",
       "            -4.5105e-05, -3.6153e-04,  1.9799e-05, -1.4883e-05,  2.8078e-04,\n",
       "             4.7981e-04,  1.1500e-05, -7.5082e-05, -1.0583e-04,  2.1471e-04,\n",
       "            -5.8091e-04, -4.3372e-04, -3.7360e-04,  4.9410e-04,  1.0167e-04,\n",
       "            -2.1705e-04,  5.3224e-05,  5.4704e-07,  1.3569e-05,  1.6553e-04,\n",
       "            -2.2803e-04, -1.4103e-04, -5.4840e-09, -1.7454e-04, -1.0275e-05,\n",
       "            -4.9916e-04,  9.7594e-05, -2.4725e-05, -7.3231e-05, -2.8285e-05,\n",
       "            -9.0266e-10,  2.8439e-05,  1.0362e-04,  1.9378e-04,  1.4238e-04,\n",
       "            -1.0360e-04,  4.9421e-04,  1.2455e-04, -1.4353e-04,  2.5410e-05,\n",
       "            -1.0691e-03,  1.0905e-04,  4.4182e-04,  1.1038e-04, -7.7287e-05,\n",
       "            -3.6512e-09,  9.6714e-05, -4.6673e-04,  3.5256e-04, -2.1177e-09,\n",
       "             1.2202e-04, -1.3903e-05, -4.4759e-04, -3.9116e-04,  3.5897e-04,\n",
       "             9.1276e-05,  4.4034e-05,  1.6651e-04, -6.2463e-05, -5.0910e-04,\n",
       "            -3.0093e-04, -3.6369e-04, -3.1493e-10, -2.3294e-05, -6.2027e-04,\n",
       "             3.1133e-04,  2.9957e-04, -8.0853e-05,  1.8057e-04, -1.8055e-03,\n",
       "            -2.4090e-04,  1.4912e-04,  6.4335e-04,  1.0129e-05, -9.1159e-05,\n",
       "            -1.0666e-04,  7.2112e-05, -3.3442e-04,  2.6860e-04,  1.4722e-04,\n",
       "             5.4253e-04, -6.4299e-04, -2.9363e-11,  6.8019e-04, -3.2352e-04,\n",
       "            -2.0529e-09, -2.3919e-04, -3.5169e-04,  1.5739e-04, -2.5038e-04,\n",
       "             8.6749e-04,  1.8178e-03, -9.1053e-05, -7.6807e-06,  1.3125e-03,\n",
       "             1.5838e-04,  2.4127e-04, -1.2941e-04,  7.9848e-05,  2.8328e-04,\n",
       "             1.9828e-08,  1.1953e-06,  3.1422e-04, -8.8370e-05, -1.3187e-03,\n",
       "            -8.6795e-04,  1.2539e-04,  3.1102e-04, -1.6123e-04, -9.9164e-04,\n",
       "            -4.6049e-04, -1.9933e-04,  3.3613e-04,  5.9493e-04, -9.5137e-04,\n",
       "             2.0675e-03,  9.2488e-04,  1.1309e-04, -5.8900e-05, -4.0474e-04,\n",
       "             4.7422e-04,  3.6321e-03,  1.6798e-04,  1.4555e-09,  2.6356e-04,\n",
       "            -2.1883e-04,  2.4899e-03, -8.8745e-05,  1.6599e-03, -4.2872e-04,\n",
       "             2.0625e-05,  2.0651e-03,  1.1724e-04,  1.0140e-04,  8.1057e-04,\n",
       "            -1.8221e-04,  2.1441e-04,  8.5993e-05,  1.3518e-03, -1.9535e-03,\n",
       "             7.4952e-04, -4.3086e-04,  1.0983e-04, -1.9932e-04, -1.1090e-03,\n",
       "             1.7676e-06, -4.2365e-05,  7.4419e-04, -3.4417e-04,  1.2062e-03,\n",
       "            -1.2635e-08, -6.8359e-04,  2.1397e-05, -3.4945e-04,  2.3074e-03,\n",
       "             7.1868e-05, -2.8965e-04,  3.5287e-04,  1.6318e-10,  3.9544e-04,\n",
       "             8.8507e-04,  2.2551e-05, -9.3141e-04, -4.5802e-04, -5.4328e-04,\n",
       "             4.5382e-04,  1.3091e-04, -1.4412e-05,  8.9755e-04, -3.7407e-04,\n",
       "            -1.1462e-03, -1.3737e-03,  6.2267e-06,  1.7592e-09,  8.7592e-07,\n",
       "            -1.1931e-03,  1.3906e-03, -1.0561e-09, -8.7289e-05, -4.8116e-05,\n",
       "            -1.0483e-03,  3.3864e-04,  1.0208e-03,  4.8004e-04,  1.1644e-04,\n",
       "            -1.7907e-04,  7.5076e-04,  1.0184e-03,  9.2532e-04,  6.0870e-04,\n",
       "            -1.0904e-10, -7.0406e-04, -9.9839e-04,  1.0347e-03, -4.6747e-04,\n",
       "            -1.3225e-04, -2.9754e-04, -2.0015e-03,  3.5298e-04,  1.0418e-03,\n",
       "             1.9825e-04, -1.2368e-05,  2.4683e-04,  3.8229e-04, -1.4067e-03,\n",
       "            -5.6509e-04, -5.5253e-04, -1.4624e-04,  7.8517e-05,  5.5958e-05,\n",
       "             2.3473e-04, -2.2285e-04,  1.7324e-04,  7.8438e-05, -4.9740e-05,\n",
       "             1.0835e-04,  3.0003e-04, -1.1095e-05, -3.9041e-04, -4.2451e-04,\n",
       "            -1.1529e-04,  1.3242e-04,  1.8737e-04,  4.8121e-05, -3.0313e-05,\n",
       "            -3.9942e-04,  2.6043e-04, -1.8633e-04, -1.5818e-04, -2.0792e-04,\n",
       "             7.7672e-05,  3.0377e-04, -9.3136e-05,  7.8676e-05, -6.8410e-05,\n",
       "             2.4913e-05, -2.9117e-04, -4.2480e-04,  5.6156e-05, -3.5612e-05,\n",
       "             1.9722e-04, -9.9856e-08, -3.1762e-05,  1.6753e-04,  2.5866e-04,\n",
       "             2.8751e-04,  6.3021e-05, -2.3106e-05,  1.0125e-05, -1.0918e-04,\n",
       "            -6.5765e-06,  1.7189e-04, -6.5379e-05, -1.6609e-04,  1.3330e-04,\n",
       "             1.3746e-04,  2.8551e-04, -4.3428e-04, -1.4862e-04, -2.2956e-04,\n",
       "             2.2651e-04,  4.2379e-04, -6.8466e-06, -2.2804e-04, -2.1792e-04,\n",
       "            -1.3737e-06, -7.1603e-05, -2.4612e-04, -1.1067e-05,  2.2651e-06,\n",
       "            -1.6844e-04,  3.2007e-05, -1.6348e-04, -1.9006e-04, -1.9540e-04,\n",
       "            -5.8052e-04, -9.7425e-06, -4.5146e-04, -2.1642e-04,  1.0827e-04,\n",
       "            -9.8292e-05,  2.2240e-04,  3.4936e-05,  6.9341e-05,  5.1597e-05,\n",
       "            -1.9624e-04, -1.3777e-04,  2.2013e-04, -1.0711e-04, -3.3279e-05,\n",
       "             4.4631e-06, -3.8883e-05,  1.1783e-04, -8.4044e-05, -2.6795e-04,\n",
       "            -1.7999e-04, -6.3883e-06,  1.6857e-04,  2.8010e-04,  6.5511e-05,\n",
       "            -3.0716e-04, -3.1610e-04, -6.5663e-05,  2.3616e-06,  3.1820e-04,\n",
       "             2.7498e-04, -4.1472e-04,  1.5420e-05, -1.0234e-04,  8.0888e-05,\n",
       "             1.8848e-04,  5.4419e-05,  1.6947e-04, -2.1454e-04, -1.3341e-04,\n",
       "            -2.7539e-04, -2.3610e-04,  7.1829e-05, -4.4795e-04, -3.5430e-04,\n",
       "             1.1573e-04,  3.2387e-04,  3.9646e-04, -4.0555e-05, -1.7445e-06,\n",
       "            -3.5303e-04,  4.0946e-04,  2.7097e-04, -9.2693e-05, -5.1284e-05,\n",
       "            -1.0640e-04,  1.4625e-05, -3.2291e-04, -2.7695e-04,  1.7808e-04,\n",
       "            -5.8916e-04,  1.6569e-04]),\n",
       "    'exp_avg_sq': tensor([5.5056e-07, 2.1896e-12, 1.7213e-08, 1.4454e-08, 1.5373e-11, 4.6654e-08,\n",
       "            3.7397e-07, 1.0944e-07, 2.3849e-10, 2.2893e-07, 1.6434e-07, 7.6985e-11,\n",
       "            2.0673e-12, 1.7528e-10, 3.4212e-10, 1.1741e-07, 5.6813e-11, 4.8785e-07,\n",
       "            1.4467e-11, 6.5880e-10, 5.4023e-09, 1.4725e-07, 3.4520e-09, 2.7642e-07,\n",
       "            1.1492e-07, 3.3724e-10, 2.5732e-12, 2.8463e-08, 2.1272e-09, 3.1479e-10,\n",
       "            1.7776e-12, 5.9105e-09, 1.1832e-08, 1.0481e-06, 7.0208e-07, 3.6918e-09,\n",
       "            1.4417e-11, 1.0949e-10, 2.9404e-07, 1.0272e-11, 2.4708e-07, 7.4418e-08,\n",
       "            2.5588e-12, 1.4553e-08, 7.2638e-12, 2.3259e-07, 1.5484e-08, 3.7679e-09,\n",
       "            2.7453e-11, 1.6637e-10, 6.1002e-07, 6.0576e-08, 1.1361e-09, 2.3496e-08,\n",
       "            8.9960e-12, 2.1736e-10, 8.3496e-10, 1.3000e-07, 5.1600e-08, 2.1172e-11,\n",
       "            2.0173e-07, 2.4838e-09, 8.9402e-08, 1.8668e-08, 1.2389e-11, 2.3409e-10,\n",
       "            1.3505e-07, 1.5990e-09, 1.0372e-07, 3.8563e-11, 1.4347e-07, 6.8212e-11,\n",
       "            5.3039e-08, 7.4647e-08, 1.3108e-10, 5.5515e-10, 2.5532e-08, 4.5076e-11,\n",
       "            6.6379e-09, 1.5793e-07, 1.6802e-07, 5.5831e-07, 1.5769e-07, 4.2047e-08,\n",
       "            8.1746e-08, 3.2820e-09, 1.5106e-10, 4.3338e-09, 7.2836e-08, 8.6700e-08,\n",
       "            3.0756e-07, 2.2952e-07, 8.3411e-11, 4.8058e-10, 4.1946e-07, 3.2405e-07,\n",
       "            9.5497e-12, 2.3106e-09, 9.0231e-10, 3.2756e-07, 2.9131e-12, 6.2376e-08,\n",
       "            1.9726e-07, 3.7469e-11, 3.3236e-11, 2.2798e-11, 1.1693e-08, 6.3706e-11,\n",
       "            3.3078e-09, 8.4531e-13, 2.3118e-09, 3.3493e-07, 7.0340e-11, 4.9326e-09,\n",
       "            9.4074e-09, 1.5307e-10, 6.1212e-08, 1.7354e-07, 2.7071e-09, 1.4630e-07,\n",
       "            5.0757e-10, 7.5114e-12, 8.2596e-11, 3.5083e-12, 5.4087e-08, 8.5650e-08,\n",
       "            1.3514e-07, 1.0611e-07, 1.9518e-06, 6.4005e-12, 3.1638e-06, 1.1400e-06,\n",
       "            1.4489e-11, 1.3196e-06, 1.1514e-06, 1.9805e-06, 2.1223e-06, 2.5406e-06,\n",
       "            2.7875e-06, 7.8519e-07, 5.2796e-09, 3.0094e-06, 2.5189e-06, 5.4070e-06,\n",
       "            3.7243e-07, 1.8797e-06, 1.6729e-06, 4.5569e-09, 2.7423e-07, 7.3421e-07,\n",
       "            6.6626e-07, 3.6998e-06, 2.1063e-06, 1.0356e-06, 7.3945e-07, 1.9815e-06,\n",
       "            3.6401e-06, 1.8411e-06, 1.2184e-06, 6.6527e-07, 1.5836e-06, 2.4205e-06,\n",
       "            1.3810e-06, 1.4862e-06, 2.5987e-07, 1.3605e-06, 1.6347e-06, 8.6870e-07,\n",
       "            3.8245e-06, 1.4641e-06, 4.8894e-12, 1.9966e-06, 2.5879e-06, 2.5817e-06,\n",
       "            9.9065e-07, 1.0316e-06, 1.0256e-06, 6.6841e-09, 2.7419e-06, 2.2029e-06,\n",
       "            2.4892e-06, 1.3702e-06, 2.6826e-06, 1.1284e-06, 1.0448e-06, 2.7578e-06,\n",
       "            2.2569e-06, 1.7041e-06, 1.9097e-06, 1.1024e-07, 1.0374e-06, 2.2825e-06,\n",
       "            3.1994e-09, 5.4306e-09, 1.8868e-06, 1.9014e-06, 7.7055e-07, 5.5223e-11,\n",
       "            2.2803e-06, 5.7081e-09, 1.5778e-06, 4.2547e-06, 3.4199e-07, 9.6835e-07,\n",
       "            8.1321e-07, 2.9900e-12, 1.7026e-06, 3.0083e-06, 1.4444e-06, 3.0448e-06,\n",
       "            1.0658e-06, 1.1705e-06, 1.8814e-06, 3.3073e-07, 4.8175e-09, 2.8288e-06,\n",
       "            1.7609e-06, 3.6934e-06, 2.3873e-06, 3.2312e-06, 1.5522e-11, 4.5991e-07,\n",
       "            1.6570e-06, 1.2096e-06, 1.0891e-11, 1.0039e-06, 2.6322e-08, 1.8822e-06,\n",
       "            1.4193e-06, 1.4553e-06, 1.0895e-06, 1.6897e-08, 1.8853e-06, 1.2298e-06,\n",
       "            1.2651e-06, 2.1863e-06, 2.5712e-06, 1.7206e-13, 2.8262e-06, 1.4844e-06,\n",
       "            2.2805e-06, 7.7379e-07, 2.0494e-06, 7.9894e-07, 6.7609e-06, 8.3627e-06,\n",
       "            1.7812e-06, 2.5781e-06, 1.2500e-07, 1.7663e-07, 1.2605e-06, 1.0481e-06,\n",
       "            3.1674e-06, 2.5840e-06, 3.2169e-06, 3.2464e-06, 1.6850e-05, 2.1240e-09,\n",
       "            2.1322e-05, 2.7662e-06, 2.1386e-09, 1.8283e-05, 3.4862e-05, 4.0646e-05,\n",
       "            1.5148e-05, 4.5898e-05, 3.2303e-05, 3.6586e-06, 6.0268e-09, 3.9849e-05,\n",
       "            5.5919e-06, 2.0803e-05, 5.8303e-07, 2.0639e-05, 9.5489e-06, 9.7669e-10,\n",
       "            2.8984e-06, 4.5591e-06, 2.4908e-06, 5.0485e-05, 6.8055e-06, 1.0221e-05,\n",
       "            3.0157e-06, 3.6480e-06, 1.7961e-05, 6.5500e-06, 3.9747e-06, 1.4282e-06,\n",
       "            7.7648e-06, 1.5588e-05, 2.6510e-05, 9.0271e-06, 6.3496e-07, 6.6252e-06,\n",
       "            8.9730e-06, 4.4758e-06, 6.1558e-05, 3.4798e-05, 1.7232e-10, 5.1560e-06,\n",
       "            8.1870e-06, 1.7756e-05, 1.1589e-06, 1.9377e-05, 1.9598e-06, 2.1274e-08,\n",
       "            5.9146e-05, 1.5792e-05, 1.0006e-05, 4.9760e-06, 1.7969e-06, 1.0214e-05,\n",
       "            4.5959e-06, 7.4640e-06, 2.4862e-05, 6.6142e-06, 3.4125e-05, 1.0575e-07,\n",
       "            1.5924e-05, 1.3145e-05, 2.3610e-08, 2.0682e-08, 1.5486e-05, 7.2910e-06,\n",
       "            1.3786e-05, 1.1713e-09, 1.2873e-05, 2.6546e-08, 2.4396e-05, 3.1805e-05,\n",
       "            5.7021e-07, 5.5493e-07, 9.1190e-07, 2.6025e-09, 4.6911e-06, 1.2732e-05,\n",
       "            1.9733e-05, 1.9866e-05, 1.7986e-05, 8.2292e-06, 2.0767e-05, 1.1345e-06,\n",
       "            5.4512e-09, 8.8310e-06, 6.0095e-06, 1.1544e-05, 1.4054e-05, 4.2129e-05,\n",
       "            2.7968e-09, 3.5875e-07, 1.2358e-05, 4.6694e-05, 3.3977e-10, 2.6268e-06,\n",
       "            2.1310e-07, 1.8269e-05, 3.0795e-06, 4.7205e-05, 1.6782e-05, 2.8673e-08,\n",
       "            2.4257e-06, 6.1000e-06, 7.9659e-06, 1.3431e-05, 2.6193e-05, 6.2785e-11,\n",
       "            1.5415e-05, 1.3151e-05, 1.5940e-05, 7.8011e-06, 9.3108e-06, 1.2450e-05,\n",
       "            1.9923e-05, 3.9637e-05, 8.4936e-06, 3.3974e-05, 6.1844e-08, 4.5647e-07,\n",
       "            1.0640e-05, 9.5054e-06, 1.0396e-05, 2.2697e-05, 1.0899e-05, 9.5618e-06,\n",
       "            2.5657e-07, 1.1572e-06, 1.7111e-07, 7.7619e-07, 9.1682e-07, 4.1778e-07,\n",
       "            8.5712e-08, 4.9033e-07, 5.9870e-07, 1.4249e-06, 6.0279e-07, 1.0247e-06,\n",
       "            4.5224e-07, 7.6254e-07, 2.0463e-06, 4.7591e-07, 6.9063e-07, 2.6450e-06,\n",
       "            4.9912e-07, 3.3215e-07, 1.7455e-06, 1.4840e-06, 1.3322e-06, 9.7914e-07,\n",
       "            2.3586e-06, 5.9609e-07, 5.3135e-07, 1.3124e-06, 6.0029e-07, 9.5287e-07,\n",
       "            1.0891e-06, 9.0482e-07, 4.8139e-07, 9.9070e-07, 3.6158e-07, 3.2022e-07,\n",
       "            5.2570e-07, 7.6852e-08, 1.1798e-08, 9.2037e-07, 7.4813e-07, 8.9220e-07,\n",
       "            3.2979e-07, 8.4778e-07, 9.6650e-07, 1.4960e-06, 5.9371e-07, 1.8103e-06,\n",
       "            9.2842e-07, 9.9369e-07, 7.3012e-07, 1.0368e-06, 1.4021e-06, 1.4791e-06,\n",
       "            6.5212e-07, 2.1283e-06, 8.2172e-07, 8.8645e-07, 1.3988e-06, 8.0507e-07,\n",
       "            1.0925e-09, 9.5187e-07, 7.2705e-08, 2.6321e-07, 1.7031e-06, 8.0075e-07,\n",
       "            9.2943e-07, 6.7598e-07, 1.0898e-06, 6.4717e-07, 1.2801e-06, 7.9770e-07,\n",
       "            5.4262e-07, 5.7193e-07, 4.9005e-07, 4.7550e-07, 7.6534e-07, 1.7421e-07,\n",
       "            1.0262e-06, 7.0535e-07, 8.3798e-07, 4.7248e-07, 9.9644e-07, 6.5603e-07,\n",
       "            9.1952e-07, 7.3084e-07, 6.1106e-07, 6.0213e-07, 7.4756e-07, 1.0607e-06,\n",
       "            4.6905e-07, 5.0957e-07, 5.8216e-07, 5.9547e-07, 8.3884e-11, 1.5465e-07,\n",
       "            1.0877e-06, 3.9536e-07, 4.3225e-07, 1.7209e-07, 4.8408e-07, 4.8740e-07,\n",
       "            1.5600e-06, 9.5253e-07, 6.2175e-07, 8.1766e-07, 1.4430e-06, 9.3037e-07,\n",
       "            7.6765e-07, 1.0351e-06, 1.1335e-06, 6.2252e-07, 8.8634e-07, 1.4705e-06,\n",
       "            5.4249e-07, 4.1245e-09, 6.8280e-07, 1.2122e-06, 9.3009e-07, 4.1178e-07,\n",
       "            1.1415e-06, 5.4757e-07, 1.2109e-06, 6.7609e-07, 6.4418e-07, 6.8606e-07,\n",
       "            9.8617e-07, 1.7311e-06])},\n",
       "   7: {'step': 65292,\n",
       "    'exp_avg': tensor([-7.7292e-05, -2.7988e-11,  4.2274e-06,  2.4025e-05,  6.2228e-10,\n",
       "             1.6664e-05,  2.4379e-04,  4.4331e-05, -1.1875e-06, -5.5282e-05,\n",
       "             1.4771e-04,  4.3649e-06,  6.7304e-08, -1.5361e-06,  1.6742e-06,\n",
       "             4.6818e-05,  2.4755e-07, -7.6671e-05,  4.4828e-08, -1.2177e-06,\n",
       "            -2.0669e-05, -4.3559e-05,  5.4104e-06, -1.2727e-04,  2.9628e-04,\n",
       "            -2.2579e-06,  1.2980e-07, -9.2598e-05, -9.6568e-07,  5.7359e-07,\n",
       "             2.4274e-08, -6.4534e-06, -5.0804e-06, -2.1144e-05,  9.8977e-05,\n",
       "            -3.5162e-06,  1.1957e-07,  8.3520e-07, -1.5440e-05, -4.2256e-08,\n",
       "            -1.5366e-04,  4.3962e-05, -5.4618e-10, -1.1480e-05, -9.3008e-09,\n",
       "            -2.9162e-05,  2.9118e-05,  3.6971e-07,  4.3273e-07,  3.0520e-07,\n",
       "            -6.9556e-05, -5.3043e-05, -2.6737e-06,  2.5009e-06, -4.0229e-07,\n",
       "            -6.9762e-07, -1.1114e-05, -1.0480e-04, -7.0057e-05, -3.9587e-08,\n",
       "            -1.4441e-04,  1.8939e-05,  6.9309e-05, -2.3558e-05, -3.1786e-08,\n",
       "             1.6728e-06,  6.6429e-05, -6.9761e-06,  1.4873e-07, -1.5818e-09,\n",
       "             1.3774e-05, -1.2602e-06, -6.5078e-05,  4.7743e-06,  1.5923e-06,\n",
       "            -1.9346e-06, -6.6085e-05, -1.0090e-10,  9.4032e-06, -1.6627e-04,\n",
       "             9.0529e-05,  2.7885e-04,  1.2073e-04,  4.5851e-05,  1.6730e-05,\n",
       "            -1.6047e-05,  2.1227e-06, -4.7066e-06,  1.0183e-04, -3.0511e-06,\n",
       "             7.0407e-05,  1.9284e-04, -1.0283e-09,  1.8759e-06, -1.1968e-04,\n",
       "             9.7047e-05, -3.9750e-10, -1.4555e-05, -4.5954e-08,  2.0207e-06,\n",
       "            -2.5228e-08, -2.6762e-05, -1.1501e-04, -1.0758e-08,  2.0701e-07,\n",
       "            -1.6255e-07,  2.3665e-05, -2.9242e-06, -1.6571e-05, -6.6620e-11,\n",
       "             1.6492e-05, -3.5663e-06, -1.1583e-06,  2.0686e-05, -3.7095e-05,\n",
       "             4.5777e-07, -6.3421e-06, -1.4865e-04, -6.2937e-06, -8.8050e-05,\n",
       "             2.2793e-08, -1.0958e-07,  1.0764e-06,  1.0134e-07,  1.5591e-05,\n",
       "             4.0480e-05, -9.9686e-05, -6.6085e-07, -6.2736e-06, -3.3982e-10,\n",
       "            -2.6505e-04,  3.3665e-05,  1.2099e-09, -4.2185e-04,  3.2900e-04,\n",
       "             3.5558e-04, -1.0991e-04,  8.9748e-04,  2.1006e-04, -3.0962e-05,\n",
       "             2.6215e-05,  2.3416e-04,  1.6966e-04, -2.8143e-04,  7.6842e-05,\n",
       "             2.7841e-04,  2.0112e-04,  8.6512e-06,  5.7761e-05, -1.0665e-04,\n",
       "            -8.9261e-05, -3.2802e-04,  3.4742e-04,  1.5057e-04,  9.9964e-06,\n",
       "             3.5944e-04, -3.3414e-04,  1.6634e-04,  2.7105e-04, -1.5395e-05,\n",
       "             3.4765e-05, -9.6289e-04, -3.5347e-04,  2.0466e-04, -1.0992e-04,\n",
       "             5.9248e-04,  2.4187e-04, -5.2511e-04, -2.5369e-04,  3.0143e-04,\n",
       "            -2.3962e-09, -8.7225e-05,  2.5274e-04, -4.7143e-04,  1.1949e-05,\n",
       "            -4.5105e-05, -3.6153e-04,  1.9799e-05, -1.4883e-05,  2.8078e-04,\n",
       "             4.7981e-04,  1.1500e-05, -7.5082e-05, -1.0583e-04,  2.1471e-04,\n",
       "            -5.8091e-04, -4.3372e-04, -3.7360e-04,  4.9410e-04,  1.0167e-04,\n",
       "            -2.1705e-04,  5.3224e-05,  5.4704e-07,  1.3569e-05,  1.6553e-04,\n",
       "            -2.2803e-04, -1.4103e-04, -5.4840e-09, -1.7454e-04, -1.0275e-05,\n",
       "            -4.9916e-04,  9.7594e-05, -2.4725e-05, -7.3231e-05, -2.8285e-05,\n",
       "            -9.0266e-10,  2.8439e-05,  1.0362e-04,  1.9378e-04,  1.4238e-04,\n",
       "            -1.0360e-04,  4.9421e-04,  1.2455e-04, -1.4353e-04,  2.5410e-05,\n",
       "            -1.0691e-03,  1.0905e-04,  4.4182e-04,  1.1038e-04, -7.7287e-05,\n",
       "            -3.6512e-09,  9.6714e-05, -4.6673e-04,  3.5256e-04, -2.1177e-09,\n",
       "             1.2202e-04, -1.3903e-05, -4.4759e-04, -3.9116e-04,  3.5897e-04,\n",
       "             9.1276e-05,  4.4034e-05,  1.6651e-04, -6.2463e-05, -5.0910e-04,\n",
       "            -3.0093e-04, -3.6369e-04, -3.1493e-10, -2.3294e-05, -6.2027e-04,\n",
       "             3.1133e-04,  2.9957e-04, -8.0853e-05,  1.8057e-04, -1.8055e-03,\n",
       "            -2.4090e-04,  1.4912e-04,  6.4335e-04,  1.0129e-05, -9.1159e-05,\n",
       "            -1.0666e-04,  7.2112e-05, -3.3442e-04,  2.6860e-04,  1.4722e-04,\n",
       "             5.4253e-04, -6.4299e-04, -2.9363e-11,  6.8019e-04, -3.2352e-04,\n",
       "            -2.0529e-09, -2.3919e-04, -3.5169e-04,  1.5739e-04, -2.5038e-04,\n",
       "             8.6749e-04,  1.8178e-03, -9.1053e-05, -7.6807e-06,  1.3125e-03,\n",
       "             1.5838e-04,  2.4127e-04, -1.2941e-04,  7.9848e-05,  2.8328e-04,\n",
       "             1.9828e-08,  1.1953e-06,  3.1422e-04, -8.8370e-05, -1.3187e-03,\n",
       "            -8.6795e-04,  1.2539e-04,  3.1102e-04, -1.6123e-04, -9.9164e-04,\n",
       "            -4.6049e-04, -1.9933e-04,  3.3613e-04,  5.9493e-04, -9.5137e-04,\n",
       "             2.0675e-03,  9.2488e-04,  1.1309e-04, -5.8900e-05, -4.0474e-04,\n",
       "             4.7422e-04,  3.6321e-03,  1.6798e-04,  1.4555e-09,  2.6356e-04,\n",
       "            -2.1883e-04,  2.4899e-03, -8.8745e-05,  1.6599e-03, -4.2872e-04,\n",
       "             2.0625e-05,  2.0651e-03,  1.1724e-04,  1.0140e-04,  8.1057e-04,\n",
       "            -1.8221e-04,  2.1441e-04,  8.5993e-05,  1.3518e-03, -1.9535e-03,\n",
       "             7.4952e-04, -4.3086e-04,  1.0983e-04, -1.9932e-04, -1.1090e-03,\n",
       "             1.7676e-06, -4.2365e-05,  7.4419e-04, -3.4417e-04,  1.2062e-03,\n",
       "            -1.2635e-08, -6.8359e-04,  2.1397e-05, -3.4945e-04,  2.3074e-03,\n",
       "             7.1868e-05, -2.8965e-04,  3.5287e-04,  1.6318e-10,  3.9544e-04,\n",
       "             8.8507e-04,  2.2551e-05, -9.3141e-04, -4.5802e-04, -5.4328e-04,\n",
       "             4.5382e-04,  1.3091e-04, -1.4412e-05,  8.9755e-04, -3.7407e-04,\n",
       "            -1.1462e-03, -1.3737e-03,  6.2267e-06,  1.7592e-09,  8.7592e-07,\n",
       "            -1.1931e-03,  1.3906e-03, -1.0561e-09, -8.7289e-05, -4.8116e-05,\n",
       "            -1.0483e-03,  3.3864e-04,  1.0208e-03,  4.8004e-04,  1.1644e-04,\n",
       "            -1.7907e-04,  7.5076e-04,  1.0184e-03,  9.2532e-04,  6.0870e-04,\n",
       "            -1.0904e-10, -7.0406e-04, -9.9839e-04,  1.0347e-03, -4.6747e-04,\n",
       "            -1.3225e-04, -2.9754e-04, -2.0015e-03,  3.5298e-04,  1.0418e-03,\n",
       "             1.9825e-04, -1.2368e-05,  2.4683e-04,  3.8229e-04, -1.4067e-03,\n",
       "            -5.6509e-04, -5.5253e-04, -1.4624e-04,  7.8517e-05,  5.5958e-05,\n",
       "             2.3473e-04, -2.2285e-04,  1.7324e-04,  7.8438e-05, -4.9740e-05,\n",
       "             1.0835e-04,  3.0003e-04, -1.1095e-05, -3.9041e-04, -4.2451e-04,\n",
       "            -1.1529e-04,  1.3242e-04,  1.8737e-04,  4.8121e-05, -3.0313e-05,\n",
       "            -3.9942e-04,  2.6043e-04, -1.8633e-04, -1.5818e-04, -2.0792e-04,\n",
       "             7.7672e-05,  3.0377e-04, -9.3136e-05,  7.8676e-05, -6.8410e-05,\n",
       "             2.4913e-05, -2.9117e-04, -4.2480e-04,  5.6156e-05, -3.5612e-05,\n",
       "             1.9722e-04, -9.9856e-08, -3.1762e-05,  1.6753e-04,  2.5866e-04,\n",
       "             2.8751e-04,  6.3021e-05, -2.3106e-05,  1.0125e-05, -1.0918e-04,\n",
       "            -6.5765e-06,  1.7189e-04, -6.5379e-05, -1.6609e-04,  1.3330e-04,\n",
       "             1.3746e-04,  2.8551e-04, -4.3428e-04, -1.4862e-04, -2.2956e-04,\n",
       "             2.2651e-04,  4.2379e-04, -6.8466e-06, -2.2804e-04, -2.1792e-04,\n",
       "            -1.3737e-06, -7.1603e-05, -2.4612e-04, -1.1067e-05,  2.2651e-06,\n",
       "            -1.6844e-04,  3.2007e-05, -1.6348e-04, -1.9006e-04, -1.9540e-04,\n",
       "            -5.8052e-04, -9.7425e-06, -4.5146e-04, -2.1642e-04,  1.0827e-04,\n",
       "            -9.8292e-05,  2.2240e-04,  3.4936e-05,  6.9341e-05,  5.1597e-05,\n",
       "            -1.9624e-04, -1.3777e-04,  2.2013e-04, -1.0711e-04, -3.3279e-05,\n",
       "             4.4631e-06, -3.8883e-05,  1.1783e-04, -8.4044e-05, -2.6795e-04,\n",
       "            -1.7999e-04, -6.3883e-06,  1.6857e-04,  2.8010e-04,  6.5511e-05,\n",
       "            -3.0716e-04, -3.1610e-04, -6.5663e-05,  2.3616e-06,  3.1820e-04,\n",
       "             2.7498e-04, -4.1472e-04,  1.5420e-05, -1.0234e-04,  8.0888e-05,\n",
       "             1.8848e-04,  5.4419e-05,  1.6947e-04, -2.1454e-04, -1.3341e-04,\n",
       "            -2.7539e-04, -2.3610e-04,  7.1829e-05, -4.4795e-04, -3.5430e-04,\n",
       "             1.1573e-04,  3.2387e-04,  3.9646e-04, -4.0555e-05, -1.7445e-06,\n",
       "            -3.5303e-04,  4.0946e-04,  2.7097e-04, -9.2693e-05, -5.1284e-05,\n",
       "            -1.0640e-04,  1.4625e-05, -3.2291e-04, -2.7695e-04,  1.7808e-04,\n",
       "            -5.8916e-04,  1.6569e-04]),\n",
       "    'exp_avg_sq': tensor([5.5056e-07, 2.1896e-12, 1.7213e-08, 1.4454e-08, 1.5373e-11, 4.6654e-08,\n",
       "            3.7397e-07, 1.0944e-07, 2.3849e-10, 2.2893e-07, 1.6434e-07, 7.6985e-11,\n",
       "            2.0673e-12, 1.7528e-10, 3.4212e-10, 1.1741e-07, 5.6813e-11, 4.8785e-07,\n",
       "            1.4467e-11, 6.5880e-10, 5.4023e-09, 1.4725e-07, 3.4520e-09, 2.7642e-07,\n",
       "            1.1492e-07, 3.3724e-10, 2.5732e-12, 2.8463e-08, 2.1272e-09, 3.1479e-10,\n",
       "            1.7776e-12, 5.9105e-09, 1.1832e-08, 1.0481e-06, 7.0208e-07, 3.6918e-09,\n",
       "            1.4417e-11, 1.0949e-10, 2.9404e-07, 1.0272e-11, 2.4708e-07, 7.4418e-08,\n",
       "            2.5588e-12, 1.4553e-08, 7.2638e-12, 2.3259e-07, 1.5484e-08, 3.7679e-09,\n",
       "            2.7453e-11, 1.6637e-10, 6.1002e-07, 6.0576e-08, 1.1361e-09, 2.3496e-08,\n",
       "            8.9960e-12, 2.1736e-10, 8.3496e-10, 1.3000e-07, 5.1600e-08, 2.1172e-11,\n",
       "            2.0173e-07, 2.4838e-09, 8.9402e-08, 1.8668e-08, 1.2389e-11, 2.3409e-10,\n",
       "            1.3505e-07, 1.5990e-09, 1.0372e-07, 3.8563e-11, 1.4347e-07, 6.8212e-11,\n",
       "            5.3039e-08, 7.4647e-08, 1.3108e-10, 5.5515e-10, 2.5532e-08, 4.5076e-11,\n",
       "            6.6379e-09, 1.5793e-07, 1.6802e-07, 5.5831e-07, 1.5769e-07, 4.2047e-08,\n",
       "            8.1746e-08, 3.2820e-09, 1.5106e-10, 4.3338e-09, 7.2836e-08, 8.6700e-08,\n",
       "            3.0756e-07, 2.2952e-07, 8.3411e-11, 4.8058e-10, 4.1946e-07, 3.2405e-07,\n",
       "            9.5497e-12, 2.3106e-09, 9.0231e-10, 3.2756e-07, 2.9131e-12, 6.2376e-08,\n",
       "            1.9726e-07, 3.7469e-11, 3.3236e-11, 2.2798e-11, 1.1693e-08, 6.3706e-11,\n",
       "            3.3078e-09, 8.4531e-13, 2.3118e-09, 3.3493e-07, 7.0340e-11, 4.9326e-09,\n",
       "            9.4074e-09, 1.5307e-10, 6.1212e-08, 1.7354e-07, 2.7071e-09, 1.4630e-07,\n",
       "            5.0757e-10, 7.5114e-12, 8.2596e-11, 3.5083e-12, 5.4087e-08, 8.5650e-08,\n",
       "            1.3514e-07, 1.0611e-07, 1.9518e-06, 6.4005e-12, 3.1638e-06, 1.1400e-06,\n",
       "            1.4489e-11, 1.3196e-06, 1.1514e-06, 1.9805e-06, 2.1223e-06, 2.5406e-06,\n",
       "            2.7875e-06, 7.8519e-07, 5.2796e-09, 3.0094e-06, 2.5189e-06, 5.4070e-06,\n",
       "            3.7243e-07, 1.8797e-06, 1.6729e-06, 4.5569e-09, 2.7423e-07, 7.3421e-07,\n",
       "            6.6626e-07, 3.6998e-06, 2.1063e-06, 1.0356e-06, 7.3945e-07, 1.9815e-06,\n",
       "            3.6401e-06, 1.8411e-06, 1.2184e-06, 6.6527e-07, 1.5836e-06, 2.4205e-06,\n",
       "            1.3810e-06, 1.4862e-06, 2.5987e-07, 1.3605e-06, 1.6347e-06, 8.6870e-07,\n",
       "            3.8245e-06, 1.4641e-06, 4.8894e-12, 1.9966e-06, 2.5879e-06, 2.5817e-06,\n",
       "            9.9065e-07, 1.0316e-06, 1.0256e-06, 6.6841e-09, 2.7419e-06, 2.2029e-06,\n",
       "            2.4892e-06, 1.3702e-06, 2.6826e-06, 1.1284e-06, 1.0448e-06, 2.7578e-06,\n",
       "            2.2569e-06, 1.7041e-06, 1.9097e-06, 1.1024e-07, 1.0374e-06, 2.2825e-06,\n",
       "            3.1994e-09, 5.4306e-09, 1.8868e-06, 1.9014e-06, 7.7055e-07, 5.5223e-11,\n",
       "            2.2803e-06, 5.7081e-09, 1.5778e-06, 4.2547e-06, 3.4199e-07, 9.6835e-07,\n",
       "            8.1321e-07, 2.9900e-12, 1.7026e-06, 3.0083e-06, 1.4444e-06, 3.0448e-06,\n",
       "            1.0658e-06, 1.1705e-06, 1.8814e-06, 3.3073e-07, 4.8175e-09, 2.8288e-06,\n",
       "            1.7609e-06, 3.6934e-06, 2.3873e-06, 3.2312e-06, 1.5522e-11, 4.5991e-07,\n",
       "            1.6570e-06, 1.2096e-06, 1.0891e-11, 1.0039e-06, 2.6322e-08, 1.8822e-06,\n",
       "            1.4193e-06, 1.4553e-06, 1.0895e-06, 1.6897e-08, 1.8853e-06, 1.2298e-06,\n",
       "            1.2651e-06, 2.1863e-06, 2.5712e-06, 1.7206e-13, 2.8262e-06, 1.4844e-06,\n",
       "            2.2805e-06, 7.7379e-07, 2.0494e-06, 7.9894e-07, 6.7609e-06, 8.3627e-06,\n",
       "            1.7812e-06, 2.5781e-06, 1.2500e-07, 1.7663e-07, 1.2605e-06, 1.0481e-06,\n",
       "            3.1674e-06, 2.5840e-06, 3.2169e-06, 3.2464e-06, 1.6850e-05, 2.1240e-09,\n",
       "            2.1322e-05, 2.7662e-06, 2.1386e-09, 1.8283e-05, 3.4862e-05, 4.0646e-05,\n",
       "            1.5148e-05, 4.5898e-05, 3.2303e-05, 3.6586e-06, 6.0268e-09, 3.9849e-05,\n",
       "            5.5919e-06, 2.0803e-05, 5.8303e-07, 2.0639e-05, 9.5489e-06, 9.7669e-10,\n",
       "            2.8984e-06, 4.5591e-06, 2.4908e-06, 5.0485e-05, 6.8055e-06, 1.0221e-05,\n",
       "            3.0157e-06, 3.6480e-06, 1.7961e-05, 6.5500e-06, 3.9747e-06, 1.4282e-06,\n",
       "            7.7648e-06, 1.5588e-05, 2.6510e-05, 9.0271e-06, 6.3496e-07, 6.6252e-06,\n",
       "            8.9730e-06, 4.4758e-06, 6.1558e-05, 3.4798e-05, 1.7232e-10, 5.1560e-06,\n",
       "            8.1870e-06, 1.7756e-05, 1.1589e-06, 1.9377e-05, 1.9598e-06, 2.1274e-08,\n",
       "            5.9146e-05, 1.5792e-05, 1.0006e-05, 4.9760e-06, 1.7969e-06, 1.0214e-05,\n",
       "            4.5959e-06, 7.4640e-06, 2.4862e-05, 6.6142e-06, 3.4125e-05, 1.0575e-07,\n",
       "            1.5924e-05, 1.3145e-05, 2.3610e-08, 2.0682e-08, 1.5486e-05, 7.2910e-06,\n",
       "            1.3786e-05, 1.1713e-09, 1.2873e-05, 2.6546e-08, 2.4396e-05, 3.1805e-05,\n",
       "            5.7021e-07, 5.5493e-07, 9.1190e-07, 2.6025e-09, 4.6911e-06, 1.2732e-05,\n",
       "            1.9733e-05, 1.9866e-05, 1.7986e-05, 8.2292e-06, 2.0767e-05, 1.1345e-06,\n",
       "            5.4512e-09, 8.8310e-06, 6.0095e-06, 1.1544e-05, 1.4054e-05, 4.2129e-05,\n",
       "            2.7968e-09, 3.5875e-07, 1.2358e-05, 4.6694e-05, 3.3977e-10, 2.6268e-06,\n",
       "            2.1310e-07, 1.8269e-05, 3.0795e-06, 4.7205e-05, 1.6782e-05, 2.8673e-08,\n",
       "            2.4257e-06, 6.1000e-06, 7.9659e-06, 1.3431e-05, 2.6193e-05, 6.2785e-11,\n",
       "            1.5415e-05, 1.3151e-05, 1.5940e-05, 7.8011e-06, 9.3108e-06, 1.2450e-05,\n",
       "            1.9923e-05, 3.9637e-05, 8.4936e-06, 3.3974e-05, 6.1844e-08, 4.5647e-07,\n",
       "            1.0640e-05, 9.5054e-06, 1.0396e-05, 2.2697e-05, 1.0899e-05, 9.5618e-06,\n",
       "            2.5657e-07, 1.1572e-06, 1.7111e-07, 7.7619e-07, 9.1682e-07, 4.1778e-07,\n",
       "            8.5712e-08, 4.9033e-07, 5.9870e-07, 1.4249e-06, 6.0279e-07, 1.0247e-06,\n",
       "            4.5224e-07, 7.6254e-07, 2.0463e-06, 4.7591e-07, 6.9063e-07, 2.6450e-06,\n",
       "            4.9912e-07, 3.3215e-07, 1.7455e-06, 1.4840e-06, 1.3322e-06, 9.7914e-07,\n",
       "            2.3586e-06, 5.9609e-07, 5.3135e-07, 1.3124e-06, 6.0029e-07, 9.5287e-07,\n",
       "            1.0891e-06, 9.0482e-07, 4.8139e-07, 9.9070e-07, 3.6158e-07, 3.2022e-07,\n",
       "            5.2570e-07, 7.6852e-08, 1.1798e-08, 9.2037e-07, 7.4813e-07, 8.9220e-07,\n",
       "            3.2979e-07, 8.4778e-07, 9.6650e-07, 1.4960e-06, 5.9371e-07, 1.8103e-06,\n",
       "            9.2842e-07, 9.9369e-07, 7.3012e-07, 1.0368e-06, 1.4021e-06, 1.4791e-06,\n",
       "            6.5212e-07, 2.1283e-06, 8.2172e-07, 8.8645e-07, 1.3988e-06, 8.0507e-07,\n",
       "            1.0925e-09, 9.5187e-07, 7.2705e-08, 2.6321e-07, 1.7031e-06, 8.0075e-07,\n",
       "            9.2943e-07, 6.7598e-07, 1.0898e-06, 6.4717e-07, 1.2801e-06, 7.9770e-07,\n",
       "            5.4262e-07, 5.7193e-07, 4.9005e-07, 4.7550e-07, 7.6534e-07, 1.7421e-07,\n",
       "            1.0262e-06, 7.0535e-07, 8.3798e-07, 4.7248e-07, 9.9644e-07, 6.5603e-07,\n",
       "            9.1952e-07, 7.3084e-07, 6.1106e-07, 6.0213e-07, 7.4756e-07, 1.0607e-06,\n",
       "            4.6905e-07, 5.0957e-07, 5.8216e-07, 5.9547e-07, 8.3884e-11, 1.5465e-07,\n",
       "            1.0877e-06, 3.9536e-07, 4.3225e-07, 1.7209e-07, 4.8408e-07, 4.8740e-07,\n",
       "            1.5600e-06, 9.5253e-07, 6.2175e-07, 8.1766e-07, 1.4430e-06, 9.3037e-07,\n",
       "            7.6765e-07, 1.0351e-06, 1.1335e-06, 6.2252e-07, 8.8634e-07, 1.4705e-06,\n",
       "            5.4249e-07, 4.1245e-09, 6.8280e-07, 1.2122e-06, 9.3009e-07, 4.1178e-07,\n",
       "            1.1415e-06, 5.4757e-07, 1.2109e-06, 6.7609e-07, 6.4418e-07, 6.8606e-07,\n",
       "            9.8617e-07, 1.7311e-06])},\n",
       "   8: {'step': 65292,\n",
       "    'exp_avg': tensor([[-1.1861e-05,  1.2351e-06,  5.2225e-06,  ..., -1.1708e-06,\n",
       "             -1.0052e-06,  2.0453e-06],\n",
       "            [ 3.2797e-05,  5.6132e-07, -4.2611e-05,  ...,  1.0488e-05,\n",
       "             -8.1350e-07,  8.4116e-07],\n",
       "            [-3.7484e-04, -1.1720e-04,  3.8584e-04,  ...,  5.4540e-04,\n",
       "              2.3127e-04, -7.2105e-04],\n",
       "            ...,\n",
       "            [-5.3136e-10,  3.1350e-10, -7.0381e-11,  ..., -2.7024e-10,\n",
       "             -2.7720e-10,  2.2590e-10],\n",
       "            [-5.6185e-10,  3.2086e-10, -4.4374e-11,  ..., -2.7954e-10,\n",
       "             -2.9869e-10,  2.2448e-10],\n",
       "            [-5.8005e-10,  3.3914e-10, -2.8489e-11,  ..., -2.9892e-10,\n",
       "             -3.0277e-10,  2.3943e-10]]),\n",
       "    'exp_avg_sq': tensor([[4.4884e-08, 9.0932e-10, 2.2963e-08,  ..., 4.3183e-09, 1.6846e-09,\n",
       "             3.3682e-09],\n",
       "            [1.6079e-08, 1.6930e-09, 9.1933e-09,  ..., 2.0407e-09, 2.0535e-09,\n",
       "             3.2724e-09],\n",
       "            [1.0303e-05, 1.8922e-06, 1.3532e-05,  ..., 9.4504e-06, 4.4404e-06,\n",
       "             3.9996e-06],\n",
       "            ...,\n",
       "            [5.4914e-15, 9.5247e-15, 4.7302e-16,  ..., 5.3946e-15, 1.3678e-15,\n",
       "             1.8056e-14],\n",
       "            [7.7783e-15, 1.0231e-14, 7.5369e-16,  ..., 7.8803e-15, 2.3864e-15,\n",
       "             1.7619e-14],\n",
       "            [2.8653e-15, 1.0441e-14, 6.6212e-16,  ..., 5.8418e-15, 7.0168e-16,\n",
       "             2.1110e-14]])},\n",
       "   9: {'step': 65292,\n",
       "    'exp_avg': tensor([ 1.9980e-05, -4.1674e-05, -1.0006e-03,  1.3460e-05, -1.1929e-04,\n",
       "             3.3961e-07, -2.0932e-04, -3.6931e-04,  6.5450e-08, -8.1387e-06,\n",
       "             1.3675e-03, -1.6541e-03,  1.1472e-04, -7.5356e-04, -2.8290e-04,\n",
       "            -3.9753e-05, -5.1665e-05,  2.5543e-04,  4.0846e-07, -4.5695e-04,\n",
       "             1.4535e-09, -1.1461e-03,  1.9628e-04,  8.8613e-05,  1.5108e-09,\n",
       "            -3.9593e-04,  5.9432e-04, -3.4838e-04,  6.9473e-06, -1.6409e-03,\n",
       "             1.0379e-03,  3.4668e-04,  5.9200e-05,  9.4648e-05, -1.4924e-05,\n",
       "            -7.1363e-06,  1.4500e-04,  6.5246e-06,  1.6705e-09,  4.0719e-05,\n",
       "            -3.5766e-03,  3.8353e-04,  1.6891e-04, -3.2554e-04, -9.1468e-05,\n",
       "            -1.8482e-04,  7.1520e-04,  2.2157e-04,  5.4991e-04,  1.7640e-03,\n",
       "             8.9081e-06,  1.4823e-09,  4.1153e-04,  6.8542e-06, -2.7144e-04,\n",
       "             1.1452e-03, -1.2746e-03,  9.7861e-05, -4.4507e-05,  3.1365e-04,\n",
       "             1.2592e-03,  1.4717e-09,  1.9868e-05, -1.9220e-04,  6.5763e-05,\n",
       "             4.8984e-06,  2.9543e-03,  3.7216e-04,  2.9915e-07,  7.6367e-06,\n",
       "             1.7023e-09,  1.1767e-03,  4.8933e-04,  1.4745e-09, -3.5961e-03,\n",
       "            -1.7757e-04,  1.8589e-04,  2.7140e-05, -4.1723e-05, -2.0957e-04,\n",
       "            -7.0190e-05, -4.0182e-04,  1.6519e-07,  2.6869e-04,  6.6104e-04,\n",
       "            -1.2591e-04,  1.9964e-04,  1.2460e-03,  1.0623e-05, -4.7999e-07,\n",
       "             1.4679e-09,  1.4770e-09,  1.5353e-09,  1.6076e-09,  1.3506e-09,\n",
       "             1.5191e-09,  1.4026e-09,  1.4849e-09,  1.5625e-09,  1.5805e-09]),\n",
       "    'exp_avg_sq': tensor([8.7214e-08, 2.8734e-08, 2.3462e-05, 1.2768e-06, 2.3256e-08, 2.5582e-10,\n",
       "            4.0118e-05, 5.7208e-07, 2.1240e-10, 3.7331e-08, 2.3307e-05, 1.0192e-04,\n",
       "            5.5230e-07, 1.8673e-05, 1.7020e-06, 2.1521e-07, 1.7578e-06, 5.2774e-06,\n",
       "            1.3545e-09, 2.5947e-06, 2.4816e-12, 2.1888e-06, 2.3267e-06, 9.5649e-07,\n",
       "            2.7232e-12, 9.1675e-07, 7.5967e-06, 1.3738e-06, 9.5325e-08, 2.4936e-05,\n",
       "            1.7833e-05, 4.1498e-06, 1.4661e-06, 1.0109e-06, 1.5911e-06, 6.0364e-07,\n",
       "            1.0678e-06, 4.0342e-08, 3.1417e-12, 8.2906e-08, 7.9486e-05, 8.8055e-07,\n",
       "            4.4025e-07, 8.8755e-07, 2.0698e-07, 6.1397e-08, 5.5051e-05, 3.6905e-06,\n",
       "            2.7700e-05, 1.5253e-05, 2.7475e-08, 2.6104e-12, 1.6617e-06, 2.8898e-08,\n",
       "            8.2965e-06, 8.7401e-05, 9.4684e-05, 2.6420e-07, 4.1041e-07, 2.3221e-05,\n",
       "            5.4813e-05, 2.8382e-12, 2.1303e-07, 3.6165e-08, 2.8666e-05, 1.0575e-08,\n",
       "            5.0590e-05, 1.5050e-06, 5.1713e-10, 3.2977e-08, 3.4733e-12, 1.8559e-05,\n",
       "            1.1139e-05, 2.6125e-12, 7.8667e-05, 3.1540e-05, 1.7186e-06, 6.8335e-08,\n",
       "            8.1962e-05, 2.8453e-06, 7.7173e-07, 6.6962e-05, 2.4812e-09, 9.4921e-07,\n",
       "            1.6522e-05, 1.3443e-06, 5.3195e-06, 2.8865e-05, 5.8754e-08, 3.6929e-09,\n",
       "            3.2261e-12, 2.6673e-12, 3.0392e-12, 3.4836e-12, 2.6775e-12, 2.8261e-12,\n",
       "            2.4956e-12, 2.5167e-12, 2.8633e-12, 2.7930e-12])}},\n",
       "  'param_groups': [{'lr': 0.0001,\n",
       "    'betas': (0.9, 0.999),\n",
       "    'eps': 1e-08,\n",
       "    'weight_decay': 0,\n",
       "    'amsgrad': False,\n",
       "    'maximize': False,\n",
       "    'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]},\n",
       " 'epoch': 17,\n",
       " 'train_loss': 1.5450665950775146,\n",
       " 'test_loss': tensor(1.9064, requires_grad=True)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66ec15f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model_state_dict', 'optimizer_state_dict', 'epoch', 'train_loss', 'test_loss'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0a8f4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get text for validation data\n",
    "val_text = get_text(os.path.join(DATA_DIR, \"The-Prince.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0922ab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encode train data\n",
    "encoded_text, _, _, _ = encode_text(text, False, unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a13fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encode validation data\n",
    "encoding_results = encode_text(val_text, False, unique_chars)\n",
    "encoded_val = encoding_results.encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa2cc040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_char = encoding_results.unique_char\n",
    "len(unique_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6efab6",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    " |              N ={} & \\text{batch size} \\\\\n",
    " |              L ={} & \\text{sequence length} \\\\\n",
    " |              D ={} & 2 \\text{ if bidirectional=True otherwise } 1 \\\\\n",
    " |              H_{in} ={} & \\text{input\\_size} \\\\\n",
    " |              H_{cell} ={} & \\text{hidden\\_size} \\\\\n",
    " |              H_{out} ={} & \\text{proj\\_size if } \\text{proj\\_size}>0 \\text{ otherwise hidden\\_size} \\\\\n",
    " |          \\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c2b11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Character-level LSTM.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_size:\n",
    "        Number of output features for LSTM.\n",
    "    dropout:\n",
    "        Dropout probabilityfor LSTM.\n",
    "    batch_size:\n",
    "        Number of sequences in a batch.\n",
    "    D:\n",
    "        Number of directions: uni- or bidirectional architecture for LSTM.\n",
    "    num_layers:\n",
    "        Number of LSTM stacks.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output:\n",
    "        Shape: [batch_size, sequence_length, num_features]\n",
    "    hidden_state:\n",
    "        Tuple containing:\n",
    "        - Short-term hidden state\n",
    "            Shape: [batch_size, sequence_length, num_features]\n",
    "        - Cell state\n",
    "            Shape: [batch_size, sequence_length, num_features]\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size = 128, dropout = 0.25,\n",
    "                 batch_size = 32, D = 1, num_layers = 2):\n",
    "        \n",
    "        super(CharRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.D = D\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = len(unique_chars), hidden_size = self.hidden_size,\n",
    "                            dropout = self.dropout_rate, batch_first = True,\n",
    "                            bidirectional = True if self.D == 2 else False, bias = True,\n",
    "                            num_layers = self.num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(self.D*self.hidden_size, len(unique_chars))\n",
    "        \n",
    "    def forward(self, x, hidden_state):\n",
    "        outputs, hidden_state = self.lstm(x, hidden_state)\n",
    "        outputs = outputs.contiguous().view(-1, self.D*self.hidden_size)\n",
    "        outputs = self.fc(outputs)\n",
    "        \n",
    "        return outputs, hidden_state\n",
    "    \n",
    "    def init_hidden_state(self, mean, stddev):\n",
    "        \"\"\"\n",
    "        Initialize hidden state and context tensors.\n",
    "        \"\"\"\n",
    "        \n",
    "        h = torch.distributions.Normal(mean, stddev).sample((self.D*self.num_layers, self.batch_size, self.hidden_size))\n",
    "        c = torch.distributions.Normal(mean, stddev).sample((self.D*self.num_layers, self.batch_size, self.hidden_size))\n",
    "        \n",
    "        return (h, c)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc5a71f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRNN(D = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fd1e3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_length = 16\n",
    "\n",
    "max_norm = 1.5\n",
    "epochs = 5\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96b1a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.25)\n",
      "  (fc): Linear(in_features=128, out_features=100, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0213c78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(info['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42b8ba83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (lstm): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.25)\n",
       "  (fc): Linear(in_features=128, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bc77a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.requires_grad_ = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27203450",
   "metadata": {},
   "outputs": [],
   "source": [
    "int2char = encoding_results.int2char\n",
    "char2int = encoding_results.char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59160640",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'David'\n",
    "k = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b4f31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_list = list(seed)\n",
    "a = torch.distributions.Normal(scale = 0.5, loc = 0.).sample((2, 1, 128))\n",
    "b = torch.distributions.Normal(scale = 0.5, loc = 0.).sample((2, 1, 128))\n",
    "h = (a, b)\n",
    "for char in seed_list:\n",
    "    h = tuple([each.data for each in h])\n",
    "    out, h = model(torch.tensor(one_hot_encode(np.array(char2int[char]).reshape(1, -1), 100)), h)\n",
    "    p = F.softmax(out, dim=-1)\n",
    "    _, chars = p.topk(k = k, dim=-1)\n",
    "    next_char = chars[0,torch.randint(low = 0, high = k, size = (1,))].item()\n",
    "\n",
    "seed_list.append(int2char[next_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cea0787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'a', 'v', 'i', 'd', 'y']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "535411a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.2452, 0.2362, 0.1562, 0.0549, 0.0481]], grad_fn=<TopkBackward0>),\n",
       "indices=tensor([[30, 56, 74, 40, 11]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.topk(k = 5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d7449e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix in range(1000):\n",
    "    h = tuple([each.data for each in h])\n",
    "    out, h = model(torch.tensor(one_hot_encode(np.array(char2int[seed_list[-1]]).reshape(1, -1), 100)), h)\n",
    "    p = F.softmax(out, dim=-1)\n",
    "    _, chars = p.topk(k = k, dim=-1)\n",
    "    next_char = chars[0,torch.distributions.Uniform(low = 0, high = k).sample().to(torch.int32).item()].item()\n",
    "    seed_list.append(int2char[next_char])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09e400d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f4cc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_text = ''.join(seed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1bc1bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davidya\n",
      "or easll hllwifomesibracy:-'All,:'-_\n",
      "Lvvr'\n",
      "medltiny-lrationslibfhs-wounstss ollrine elblr styamsine,\n",
      "swnolivua say hrope\n",
      "pelly;\".\n",
      "Ardiabtaces;;,)e-Lvyu.'s\n",
      "attwyatieft._S\n",
      "ato\" an efphing,).)\n",
      "I'd,\";\",'m to\n",
      "cieft,'g,_,,. Lots.-\"Teen.n.''.f)-\n",
      "hrwsoon't,; ad so chty.,\n",
      "Agoanizcley'ls;- t erf id.',\n",
      "alame\n",
      "tulisf hrilkey'-.-Bnuvy.-Betrucus.,'s.-Wimhlods mmacpor fucceribll tmea twuert. Ad\n",
      "oniade sailiess,, bvet'ostey fmignaif,'lyeldse._n-.I\"ss\n",
      "Spife.-\"T'houblused\"-_Lvru\n",
      "knand\n",
      "ime bvencily ng dose-daypiny-bowaictsed's- holfaucy\n",
      "shit.\n",
      "\"Be,;'r.,,'h.?,? Haspevtins.,ns,'\"--is exe thry-to\n",
      "lugkl bleam hyapinaciefunl,'g-hanchs-wott adonivuler,;.;.n\n",
      "I, wilkiglil feerisiozie mucabeem\n",
      "bousht wyirk,'c tuimiel,),;)st,\n",
      "ifenistor,, smilf-or iln.s.\"'-\"Ivrry\n",
      "trmmougen?_'Ald-rweodibye\n",
      "cutnanly-dhad'f?e\"\"'-_Syokn'g'k\n",
      "hhavcoece._' hons\n",
      "ago,\n",
      "ene\n",
      "oboskayialy' wlhent\n",
      "slulf,.'\n",
      "Hh surdeemeyth,s twalcsoryshs;-saiche\n",
      "conannsko timoig.'\"\".n-How,,;;_'s);)\n",
      "Kesyit,'m..;;,'t,;-.\n",
      "Age,-'flompuem,ss wavy;.-Befut,.' alsoansivrt \n"
     ]
    }
   ],
   "source": [
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e25269e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Davide,\n",
      "treinily wonle.. \"Tely,\n",
      "way a poisule,.\n",
      "\n",
      "I'ld,...\n",
      "\"All, aled,,\n",
      "trepill to\n",
      "many, answey,\n",
      "thoogs. I'r\n",
      "samficatinc tham....\".\n",
      "Ans trivalialy\n",
      "breathiom, thind trings tarth a leatisher,\"\n",
      "haves\n",
      "a sencioul faringet,\n",
      "takay\n",
      "any\n",
      "anspisely,\n",
      "steivn on\n",
      "shoold howered,\n",
      "she\n",
      "he wosddevenced. \"All a lanctiomss.\" Ho tounglliens. \"When'her\n",
      "same..\n",
      "The clovoube homs, we so wife\n",
      "atticuted\n",
      "im a lanct...\n",
      "\n",
      "A his\n",
      "mothach.\n",
      "\"I\n",
      "al at\n",
      "im deciling.. To\n",
      "you'se. Havies.\" \n",
      "Shavarevs whateder,\"\"\" sinicatiagay,\n",
      "at\n",
      "tiself.\n",
      "\"\n",
      "Tinky\n",
      "had. At an toubted\n",
      "trouttlien\n",
      "tay strildialy at\n",
      "hid work whis,,\n",
      "sain\n",
      "too,\n",
      "weriged.\n",
      " \"Yay tell.\n",
      "I\n",
      "asterst. In\n",
      "takiness\n",
      "as\n",
      "is\n",
      "wiflire.\n",
      "Tusko he hoors, tak it. Ithored thints oven\n",
      "had,\"\n",
      "ser iflenstallite, we as hereles,,\n",
      "attonts at\n",
      "its anst imsisfed, takant\n",
      "at her, brand a morow-dreat of\n",
      "it any\n",
      "tensed,\".\"s\".\"\n",
      "\n",
      "Show have\n",
      "herstichs,\"\n",
      "he'ce\n",
      "hig stoob took her\n",
      "facurang on tak anowithiods,,. Alderadlive,\"\" taled,\" soid, stations\n",
      "wher, tho town, a stroumly\n",
      "sourd,\n",
      "went tira is and..\"\n",
      "\"And soletely,\n",
      "we s\n"
     ]
    }
   ],
   "source": [
    "print(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c4beb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0df5e09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ead19aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 100])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.tensor(one_hot_encode(np.array(char2int[char]).reshape(1, -1), 100))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b8b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
